{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "from six.moves import cPickle\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "# from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from prednet import PredNet\n",
    "# from data_utils import SequenceGenerator\n",
    "from kitti_settings import *\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "raw_RAD_id_list = os.listdir('/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/')\n",
    "print(len(raw_RAD_id_list))\n",
    "RAD_id_list = raw_RAD_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_RAD_id(RAD_id):\n",
    "    mean_list = []\n",
    "    for k in range(61):\n",
    "        mean_list.append(np.array(PIL.Image.open('/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png'\n",
    "                         % (RAD_id, RAD_id,\n",
    "                        k))).astype(np.int8).ravel().mean())\n",
    "    mean_list = np.array(mean_list)\n",
    "    if i % 100 == 0:\n",
    "        print(time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time)))\n",
    "        print(i, mean_list[:6])\n",
    "    if mean_list.mean() < 0:\n",
    "        return False\n",
    "    for k in range(59):\n",
    "        if abs(mean_list[k] + mean_list[k + 2] - 2 * mean_list[k + 1]) > 2:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(time.strftime('start time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "RAD_id_list = []\n",
    "for (i, RAD_id) in enumerate(raw_RAD_id_list):\n",
    "    if check_RAD_id(RAD_id):\n",
    "        RAD_id_list.append(RAD_id)\n",
    "print(time.strftime('end time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "print(time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time)))\n",
    "print(len(RAD_id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, nt, image_size, image_scalar, batch_size=32, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.nt = nt\n",
    "        self.image_size = image_size\n",
    "        self.image_scalar = image_scalar\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        X = np.empty((self.batch_size, self.nt, self.image_size, self.image_size, 1))\n",
    "        y = np.empty((self.batch_size, self.image_size, self.image_size, 1))\n",
    "        # Generate data\n",
    "#         print(list_IDs_temp)\n",
    "        for i, RAD_id in enumerate(list_IDs_temp):\n",
    "            for j in range(self.nt):\n",
    "                X[i][j] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * 5)).resize((self.image_size, self.image_size))).astype(np.int8).reshape((self.image_size, self.image_size, 1)) / self.image_scalar\n",
    "            y[i] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, (self.nt) * 5)).resize((self.image_size, self.image_size))).astype(np.int8).reshape((self.image_size, self.image_size, 1)) / self.image_scalar\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "\n",
    "save_model = True  # if weights will be saved\n",
    "weights_file = os.path.join(WEIGHTS_DIR, 'prednet_kitti_weights.hdf5')  # where weights will be saved\n",
    "json_file = os.path.join(WEIGHTS_DIR, 'prednet_kitti_model.json')\n",
    "\n",
    "# # Data files\n",
    "# train_file = os.path.join(DATA_DIR, 'X_train.hkl')\n",
    "# train_sources = os.path.join(DATA_DIR, 'sources_train.hkl')\n",
    "# val_file = os.path.join(DATA_DIR, 'X_val.hkl')\n",
    "# val_sources = os.path.join(DATA_DIR, 'sources_val.hkl')\n",
    "\n",
    "# Training parameters\n",
    "nb_epoch = 15\n",
    "batch_size = 4\n",
    "samples_per_epoch = 50  # 500\n",
    "N_seq_val = None  # 100  number of sequences to use for validation\n",
    "\n",
    "# Model parameters\n",
    "image_size = 256\n",
    "nt = 2  # number of timesteps used for sequences in training\n",
    "image_scalar = 80\n",
    "n_channels, im_height, im_width = (1, image_size, image_size)  # (3, 128, 160)\n",
    "input_shape = (n_channels, im_height, im_width) if K.image_data_format() == 'channels_first' else (im_height, im_width, n_channels)\n",
    "stack_sizes = (n_channels, 48, 96)  # (n_channels, 48, 96, 192)\n",
    "R_stack_sizes = stack_sizes\n",
    "A_filt_sizes = (2, 2)\n",
    "Ahat_filt_sizes = (2, 2, 2)\n",
    "R_filt_sizes = (2, 2, 2)\n",
    "layer_loss_weights = np.array([1., 0., 0.])  # weighting for each layer in final loss; \"L_0\" model:  [1, 0, 0, 0], \"L_all\": [1, 0.1, 0.1, 0.1]\n",
    "layer_loss_weights = np.expand_dims(layer_loss_weights, 1)\n",
    "time_loss_weights = 1./ (nt - 1) * np.ones((nt,1))  # equally weight all timesteps except the first\n",
    "time_loss_weights[0] = 0\n",
    "\n",
    "\n",
    "prednet = PredNet(stack_sizes, R_stack_sizes,\n",
    "                  A_filt_sizes, Ahat_filt_sizes, R_filt_sizes, A_activation='elu', error_activation='elu',\n",
    "                  output_mode='prediction', return_sequences=False)\n",
    "\n",
    "inputs = Input(shape=(nt,) + input_shape)\n",
    "outputs = prednet(inputs)\n",
    "# outputs = np.empty(shape=(nt,) + input_shape)\n",
    "# for i in range(nt):\n",
    "#     temp_outputs = prednet(inputs)\n",
    "#     outputs[i] = temp_outputs\n",
    "#     for j in range(nt - 1):\n",
    "#         inputs[j] = inputs[j + 1]\n",
    "#     inputs[-1] = temp_outputs\n",
    "\n",
    "def my_loss(y_true, y_pred):\n",
    "    w = tf.add(y_true, tf.constant(10.0))\n",
    "    w = tf.add(y_pred, w)\n",
    "#     loss = tf.losses.mean_squared_error(y_true, y_pred, weights=w)\n",
    "    loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "    loss = tf.multiply(loss, tf.constant(1000.0))\n",
    "    return loss\n",
    "\n",
    "# errors = prednet(inputs)  # errors will be (batch_size, nt, nb_layers)\n",
    "# errors_by_time = TimeDistributed(Dense(1, trainable=False), weights=[layer_loss_weights, np.zeros(1)], trainable=False)(errors)  # calculate weighted error by layer\n",
    "# errors_by_time = Flatten()(errors_by_time)  # will be (batch_size, nt)\n",
    "# final_errors = Dense(1, weights=[time_loss_weights, np.zeros(1)], trainable=False)(errors_by_time)  # weight errors by time\n",
    "model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss=my_loss, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 2, 256, 256, 1)    0         \n",
      "_________________________________________________________________\n",
      "pred_net_3 (PredNet)         (None, 256, 256, 1)       711705    \n",
      "=================================================================\n",
      "Total params: 711,705\n",
      "Trainable params: 711,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to use tensorboard\n",
    "```bash\n",
    "python3 /usr/local/lib/python3.5/dist-packages/tensorboard/main.py --logdir='/home/hadoop/Documents/Neutrino/tensorboard_logs' --host=192.168.1.115\n",
    "rm -rf /home/hadoop/Documents/Neutrino/tensorboard_logs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = lambda epoch: 0.003 if epoch < 75 else 0.0003    # start with lr of 0.001 and then drop to 0.0001 after 75 epochs\n",
    "tb_callback = keras.callbacks.TensorBoard(log_dir='/home/hadoop/Documents/Neutrino/tensorboard_logs', histogram_freq=0)\n",
    "callbacks = [LearningRateScheduler(lr_schedule), tb_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "10/10 [==============================] - 2s 241ms/step - loss: 0.4222 - val_loss: 5.2953\n",
      "Epoch 2/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.4003 - val_loss: 5.5075\n",
      "Epoch 3/5000\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3960 - val_loss: 5.2772\n",
      "Epoch 4/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3972 - val_loss: 5.2844\n",
      "Epoch 5/5000\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3892 - val_loss: 4.3500\n",
      "Epoch 6/5000\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3843 - val_loss: 4.3420\n",
      "Epoch 7/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.3816 - val_loss: 4.3421\n",
      "Epoch 8/5000\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3788 - val_loss: 4.4277\n",
      "Epoch 9/5000\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3750 - val_loss: 4.6593\n",
      "Epoch 10/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.3789 - val_loss: 5.6406\n",
      "Epoch 11/5000\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3886 - val_loss: 5.5043\n",
      "Epoch 12/5000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3927 - val_loss: 5.3732\n",
      "Epoch 13/5000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3913 - val_loss: 5.2064\n",
      "Epoch 14/5000\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.3868 - val_loss: 4.6131\n",
      "Epoch 15/5000\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3834 - val_loss: 4.8998\n",
      "Epoch 16/5000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3999 - val_loss: 5.7880\n",
      "Epoch 17/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3914 - val_loss: 5.1664\n",
      "Epoch 18/5000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.3873 - val_loss: 4.8025\n",
      "Epoch 19/5000\n",
      "Epoch 18/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3814 - val_loss: 4.4988\n",
      "Epoch 20/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3778 - val_loss: 4.6509\n",
      "Epoch 21/5000\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3753 - val_loss: 4.7883\n",
      "Epoch 22/5000\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3713 - val_loss: 4.7082\n",
      "Epoch 23/5000\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3629 - val_loss: 4.6877\n",
      "Epoch 24/5000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.3713 - val_loss: 5.0760\n",
      "Epoch 25/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3677 - val_loss: 4.4543\n",
      "Epoch 26/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3594 - val_loss: 4.5160\n",
      "Epoch 27/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3511 - val_loss: 4.9776\n",
      "Epoch 28/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3470 - val_loss: 5.1910\n",
      "Epoch 29/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3394 - val_loss: 4.8642\n",
      "Epoch 30/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3388 - val_loss: 4.7694\n",
      "Epoch 31/5000\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3329 - val_loss: 4.8440\n",
      "Epoch 32/5000\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3312 - val_loss: 4.8498\n",
      "Epoch 33/5000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3312 - val_loss: 4.6935\n",
      "Epoch 34/5000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 0.3340 - val_loss: 5.0205\n",
      "Epoch 35/5000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.3393 - val_loss: 4.9372\n",
      "Epoch 36/5000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3356 - val_loss: 4.7466\n",
      "Epoch 37/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.3254 - val_loss: 4.6950\n",
      "Epoch 38/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3216 - val_loss: 4.8319\n",
      "Epoch 39/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3201 - val_loss: 4.8630\n",
      "Epoch 40/5000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3185 - val_loss: 4.8869\n",
      "Epoch 41/5000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.3166 - val_loss: 4.9416\n",
      "Epoch 42/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.3164 - val_loss: 4.8747\n",
      "Epoch 43/5000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.3232 - val_loss: 4.7536\n",
      "Epoch 44/5000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.3174 - val_loss: 4.8054\n",
      "Epoch 45/5000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 0.3130 - val_loss: 4.8405\n",
      "Epoch 46/5000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 0.3105 - val_loss: 4.8148\n",
      "Epoch 47/5000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 0.3081 - val_loss: 4.8292\n",
      "Epoch 48/5000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.3068 - val_loss: 4.7760\n",
      "Epoch 49/5000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.3065 - val_loss: 5.1634\n",
      "Epoch 50/5000\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 0.3270 - val_loss: 5.1456\n",
      "Epoch 51/5000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 0.3184 - val_loss: 4.7010\n",
      "Epoch 52/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3104 - val_loss: 4.8446\n",
      "Epoch 53/5000\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3082 - val_loss: 4.7417\n",
      "Epoch 54/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3048 - val_loss: 4.6976\n",
      "Epoch 55/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.3035 - val_loss: 4.6905\n",
      "Epoch 56/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3004 - val_loss: 4.6885\n",
      "Epoch 57/5000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 0.2977 - val_loss: 4.6899\n",
      "Epoch 58/5000\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2958 - val_loss: 4.6973\n",
      "Epoch 59/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3075 - val_loss: 4.6166\n",
      "Epoch 60/5000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 0.3089 - val_loss: 4.6341\n",
      "Epoch 61/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3062 - val_loss: 4.6237\n",
      "Epoch 62/5000\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.3110 - val_loss: 4.7157\n",
      "Epoch 63/5000\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3060 - val_loss: 4.9179\n",
      "Epoch 64/5000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2998 - val_loss: 4.7459\n",
      "Epoch 65/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2971 - val_loss: 4.7758\n",
      "Epoch 66/5000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2943 - val_loss: 4.7736\n",
      "Epoch 67/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2917 - val_loss: 4.7068\n",
      "Epoch 68/5000\n",
      "10/10 [==============================] - 1s 130ms/step - loss: 0.2899 - val_loss: 4.6479\n",
      "Epoch 69/5000\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.2879 - val_loss: 4.6412\n",
      "Epoch 70/5000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 0.2853 - val_loss: 4.6415\n",
      "Epoch 71/5000\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2854 - val_loss: 4.6334\n",
      "Epoch 72/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2835 - val_loss: 4.6368\n",
      "Epoch 73/5000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 0.2863 - val_loss: 4.6558\n",
      "Epoch 74/5000\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2870 - val_loss: 4.7041\n",
      "Epoch 75/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2823 - val_loss: 4.5475\n",
      "Epoch 76/5000\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2784 - val_loss: 4.5604\n",
      "Epoch 77/5000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2762 - val_loss: 4.5756\n",
      "Epoch 78/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2752 - val_loss: 4.5888\n",
      "Epoch 79/5000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2747 - val_loss: 4.5925\n",
      "Epoch 80/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2741 - val_loss: 4.5927\n",
      "Epoch 81/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2737 - val_loss: 4.5923\n",
      "Epoch 82/5000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2733 - val_loss: 4.5932\n",
      "Epoch 83/5000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2729 - val_loss: 4.5972\n",
      "Epoch 84/5000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 0.2726 - val_loss: 4.5971\n",
      "Epoch 85/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2722 - val_loss: 4.5985\n",
      "Epoch 86/5000\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2719 - val_loss: 4.5997\n",
      "Epoch 87/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2716 - val_loss: 4.6000\n",
      "Epoch 88/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2713 - val_loss: 4.6000\n",
      "Epoch 89/5000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2709 - val_loss: 4.6005\n",
      "Epoch 90/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2706 - val_loss: 4.6017\n",
      "Epoch 91/5000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 0.2703 - val_loss: 4.6027\n",
      "Epoch 92/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2700 - val_loss: 4.6043\n",
      "Epoch 93/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2697 - val_loss: 4.6053\n",
      "Epoch 94/5000\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2694 - val_loss: 4.6074\n",
      "Epoch 95/5000\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.2692 - val_loss: 4.6082\n",
      "Epoch 96/5000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2689 - val_loss: 4.6101\n",
      "Epoch 97/5000\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2687 - val_loss: 4.6096\n",
      "Epoch 98/5000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 0.2684 - val_loss: 4.6124\n",
      "Epoch 99/5000\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2682 - val_loss: 4.6135\n",
      "Epoch 100/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2680 - val_loss: 4.6149\n",
      "Epoch 101/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2678 - val_loss: 4.6157\n",
      "Epoch 102/5000\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2676 - val_loss: 4.6171\n",
      "Epoch 103/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2674 - val_loss: 4.6179\n",
      "Epoch 104/5000\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2672 - val_loss: 4.6191\n",
      "Epoch 105/5000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2670 - val_loss: 4.6207\n",
      "Epoch 106/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2667 - val_loss: 4.6208\n",
      "Epoch 107/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2665 - val_loss: 4.6233\n",
      "Epoch 108/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2661 - val_loss: 4.6271\n",
      "Epoch 109/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2657 - val_loss: 4.6268\n",
      "Epoch 110/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2652 - val_loss: 4.6326\n",
      "Epoch 111/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2648 - val_loss: 4.6344\n",
      "Epoch 112/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2645 - val_loss: 4.6358\n",
      "Epoch 113/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2643 - val_loss: 4.6371\n",
      "Epoch 114/5000\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2640 - val_loss: 4.6377\n",
      "Epoch 115/5000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2638 - val_loss: 4.6389\n",
      "Epoch 116/5000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2635 - val_loss: 4.6425\n",
      "Epoch 117/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2633 - val_loss: 4.6440\n",
      "Epoch 118/5000\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2631 - val_loss: 4.6427\n",
      "Epoch 119/5000\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2629 - val_loss: 4.6456\n",
      "Epoch 120/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2626 - val_loss: 4.6498\n",
      "Epoch 121/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2624 - val_loss: 4.6489\n",
      "Epoch 122/5000\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2622 - val_loss: 4.6515\n",
      "Epoch 123/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2620 - val_loss: 4.6518\n",
      "Epoch 124/5000\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2619 - val_loss: 4.6531\n",
      "Epoch 125/5000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2617 - val_loss: 4.6542\n",
      "Epoch 126/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2616 - val_loss: 4.6560\n",
      "Epoch 127/5000\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2614 - val_loss: 4.6560\n",
      "Epoch 128/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2613 - val_loss: 4.6598\n",
      "Epoch 129/5000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 0.2611 - val_loss: 4.6580\n",
      "Epoch 130/5000\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2610 - val_loss: 4.6613\n",
      "Epoch 131/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2609 - val_loss: 4.6620\n",
      "Epoch 132/5000\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2608 - val_loss: 4.6639\n",
      "Epoch 133/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2608 - val_loss: 4.6615\n",
      "Epoch 134/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2607 - val_loss: 4.6668\n",
      "Epoch 135/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2606 - val_loss: 4.6611\n",
      "Epoch 136/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2607 - val_loss: 4.6656\n",
      "Epoch 137/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2604 - val_loss: 4.6672\n",
      "Epoch 138/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2600 - val_loss: 4.6659\n",
      "Epoch 139/5000\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2600 - val_loss: 4.6679\n",
      "Epoch 140/5000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 0.2598 - val_loss: 4.6698\n",
      "Epoch 141/5000\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2597 - val_loss: 4.6687\n",
      "Epoch 142/5000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2595 - val_loss: 4.6712\n",
      "Epoch 143/5000\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2594 - val_loss: 4.6716\n",
      "Epoch 144/5000\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2593 - val_loss: 4.6713\n",
      "Epoch 145/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2593 - val_loss: 4.6728\n",
      "Epoch 146/5000\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2592 - val_loss: 4.6742\n",
      "Epoch 147/5000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.2592 - val_loss: 4.6743\n",
      "Epoch 148/5000\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2591 - val_loss: 4.6735\n",
      "Epoch 149/5000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2591 - val_loss: 4.6764\n",
      "Epoch 150/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2592 - val_loss: 4.6748\n",
      "Epoch 151/5000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2589 - val_loss: 4.6753\n",
      "Epoch 152/5000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2589 - val_loss: 4.6756\n",
      "Epoch 153/5000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 0.2586 - val_loss: 4.6773\n",
      "Epoch 154/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2584 - val_loss: 4.6762\n",
      "Epoch 155/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2583 - val_loss: 4.6765\n",
      "Epoch 156/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2583 - val_loss: 4.6770\n",
      "Epoch 157/5000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.2583 - val_loss: 4.6773\n",
      "Epoch 158/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2584 - val_loss: 4.6770\n",
      "Epoch 159/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2589 - val_loss: 4.6753\n",
      "Epoch 160/5000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2605 - val_loss: 4.6772\n",
      "Epoch 161/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2588 - val_loss: 4.6759\n",
      "Epoch 162/5000\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2585 - val_loss: 4.6751\n",
      "Epoch 163/5000\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.2581 - val_loss: 4.6717\n",
      "Epoch 164/5000\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.2576 - val_loss: 4.6783\n",
      "Epoch 165/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2573 - val_loss: 4.6786\n",
      "Epoch 166/5000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2572 - val_loss: 4.6806\n",
      "Epoch 167/5000\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2570 - val_loss: 4.6830\n",
      "Epoch 168/5000\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2569 - val_loss: 4.6834\n",
      "Epoch 169/5000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 0.2568 - val_loss: 4.6831\n",
      "Epoch 170/5000\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2568 - val_loss: 4.6845\n",
      "Epoch 171/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2567 - val_loss: 4.6851\n",
      "Epoch 172/5000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2567 - val_loss: 4.6857\n",
      "Epoch 173/5000\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2567 - val_loss: 4.6871\n",
      "Epoch 174/5000\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2568 - val_loss: 4.6852\n",
      "Epoch 175/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2566 - val_loss: 4.6870\n",
      "Epoch 176/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2565 - val_loss: 4.6840\n",
      "Epoch 177/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2566 - val_loss: 4.6878\n",
      "Epoch 178/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2563 - val_loss: 4.6881\n",
      "Epoch 179/5000\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2562 - val_loss: 4.6897\n",
      "Epoch 180/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2562 - val_loss: 4.6902\n",
      "Epoch 181/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2561 - val_loss: 4.6898\n",
      "Epoch 182/5000\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2560 - val_loss: 4.6906\n",
      "Epoch 183/5000\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.2560 - val_loss: 4.6907\n",
      "Epoch 184/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2560 - val_loss: 4.6912\n",
      "Epoch 185/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2561 - val_loss: 4.6902\n",
      "Epoch 186/5000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2562 - val_loss: 4.6886\n",
      "Epoch 187/5000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 0.2567 - val_loss: 4.6875\n",
      "Epoch 188/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2577 - val_loss: 4.6971\n",
      "Epoch 189/5000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 0.2586 - val_loss: 4.6827\n",
      "Epoch 190/5000\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2581 - val_loss: 4.6876\n",
      "Epoch 191/5000\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2581 - val_loss: 4.6793\n",
      "Epoch 192/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2581 - val_loss: 4.6762\n",
      "Epoch 193/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2583 - val_loss: 4.6810\n",
      "Epoch 194/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2570 - val_loss: 4.6834\n",
      "Epoch 195/5000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2565 - val_loss: 4.6751\n",
      "Epoch 196/5000\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2561 - val_loss: 4.6874\n",
      "Epoch 197/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2560 - val_loss: 4.6766\n",
      "Epoch 198/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2557 - val_loss: 4.6863\n",
      "Epoch 199/5000\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.2556 - val_loss: 4.6857\n",
      "Epoch 200/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2554 - val_loss: 4.6829\n",
      "Epoch 201/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2553 - val_loss: 4.6870\n",
      "Epoch 202/5000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 0.2553 - val_loss: 4.6861\n",
      "Epoch 203/5000\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2552 - val_loss: 4.6871\n",
      "Epoch 204/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2551 - val_loss: 4.6879\n",
      "Epoch 205/5000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2551 - val_loss: 4.6891\n",
      "Epoch 206/5000\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2025Epoch 205/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2550 - val_loss: 4.6887\n",
      "Epoch 207/5000\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2550 - val_loss: 4.6909\n",
      "Epoch 208/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2549 - val_loss: 4.6905\n",
      "Epoch 209/5000\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2549 - val_loss: 4.6887\n",
      "Epoch 210/5000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2549 - val_loss: 4.6907\n",
      "Epoch 211/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2548 - val_loss: 4.6917\n",
      "Epoch 212/5000\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2549 - val_loss: 4.6925\n",
      "Epoch 213/5000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2549 - val_loss: 4.6938\n",
      "Epoch 214/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2549 - val_loss: 4.6944\n",
      "Epoch 215/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2548 - val_loss: 4.6909\n",
      "Epoch 216/5000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.2547 - val_loss: 4.6952\n",
      "Epoch 217/5000\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2546 - val_loss: 4.6935\n",
      "Epoch 218/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2546 - val_loss: 4.6940\n",
      "Epoch 219/5000\n",
      "10/10 [==============================] - 1s 130ms/step - loss: 0.2545 - val_loss: 4.6944\n",
      "Epoch 220/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2545 - val_loss: 4.6922\n",
      "Epoch 221/5000\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2545 - val_loss: 4.6962\n",
      "Epoch 222/5000\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2544 - val_loss: 4.6940\n",
      "Epoch 223/5000\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2543 - val_loss: 4.6957\n",
      "Epoch 224/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2544 - val_loss: 4.6980\n",
      "Epoch 225/5000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.2544 - val_loss: 4.6962\n",
      "Epoch 226/5000\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2542 - val_loss: 4.6997\n",
      "Epoch 227/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2542 - val_loss: 4.6984\n",
      "Epoch 228/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2542 - val_loss: 4.6964\n",
      "Epoch 229/5000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2542 - val_loss: 4.6978\n",
      "Epoch 230/5000\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2543 - val_loss: 4.6994\n",
      "Epoch 231/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2542 - val_loss: 4.6988\n",
      "Epoch 232/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2543 - val_loss: 4.6964\n",
      "Epoch 233/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2545 - val_loss: 4.6983\n",
      "Epoch 234/5000\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2556 - val_loss: 4.6932\n",
      "Epoch 235/5000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.2565 - val_loss: 4.7007\n",
      "Epoch 236/5000\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2553 - val_loss: 4.6899\n",
      "Epoch 237/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2551 - val_loss: 4.6986\n",
      "Epoch 238/5000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2545 - val_loss: 4.6867\n",
      "Epoch 239/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2545 - val_loss: 4.6934\n",
      "Epoch 240/5000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2540 - val_loss: 4.6957\n",
      "Epoch 241/5000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2536 - val_loss: 4.6981\n",
      "Epoch 242/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2535 - val_loss: 4.6944\n",
      "Epoch 243/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2531 - val_loss: 4.6954\n",
      "Epoch 244/5000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2530 - val_loss: 4.6989\n",
      "Epoch 245/5000\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2530 - val_loss: 4.6954\n",
      "Epoch 246/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2528 - val_loss: 4.7028\n",
      "Epoch 247/5000\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.2527 - val_loss: 4.7017\n",
      "Epoch 248/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2526 - val_loss: 4.7076\n",
      "Epoch 249/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2526 - val_loss: 4.6975\n",
      "Epoch 250/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2523 - val_loss: 4.7069\n",
      "Epoch 251/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2521 - val_loss: 4.6995\n",
      "Epoch 252/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2520 - val_loss: 4.7041\n",
      "Epoch 253/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2523 - val_loss: 4.6994\n",
      "Epoch 254/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2523 - val_loss: 4.7095\n",
      "Epoch 255/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2519 - val_loss: 4.6968\n",
      "Epoch 256/5000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 0.2517 - val_loss: 4.7102\n",
      "Epoch 257/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2517 - val_loss: 4.6965\n",
      "Epoch 258/5000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2519 - val_loss: 4.7044\n",
      "Epoch 259/5000\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.2521 - val_loss: 4.7001\n",
      "Epoch 260/5000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2526 - val_loss: 4.6895\n",
      "Epoch 261/5000\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.2540 - val_loss: 4.6870\n",
      "Epoch 262/5000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2534 - val_loss: 4.6775\n",
      "Epoch 263/5000\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.2535 - val_loss: 4.6946\n",
      "Epoch 264/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2519 - val_loss: 4.6827\n",
      "Epoch 265/5000\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2515 - val_loss: 4.6989\n",
      "Epoch 266/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2511 - val_loss: 4.6813\n",
      "Epoch 267/5000\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2510 - val_loss: 4.6901\n",
      "Epoch 268/5000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2509 - val_loss: 4.6915\n",
      "Epoch 269/5000\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.2509 - val_loss: 4.6885\n",
      "Epoch 270/5000\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2508 - val_loss: 4.6919\n",
      "Epoch 271/5000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2507 - val_loss: 4.6914\n",
      "Epoch 272/5000\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2506 - val_loss: 4.6922\n",
      "Epoch 273/5000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2506 - val_loss: 4.6963\n",
      "Epoch 274/5000\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2506 - val_loss: 4.6936\n",
      "Epoch 275/5000\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2505 - val_loss: 4.6990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-1101:\n",
      "Process ForkPoolWorker-1099:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 276/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-0a10d85be849>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_IDs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRAD_id_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_scalar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_scalar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvalid_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_IDs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRAD_id_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_scalar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_scalar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2670\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2671\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2672\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2652\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2654\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2655\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_generator = myGenerator(list_IDs=RAD_id_list[:5], nt=nt, image_size=image_size, image_scalar=image_scalar, batch_size=1)\n",
    "valid_generator = myGenerator(list_IDs=RAD_id_list[-10:], nt=nt, image_size=image_size, image_scalar=image_scalar, batch_size=1, shuffle=False)\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=10, epochs=5000, validation_data=valid_generator, validation_steps=2 ,use_multiprocessing=True, max_queue_size=20, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 3.2636 - val_loss: 4.2016\n",
      "\n",
      "Epoch 2/5000\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 3.4990 - val_loss: 4.1498\n",
      "Epoch 3/5000\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 3.4817 - val_loss: 4.0558\n",
      "Epoch 4/5000\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 3.5039 - val_loss: 3.9355\n",
      "Epoch 5/5000\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 3.3304 - val_loss: 4.0036\n",
      "Epoch 6/5000\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 3.2437 - val_loss: 3.9398\n",
      "Epoch 7/5000\n",
      " 65/100 [==================>...........] - ETA: 12s - loss: 3.0933"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-231:\n",
      "Process ForkPoolWorker-225:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-67c0ed9f7c0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_IDs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRAD_id_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_scalar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_scalar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvalid_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_IDs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRAD_id_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_scalar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_scalar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2670\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2671\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2672\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2652\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2654\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2655\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-232:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "train_generator = myGenerator(list_IDs=RAD_id_list[:4500], nt=nt, image_size=image_size, image_scalar=image_scalar, batch_size=5)\n",
    "valid_generator = myGenerator(list_IDs=RAD_id_list[4500:], nt=nt, image_size=image_size, image_scalar=image_scalar, batch_size=5)\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=100, epochs=5000, validation_data=valid_generator, validation_steps=20 ,use_multiprocessing=True, max_queue_size=20, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.on_epoch_end()\n",
    "valid_generator.on_epoch_end()\n",
    "for data in [train_generator, valid_generator][0]:\n",
    "    x, y_ = data\n",
    "    break\n",
    "y = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x[0][-1].reshape((image_size, image_size)) * image_scalar, cmap=cm.gist_ncar_r)\n",
    "plt.show()\n",
    "print('')\n",
    "plt.imshow(y_[0].reshape((image_size, image_size)) * image_scalar, cmap=cm.gist_ncar_r)\n",
    "plt.show()\n",
    "print('')\n",
    "plt.imshow(y[0].reshape((image_size, image_size)) * image_scalar, cmap=cm.gist_ncar_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAD_id = RAD_id_list[3]\n",
    "x_matrix = np.empty((nt, image_size, image_size, 1))\n",
    "for i in range(nt):\n",
    "    x_matrix[i] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, i)).resize((image_size, image_size))).astype(np.int8).reshape((image_size, image_size, 1)) / SCALAR\n",
    "x_matrix = x_matrix.reshape((1, nt, image_size, image_size, 1))\n",
    "y_matrix = np.empty((nt, image_size, image_size, 1))\n",
    "for i in range(nt):\n",
    "    y_matrix[i] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, i + 10)).resize((image_size, image_size))).astype(np.int8).reshape((image_size, image_size, 1)) / SCALAR\n",
    "y_matrix = y_matrix.reshape((1, nt, image_size, image_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(a.reshape((image_size, image_size)) * SCALAR, cmap=cm.gist_ncar_r)\n",
    "plt.show()\n",
    "print('%d%d' % (1, 1))\n",
    "plt.imshow(b.reshape((image_size, image_size)) * SCALAR, cmap=cm.gist_ncar_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = list(model.layers[0].batch_input_shape[1:])\n",
    "input_shape[0] = nt\n",
    "inputs = Input(shape=tuple(input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_matrix.shape)\n",
    "print(y_matrix.shape)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_input_shape_at(0))\n",
    "print(model.get_output_shape_at(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%quickref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
