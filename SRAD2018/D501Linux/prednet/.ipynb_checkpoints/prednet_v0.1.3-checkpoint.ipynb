{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "from six.moves import cPickle\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras import backend as K\n",
    "# from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from prednet import PredNet\n",
    "# from data_utils import SequenceGenerator\n",
    "from kitti_settings import *\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from data_utils import SequenceGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hickle as hkl\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import Iterator\n",
    "\n",
    "# Data generator that creates sequences for input into PredNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "SCALAR = 80\n",
    "image_size = 512\n",
    "\n",
    "raw_RAD_id_list = os.listdir('/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/')\n",
    "print(len(raw_RAD_id_list))\n",
    "RAD_id_list = raw_RAD_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_RAD_id(RAD_id):\n",
    "    sum_list = []\n",
    "    for k in range(61):\n",
    "        sum_list.append(np.array(PIL.Image.open('/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png'\n",
    "                         % (RAD_id, RAD_id,\n",
    "                        k))).astype(np.int8).ravel().sum())\n",
    "    sum_list = np.array(sum_list)\n",
    "    if i % 100 == 0:\n",
    "        print(time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time)))\n",
    "        print(i, sum_list[:7])\n",
    "    if sum_list.mean() < 251001 * 0:\n",
    "        return False\n",
    "    for k in range(59):\n",
    "        if abs(sum_list[k] + sum_list[k + 2] - 2 * sum_list[k + 1]) > 251001 * 2:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 2018-08-27 18:56:55\n",
      "00:00:00\n",
      "0 [-218731 -223051 -219804 -220007 -214895 -215379 -215980]\n",
      "end time: 2018-08-27 18:57:00\n",
      "00:00:05\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(time.strftime('start time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "RAD_id_list = []\n",
    "for (i, RAD_id) in enumerate(raw_RAD_id_list[:100]):\n",
    "    if check_RAD_id(RAD_id):\n",
    "        RAD_id_list.append(RAD_id)\n",
    "print(time.strftime('end time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "print(time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time)))\n",
    "print(len(RAD_id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAD_id = RAD_id_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceGenerator(Iterator):\n",
    "    def __init__(self, data_file, source_file, nt,\n",
    "                 batch_size=8, shuffle=False, seed=None,\n",
    "                 output_mode='error', sequence_start_mode='all', N_seq=None,\n",
    "                 data_format=K.image_data_format()):\n",
    "#         self.X = hkl.load(data_file)  # X will be like (n_images, nb_cols, nb_rows, nb_channels)\n",
    "#         self.sources = hkl.load(source_file) # source for each image so when creating sequences can assure that consecutive frames are from same video\n",
    "\n",
    "#         x_matrix = np.empty((31, image_size, image_size, 1))\n",
    "#         for i in range(31):\n",
    "#             x_matrix[i] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, i)).resize((image_size, image_size))).astype(np.int8).reshape((image_size, image_size, 1)) / SCALAR\n",
    "#         y_matrix = np.empty((30, image_size, image_size, 1))\n",
    "#         for i in range(30):\n",
    "#             y_matrix[i] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, i + 31)).resize((image_size, image_size))).astype(np.int8).reshape((image_size, image_size, 1)) / SCALAR\n",
    "        self.X = x_matrix\n",
    "        self.sources = y_matrix\n",
    "        self.nt = nt\n",
    "        self.batch_size = batch_size\n",
    "        self.data_format = data_format\n",
    "        assert sequence_start_mode in {'all', 'unique'}, 'sequence_start_mode must be in {all, unique}'\n",
    "        self.sequence_start_mode = sequence_start_mode\n",
    "        assert output_mode in {'error', 'prediction'}, 'output_mode must be in {error, prediction}'\n",
    "        self.output_mode = output_mode\n",
    "\n",
    "        if self.data_format == 'channels_first':\n",
    "            self.X = np.transpose(self.X, (0, 3, 1, 2))\n",
    "        self.im_shape = self.X[0].shape\n",
    "\n",
    "        if self.sequence_start_mode == 'all':  # allow for any possible sequence, starting from any frame\n",
    "            self.possible_starts = np.array([i for i in range(self.X.shape[0] - self.nt) if self.sources[i] == self.sources[i + self.nt - 1]])\n",
    "        elif self.sequence_start_mode == 'unique':  #create sequences where each unique frame is in at most one sequence\n",
    "            curr_location = 0\n",
    "            possible_starts = []\n",
    "            while curr_location < self.X.shape[0] - self.nt + 1:\n",
    "                if self.sources[curr_location] == self.sources[curr_location + self.nt - 1]:\n",
    "                    possible_starts.append(curr_location)\n",
    "                    curr_location += self.nt\n",
    "                else:\n",
    "                    curr_location += 1\n",
    "            self.possible_starts = possible_starts\n",
    "\n",
    "        if shuffle:\n",
    "            self.possible_starts = np.random.permutation(self.possible_starts)\n",
    "        if N_seq is not None and len(self.possible_starts) > N_seq:  # select a subset of sequences if want to\n",
    "            self.possible_starts = self.possible_starts[:N_seq]\n",
    "        self.N_sequences = len(self.possible_starts)\n",
    "        super(SequenceGenerator, self).__init__(len(self.possible_starts), batch_size, shuffle, seed)\n",
    "\n",
    "    def next(self):\n",
    "        with self.lock:\n",
    "            index_array, current_index, current_batch_size = next(self.index_generator)\n",
    "        batch_x = np.zeros((current_batch_size, self.nt) + self.im_shape, np.float32)\n",
    "        for i, idx in enumerate(index_array):\n",
    "            idx = self.possible_starts[idx]\n",
    "            batch_x[i] = self.preprocess(self.X[idx:idx+self.nt])\n",
    "        if self.output_mode == 'error':  # model outputs errors, so y should be zeros\n",
    "            batch_y = np.zeros(current_batch_size, np.float32)\n",
    "        elif self.output_mode == 'prediction':  # output actual pixels\n",
    "            batch_y = batch_x\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def preprocess(self, X):\n",
    "        return X.astype(np.float32) / 255\n",
    "\n",
    "    def create_all(self):\n",
    "        X_all = np.zeros((self.N_sequences, self.nt) + self.im_shape, np.float32)\n",
    "        for i, idx in enumerate(self.possible_starts):\n",
    "            X_all[i] = self.preprocess(self.X[idx:idx+self.nt])\n",
    "        return X_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAD_id = RAD_id_list[0]\n",
    "SCALAR = 80\n",
    "size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.image_data_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = True  # if weights will be saved\n",
    "weights_file = os.path.join(WEIGHTS_DIR, 'prednet_kitti_weights.hdf5')  # where weights will be saved\n",
    "json_file = os.path.join(WEIGHTS_DIR, 'prednet_kitti_model.json')\n",
    "\n",
    "# Data files\n",
    "train_file = os.path.join(DATA_DIR, 'X_train.hkl')\n",
    "train_sources = os.path.join(DATA_DIR, 'sources_train.hkl')\n",
    "val_file = os.path.join(DATA_DIR, 'X_val.hkl')\n",
    "val_sources = os.path.join(DATA_DIR, 'sources_val.hkl')\n",
    "\n",
    "# Training parameters\n",
    "nb_epoch = 15\n",
    "batch_size = 4\n",
    "samples_per_epoch = 50  # 500\n",
    "N_seq_val = None  # 100  number of sequences to use for validation\n",
    "\n",
    "# Model parameters\n",
    "n_channels, im_height, im_width = (1, image_size, image_size)  # (3, 128, 160)\n",
    "input_shape = (n_channels, im_height, im_width) if K.image_data_format() == 'channels_first' else (im_height, im_width, n_channels)\n",
    "stack_sizes = (n_channels, 48, 96, 192)  # (n_channels, 48, 96, 192)\n",
    "R_stack_sizes = stack_sizes\n",
    "A_filt_sizes = (3, 3, 3)\n",
    "Ahat_filt_sizes = (3, 3, 3, 3)\n",
    "R_filt_sizes = (3, 3, 3, 3)\n",
    "layer_loss_weights = np.array([1., 0., 0., 0.])  # weighting for each layer in final loss; \"L_0\" model:  [1, 0, 0, 0], \"L_all\": [1, 0.1, 0.1, 0.1]\n",
    "layer_loss_weights = np.expand_dims(layer_loss_weights, 1)\n",
    "nt = 30  # number of timesteps used for sequences in training\n",
    "time_loss_weights = 1./ (nt - 1) * np.ones((nt,1))  # equally weight all timesteps except the first\n",
    "time_loss_weights[0] = 0\n",
    "\n",
    "\n",
    "prednet = PredNet(stack_sizes, R_stack_sizes,\n",
    "                  A_filt_sizes, Ahat_filt_sizes, R_filt_sizes, A_activation='elu', error_activation='elu',\n",
    "                  output_mode='prediction', return_sequences=True)\n",
    "\n",
    "inputs = Input(shape=(nt,) + input_shape)\n",
    "outputs = prednet(inputs)\n",
    "# errors = prednet(inputs)  # errors will be (batch_size, nt, nb_layers)\n",
    "# errors_by_time = TimeDistributed(Dense(1, trainable=False), weights=[layer_loss_weights, np.zeros(1)], trainable=False)(errors)  # calculate weighted error by layer\n",
    "# errors_by_time = Flatten()(errors_by_time)  # will be (batch_size, nt)\n",
    "# final_errors = Dense(1, weights=[time_loss_weights, np.zeros(1)], trainable=False)(errors_by_time)  # weight errors by time\n",
    "model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss='mean_absolute_error', optimizer=keras.optimizers.Adam(lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = SequenceGenerator(train_file, train_sources, nt, batch_size=batch_size, shuffle=True)\n",
    "val_generator = SequenceGenerator(val_file, val_sources, nt, batch_size=batch_size, N_seq=N_seq_val)\n",
    "\n",
    "lr_schedule = lambda epoch: 0.001 if epoch < 75 else 0.0001    # start with lr of 0.001 and then drop to 0.0001 after 75 epochs\n",
    "callbacks = [LearningRateScheduler(lr_schedule)]\n",
    "if save_model:\n",
    "    if not os.path.exists(WEIGHTS_DIR): os.mkdir(WEIGHTS_DIR)\n",
    "    callbacks.append(ModelCheckpoint(filepath=weights_file, monitor='val_loss', save_best_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_generator, samples_per_epoch / batch_size, nb_epoch, callbacks=callbacks,\n",
    "                validation_data=val_generator, validation_steps=N_seq_val / batch_size)\n",
    "\n",
    "if save_model:\n",
    "    json_string = model.to_json()\n",
    "    with open(json_file, \"w\") as f:\n",
    "        f.write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAD_id = RAD_id_list[3]\n",
    "x_matrix = np.empty((nt, image_size, image_size, 1))\n",
    "for i in range(nt):\n",
    "    x_matrix[i] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, i)).resize((image_size, image_size))).astype(np.int8).reshape((image_size, image_size, 1)) / SCALAR\n",
    "x_matrix = x_matrix.reshape((1, nt, image_size, image_size, 1))\n",
    "y_matrix = np.empty((nt, image_size, image_size, 1))\n",
    "for i in range(nt):\n",
    "    y_matrix[i] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, i + 10)).resize((image_size, image_size))).astype(np.int8).reshape((image_size, image_size, 1)) / SCALAR\n",
    "y_matrix = y_matrix.reshape((1, nt, image_size, image_size, 1))\n",
    "# a = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, 0)).resize((image_size, image_size))).astype(np.int8).reshape((1, image_size, image_size, 1)) / SCALAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, i in enumerate([10,11,12]):\n",
    "    print(a,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_matrix = np.empty((len(RAD_id_list), nt, image_size, image_size, 1))\n",
    "y_matrix = np.empty((len(RAD_id_list), nt, image_size, image_size, 1))\n",
    "for i, RAD_id in enumerate(RAD_id_list):\n",
    "    for j in range(nt):\n",
    "        x_matrix[i][j] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j)).resize((image_size, image_size))).astype(np.int8).reshape((image_size, image_size, 1)) / SCALAR\n",
    "    for j in range(nt):\n",
    "        y_matrix[i][j] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j + 30)).resize((image_size, image_size))).astype(np.int8).reshape((image_size, image_size, 1)) / SCALAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "model.fit([x_matrix], [y_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.reshape((1, image_size, image_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit([x_matrix], [y_matrix], epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(32,32,32), n_channels=1,\n",
    "                 n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = np.load('data/' + ID + '.npy')\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def myGenerator(keras.utils.Sequence()):\n",
    "#     for RAD_id in RAD_id_list:\n",
    "#         x_matrix = np.empty((nt, image_size, image_size, 1))\n",
    "#         for i in range(nt):\n",
    "#             x_matrix[i] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, i)).resize((image_size, image_size))).astype(np.int8).reshape((image_size, image_size, 1)) / SCALAR\n",
    "#         x_matrix = x_matrix.reshape((1, nt, image_size, image_size, 1))\n",
    "#         y_matrix = np.empty((nt, image_size, image_size, 1))\n",
    "#         for i in range(nt):\n",
    "#             y_matrix[i] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, i + nt)).resize((image_size, image_size))).astype(np.int8).reshape((image_size, image_size, 1)) / SCALAR\n",
    "#         y_matrix = y_matrix.reshape((1, nt, image_size, image_size, 1))\n",
    "#         yield ([x_matrix], [y_matrix])\n",
    "#         # a = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, 0)).resize((image_size, image_size))).astype(np.int8).reshape((1, image_size, image_size, 1)) / SCALAR\n",
    "\n",
    "class myGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, nt, image_size, image_scalar, batch_size=32, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.nt = nt\n",
    "        self.image_size = image_size\n",
    "        self.image_scalar = image_scalar\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        X = np.empty((self.batch_size, self.nt, self.image_size, self.image_size, 1))\n",
    "        y = np.empty((self.batch_size, self.nt, self.image_size, self.image_size, 1))\n",
    "        # Generate data\n",
    "        print(list_IDs_temp)\n",
    "        for i, RAD_id in enumerate(list_IDs_temp):\n",
    "            for j in range(self.nt):\n",
    "                X[i][j] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j)).resize((self.image_size, self.image_size))).astype(np.int8).reshape((self.image_size, self.image_size, 1)) / self.image_scalar\n",
    "                y[i][j] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j + self.nt)).resize((self.image_size, self.image_size))).astype(np.int8).reshape((self.image_size, self.image_size, 1)) / self.image_scalar\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_generator = myGenerator(list_IDs=RAD_id_list[:1], nt=30, image_size=512, image_scalar=80, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(my_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = model.predict([x_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.imshow(y_matrix[0][i].reshape((image_size, image_size)) * SCALAR, cmap=cm.gist_ncar_r)\n",
    "    plt.show()\n",
    "    print('↓↓↓下面的是模型的%d输出，上面的是%d真实值↑↑↑' % (i, i))\n",
    "    plt.imshow(b[0][i].reshape((image_size, image_size)) * SCALAR, cmap=cm.gist_ncar_r)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(a.reshape((image_size, image_size)) * SCALAR, cmap=cm.gist_ncar_r)\n",
    "plt.show()\n",
    "print('↓↓↓下面的是模型的%d输出，上面的是%d真实值↑↑↑' % (1, 1))\n",
    "plt.imshow(b.reshape((image_size, image_size)) * SCALAR, cmap=cm.gist_ncar_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = list(model.layers[0].batch_input_shape[1:])\n",
    "input_shape[0] = nt\n",
    "inputs = Input(shape=tuple(input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_matrix.shape)\n",
    "print(y_matrix.shape)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_input_shape_at(0))\n",
    "print(model.get_output_shape_at(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%quickref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
