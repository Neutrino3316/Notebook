{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "from six.moves import cPickle\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "# from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# from prednet import PredNet\n",
    "# # from data_utils import SequenceGenerator\n",
    "# from kitti_settings import *\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import time\n",
    "import multiprocessing\n",
    "import random\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build time: 2018-09-05 21:53:40\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras import activations\n",
    "from keras.layers import Recurrent\n",
    "from keras.layers import Conv2D, UpSampling2D, MaxPooling2D\n",
    "from keras.engine import InputSpec\n",
    "# from keras_utils import legacy_prednet_support\n",
    "\n",
    "class PredNet(Recurrent):\n",
    "    '''PredNet architecture - Lotter 2016.\n",
    "        Stacked convolutional LSTM inspired by predictive coding principles.\n",
    "\n",
    "    # Arguments\n",
    "        stack_sizes: number of channels in targets (A) and predictions (Ahat) in each layer of the architecture.\n",
    "            Length is the number of layers in the architecture.\n",
    "            First element is the number of channels in the input.\n",
    "            Ex. (3, 16, 32) would correspond to a 3 layer architecture that takes in RGB images and has 16 and 32\n",
    "                channels in the second and third layers, respectively.\n",
    "        R_stack_sizes: number of channels in the representation (R) modules.\n",
    "            Length must equal length of stack_sizes, but the number of channels per layer can be different.\n",
    "        A_filt_sizes: filter sizes for the target (A) modules.\n",
    "            Has length of 1 - len(stack_sizes).\n",
    "            Ex. (3, 3) would mean that targets for layers 2 and 3 are computed by a 3x3 convolution of the errors (E)\n",
    "                from the layer below (followed by max-pooling)\n",
    "        Ahat_filt_sizes: filter sizes for the prediction (Ahat) modules.\n",
    "            Has length equal to length of stack_sizes.\n",
    "            Ex. (3, 3, 3) would mean that the predictions for each layer are computed by a 3x3 convolution of the\n",
    "                representation (R) modules at each layer.\n",
    "        R_filt_sizes: filter sizes for the representation (R) modules.\n",
    "            Has length equal to length of stack_sizes.\n",
    "            Corresponds to the filter sizes for all convolutions in the LSTM.\n",
    "        pixel_max: the maximum pixel value.\n",
    "            Used to clip the pixel-layer prediction.\n",
    "        error_activation: activation function for the error (E) units.\n",
    "        A_activation: activation function for the target (A) and prediction (A_hat) units.\n",
    "        LSTM_activation: activation function for the cell and hidden states of the LSTM.\n",
    "        LSTM_inner_activation: activation function for the gates in the LSTM.\n",
    "        output_mode: either 'error', 'prediction', 'all' or layer specification (ex. R2, see below).\n",
    "            Controls what is outputted by the PredNet.\n",
    "            If 'error', the mean response of the error (E) units of each layer will be outputted.\n",
    "                That is, the output shape will be (batch_size, nb_layers).\n",
    "            If 'prediction', the frame prediction will be outputted.\n",
    "            If 'all', the output will be the frame prediction concatenated with the mean layer errors.\n",
    "                The frame prediction is flattened before concatenation.\n",
    "                Nomenclature of 'all' is kept for backwards compatibility, but should not be confused with returning all of the layers of the model\n",
    "            For returning the features of a particular layer, output_mode should be of the form unit_type + layer_number.\n",
    "                For instance, to return the features of the LSTM \"representational\" units in the lowest layer, output_mode should be specificied as 'R0'.\n",
    "                The possible unit types are 'R', 'Ahat', 'A', and 'E' corresponding to the 'representation', 'prediction', 'target', and 'error' units respectively.\n",
    "        extrap_start_time: time step for which model will start extrapolating.\n",
    "            Starting at this time step, the prediction from the previous time step will be treated as the \"actual\"\n",
    "        data_format: 'channels_first' or 'channels_last'.\n",
    "            It defaults to the `image_data_format` value found in your\n",
    "            Keras config file at `~/.keras/keras.json`.\n",
    "\n",
    "    # References\n",
    "        - [Deep predictive coding networks for video prediction and unsupervised learning](https://arxiv.org/abs/1605.08104)\n",
    "        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "        - [Convolutional LSTM network: a machine learning approach for precipitation nowcasting](http://arxiv.org/abs/1506.04214)\n",
    "        - [Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects](http://www.nature.com/neuro/journal/v2/n1/pdf/nn0199_79.pdf)\n",
    "    '''\n",
    "#     @legacy_prednet_support\n",
    "    def __init__(self, stack_sizes, R_stack_sizes,\n",
    "                 A_filt_sizes, Ahat_filt_sizes, R_filt_sizes,\n",
    "                 pixel_max=1., error_activation='relu', A_activation='relu',\n",
    "                 LSTM_activation='tanh', LSTM_inner_activation='hard_sigmoid',\n",
    "                 conv_dropout=0.8,\n",
    "                 output_mode='error', extrap_start_time=None,\n",
    "                 data_format=K.image_data_format(), **kwargs):\n",
    "        self.stack_sizes = stack_sizes\n",
    "        self.nb_layers = len(stack_sizes)\n",
    "        assert len(R_stack_sizes) == self.nb_layers, 'len(R_stack_sizes) must equal len(stack_sizes)'\n",
    "        self.R_stack_sizes = R_stack_sizes\n",
    "        assert len(A_filt_sizes) == (self.nb_layers - 1), 'len(A_filt_sizes) must equal len(stack_sizes) - 1'\n",
    "        self.A_filt_sizes = A_filt_sizes\n",
    "        assert len(Ahat_filt_sizes) == self.nb_layers, 'len(Ahat_filt_sizes) must equal len(stack_sizes)'\n",
    "        self.Ahat_filt_sizes = Ahat_filt_sizes\n",
    "        assert len(R_filt_sizes) == (self.nb_layers), 'len(R_filt_sizes) must equal len(stack_sizes)'\n",
    "        self.R_filt_sizes = R_filt_sizes\n",
    "\n",
    "        self.pixel_max = pixel_max\n",
    "        self.error_activation = activations.get(error_activation)\n",
    "        self.A_activation = activations.get(A_activation)\n",
    "        self.LSTM_activation = activations.get(LSTM_activation)\n",
    "        self.LSTM_inner_activation = activations.get(LSTM_inner_activation)\n",
    "        self.conv_dropout = conv_dropout\n",
    "\n",
    "        default_output_modes = ['prediction', 'error', 'all']\n",
    "        layer_output_modes = [layer + str(n) for n in range(self.nb_layers) for layer in ['R', 'E', 'A', 'Ahat']]\n",
    "        assert output_mode in default_output_modes + layer_output_modes, 'Invalid output_mode: ' + str(output_mode)\n",
    "        self.output_mode = output_mode\n",
    "        if self.output_mode in layer_output_modes:\n",
    "            self.output_layer_type = self.output_mode[:-1]\n",
    "            self.output_layer_num = int(self.output_mode[-1])\n",
    "        else:\n",
    "            self.output_layer_type = None\n",
    "            self.output_layer_num = None\n",
    "        self.extrap_start_time = extrap_start_time\n",
    "\n",
    "        assert data_format in {'channels_last', 'channels_first'}, 'data_format must be in {channels_last, channels_first}'\n",
    "        self.data_format = data_format\n",
    "        self.channel_axis = -3 if data_format == 'channels_first' else -1\n",
    "        self.row_axis = -2 if data_format == 'channels_first' else -3\n",
    "        self.column_axis = -1 if data_format == 'channels_first' else -2\n",
    "        super(PredNet, self).__init__(**kwargs)\n",
    "        self.input_spec = [InputSpec(ndim=5)]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.output_mode == 'prediction':\n",
    "            out_shape = input_shape[2:]\n",
    "        elif self.output_mode == 'error':\n",
    "            out_shape = (self.nb_layers,)\n",
    "        elif self.output_mode == 'all':\n",
    "            out_shape = (np.prod(input_shape[2:]) + self.nb_layers,)\n",
    "        else:\n",
    "            stack_str = 'R_stack_sizes' if self.output_layer_type == 'R' else 'stack_sizes'\n",
    "            stack_mult = 2 if self.output_layer_type == 'E' else 1\n",
    "            out_stack_size = stack_mult * getattr(self, stack_str)[self.output_layer_num]\n",
    "            out_nb_row = input_shape[self.row_axis] / 2**self.output_layer_num\n",
    "            out_nb_col = input_shape[self.column_axis] / 2**self.output_layer_num\n",
    "            if self.data_format == 'channels_first':\n",
    "                out_shape = (out_stack_size, out_nb_row, out_nb_col)\n",
    "            else:\n",
    "                out_shape = (out_nb_row, out_nb_col, out_stack_size)\n",
    "\n",
    "        if self.return_sequences:\n",
    "            return (input_shape[0], input_shape[1]) + out_shape\n",
    "        else:\n",
    "            return (input_shape[0],) + out_shape\n",
    "\n",
    "    def get_initial_state(self, x):\n",
    "        input_shape = self.input_spec[0].shape\n",
    "        init_nb_row = input_shape[self.row_axis]\n",
    "        init_nb_col = input_shape[self.column_axis]\n",
    "\n",
    "        base_initial_state = K.zeros_like(x)  # (samples, timesteps) + image_shape\n",
    "        non_channel_axis = -1 if self.data_format == 'channels_first' else -2\n",
    "        for _ in range(2):\n",
    "            base_initial_state = K.sum(base_initial_state, axis=non_channel_axis)\n",
    "        base_initial_state = K.sum(base_initial_state, axis=1)  # (samples, nb_channels)\n",
    "\n",
    "        initial_states = []\n",
    "        states_to_pass = ['r', 'c', 'e']\n",
    "        nlayers_to_pass = {u: self.nb_layers for u in states_to_pass}\n",
    "        if self.extrap_start_time is not None:\n",
    "            states_to_pass.append('ahat')  # pass prediction in states so can use as actual for t+1 when extrapolating\n",
    "            nlayers_to_pass['ahat'] = 1\n",
    "        for u in states_to_pass:\n",
    "            for l in range(nlayers_to_pass[u]):\n",
    "                ds_factor = 2 ** l\n",
    "                nb_row = init_nb_row // ds_factor\n",
    "                nb_col = init_nb_col // ds_factor\n",
    "                if u in ['r', 'c']:\n",
    "                    stack_size = self.R_stack_sizes[l]\n",
    "                elif u == 'e':\n",
    "                    stack_size = 2 * self.stack_sizes[l]\n",
    "                elif u == 'ahat':\n",
    "                    stack_size = self.stack_sizes[l]\n",
    "                output_size = stack_size * nb_row * nb_col  # flattened size\n",
    "\n",
    "                reducer = K.zeros((input_shape[self.channel_axis], output_size)) # (nb_channels, output_size)\n",
    "                initial_state = K.dot(base_initial_state, reducer) # (samples, output_size)\n",
    "                if self.data_format == 'channels_first':\n",
    "                    output_shp = (-1, stack_size, nb_row, nb_col)\n",
    "                else:\n",
    "                    output_shp = (-1, nb_row, nb_col, stack_size)\n",
    "                initial_state = K.reshape(initial_state, output_shp)\n",
    "                initial_states += [initial_state]\n",
    "\n",
    "        if K._BACKEND == 'theano':\n",
    "            from theano import tensor as T\n",
    "            # There is a known issue in the Theano scan op when dealing with inputs whose shape is 1 along a dimension.\n",
    "            # In our case, this is a problem when training on grayscale images, and the below line fixes it.\n",
    "            initial_states = [T.unbroadcast(init_state, 0, 1) for init_state in initial_states]\n",
    "\n",
    "        if self.extrap_start_time is not None:\n",
    "            initial_states += [K.variable(0, int if K.backend() != 'tensorflow' else 'int32')]  # the last state will correspond to the current timestep\n",
    "        return initial_states\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.conv_layers = {c: [] for c in ['i', 'f', 'c', 'o', 'a', 'ahat']}\n",
    "\n",
    "        for l in range(self.nb_layers):\n",
    "            for c in ['i', 'f', 'c', 'o']:\n",
    "                act = self.LSTM_activation if c == 'c' else self.LSTM_inner_activation\n",
    "                self.conv_layers[c].append(Conv2D(self.R_stack_sizes[l], self.R_filt_sizes[l], padding='same', activation=act, data_format=self.data_format))\n",
    "#                 self.conv_layers[c].append(keras.layers.Dropout(self.conv_dropout))\n",
    "\n",
    "            act = 'relu' if l == 0 else self.A_activation\n",
    "            self.conv_layers['ahat'].append(Conv2D(self.stack_sizes[l], self.Ahat_filt_sizes[l], padding='same', activation=act, data_format=self.data_format))\n",
    "\n",
    "            if l < self.nb_layers - 1:\n",
    "                self.conv_layers['a'].append(Conv2D(self.stack_sizes[l+1], self.A_filt_sizes[l], padding='same', activation=self.A_activation, data_format=self.data_format))\n",
    "\n",
    "        self.upsample = UpSampling2D(data_format=self.data_format)\n",
    "        self.pool = MaxPooling2D(data_format=self.data_format)\n",
    "\n",
    "        self.trainable_weights = []\n",
    "        nb_row, nb_col = (input_shape[-2], input_shape[-1]) if self.data_format == 'channels_first' else (input_shape[-3], input_shape[-2])\n",
    "        for c in sorted(self.conv_layers.keys()):\n",
    "            for l in range(len(self.conv_layers[c])):\n",
    "                ds_factor = 2 ** l\n",
    "                if c == 'ahat':\n",
    "                    nb_channels = self.R_stack_sizes[l]\n",
    "                elif c == 'a':\n",
    "                    nb_channels = 2 * self.R_stack_sizes[l]\n",
    "                else:\n",
    "                    nb_channels = self.stack_sizes[l] * 2 + self.R_stack_sizes[l]\n",
    "                    if l < self.nb_layers - 1:\n",
    "                        nb_channels += self.R_stack_sizes[l+1]\n",
    "                in_shape = (input_shape[0], nb_channels, nb_row // ds_factor, nb_col // ds_factor)\n",
    "                if self.data_format == 'channels_last': in_shape = (in_shape[0], in_shape[2], in_shape[3], in_shape[1])\n",
    "                with K.name_scope('layer_' + c + '_' + str(l)):\n",
    "                    self.conv_layers[c][l].build(in_shape)\n",
    "                self.trainable_weights += self.conv_layers[c][l].trainable_weights\n",
    "\n",
    "        self.states = [None] * self.nb_layers*3\n",
    "\n",
    "        if self.extrap_start_time is not None:\n",
    "            self.t_extrap = K.variable(self.extrap_start_time, int if K.backend() != 'tensorflow' else 'int32')\n",
    "            self.states += [None] * 2  # [previous frame prediction, timestep]\n",
    "\n",
    "    def step(self, a, states):\n",
    "        r_tm1 = states[:self.nb_layers]\n",
    "        c_tm1 = states[self.nb_layers:2*self.nb_layers]\n",
    "        e_tm1 = states[2*self.nb_layers:3*self.nb_layers]\n",
    "\n",
    "        if self.extrap_start_time is not None:\n",
    "            t = states[-1]\n",
    "            a = K.switch(t >= self.t_extrap, states[-2], a)  # if past self.extrap_start_time, the previous prediction will be treated as the actual\n",
    "\n",
    "        c = []\n",
    "        r = []\n",
    "        e = []\n",
    "\n",
    "        # Update R units starting from the top\n",
    "        for l in reversed(range(self.nb_layers)):\n",
    "            inputs = [r_tm1[l], e_tm1[l]]\n",
    "            if l < self.nb_layers - 1:\n",
    "                inputs.append(r_up)\n",
    "\n",
    "            inputs = K.concatenate(inputs, axis=self.channel_axis)\n",
    "            i = self.conv_layers['i'][l].call(inputs)\n",
    "            f = self.conv_layers['f'][l].call(inputs)\n",
    "            o = self.conv_layers['o'][l].call(inputs)\n",
    "            _c = f * c_tm1[l] + i * self.conv_layers['c'][l].call(inputs)\n",
    "            _r = o * self.LSTM_activation(_c)\n",
    "            c.insert(0, _c)\n",
    "            r.insert(0, _r)\n",
    "\n",
    "            if l > 0:\n",
    "                r_up = self.upsample.call(_r)\n",
    "\n",
    "        # Update feedforward path starting from the bottom\n",
    "        for l in range(self.nb_layers):\n",
    "            ahat = self.conv_layers['ahat'][l].call(r[l])\n",
    "            if l == 0:\n",
    "                ahat = K.minimum(ahat, self.pixel_max)\n",
    "                frame_prediction = ahat\n",
    "\n",
    "            # compute errors\n",
    "            e_up = self.error_activation(ahat - a)\n",
    "            e_down = self.error_activation(a - ahat)\n",
    "\n",
    "            e.append(K.concatenate((e_up, e_down), axis=self.channel_axis))\n",
    "\n",
    "            if self.output_layer_num == l:\n",
    "                if self.output_layer_type == 'A':\n",
    "                    output = a\n",
    "                elif self.output_layer_type == 'Ahat':\n",
    "                    output = ahat\n",
    "                elif self.output_layer_type == 'R':\n",
    "                    output = r[l]\n",
    "                elif self.output_layer_type == 'E':\n",
    "                    output = e[l]\n",
    "\n",
    "            if l < self.nb_layers - 1:\n",
    "                a = self.conv_layers['a'][l].call(e[l])\n",
    "                a = self.pool.call(a)  # target for next layer\n",
    "\n",
    "        if self.output_layer_type is None:\n",
    "            if self.output_mode == 'prediction':\n",
    "                output = frame_prediction\n",
    "            else:\n",
    "                for l in range(self.nb_layers):\n",
    "                    layer_error = K.mean(K.batch_flatten(e[l]), axis=-1, keepdims=True)\n",
    "                    all_error = layer_error if l == 0 else K.concatenate((all_error, layer_error), axis=-1)\n",
    "                if self.output_mode == 'error':\n",
    "                    output = all_error\n",
    "                else:\n",
    "                    output = K.concatenate((K.batch_flatten(frame_prediction), all_error), axis=-1)\n",
    "\n",
    "        states = r + c + e\n",
    "        if self.extrap_start_time is not None:\n",
    "            states += [frame_prediction, t + 1]\n",
    "        return output, states\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'stack_sizes': self.stack_sizes,\n",
    "                  'R_stack_sizes': self.R_stack_sizes,\n",
    "                  'A_filt_sizes': self.A_filt_sizes,\n",
    "                  'Ahat_filt_sizes': self.Ahat_filt_sizes,\n",
    "                  'R_filt_sizes': self.R_filt_sizes,\n",
    "                  'pixel_max': self.pixel_max,\n",
    "                  'error_activation': self.error_activation.__name__,\n",
    "                  'A_activation': self.A_activation.__name__,\n",
    "                  'LSTM_activation': self.LSTM_activation.__name__,\n",
    "                  'LSTM_inner_activation': self.LSTM_inner_activation.__name__,\n",
    "                  'data_format': self.data_format,\n",
    "                  'extrap_start_time': self.extrap_start_time,\n",
    "                  'output_mode': self.output_mode}\n",
    "        base_config = super(PredNet, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "print(time.strftime('build time: %Y-%m-%d %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n"
     ]
    }
   ],
   "source": [
    "raw_RAD_id_list = os.listdir('/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/')\n",
    "print(len(raw_RAD_id_list))\n",
    "RAD_id_list = raw_RAD_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_RAD_id(RAD_id):\n",
    "    mean_list = []\n",
    "    for k in range(61):\n",
    "        mean_list.append(np.array(PIL.Image.open('/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png'\n",
    "                         % (RAD_id, RAD_id,\n",
    "                        k))).astype(np.int8).ravel().mean())\n",
    "    mean_list = np.array(mean_list)\n",
    "    if mean_list.mean() < -0.5:\n",
    "        return None\n",
    "    for k in range(59):\n",
    "        if abs(mean_list[k] + mean_list[k + 2] - 2 * mean_list[k + 1]) > 2:\n",
    "            return None\n",
    "    return RAD_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 2018-09-05 21:53:41\n",
      "end time: 2018-09-05 21:54:40\n",
      "00:00:58\n",
      "6562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-11:\n",
      "Process ForkPoolWorker-22:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-25:\n",
      "Process ForkPoolWorker-20:\n",
      "Process ForkPoolWorker-17:\n",
      "Process ForkPoolWorker-24:\n",
      "Process ForkPoolWorker-18:\n",
      "Process ForkPoolWorker-27:\n",
      "Process ForkPoolWorker-23:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-21:\n",
      "Process ForkPoolWorker-32:\n",
      "Process ForkPoolWorker-13:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-26:\n",
      "Process ForkPoolWorker-29:\n",
      "Process ForkPoolWorker-19:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-31:\n",
      "Process ForkPoolWorker-15:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-28:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-30:\n",
      "Process ForkPoolWorker-9:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-12:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-4:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "pool = multiprocessing.Pool()\n",
    "start_time = time.time()\n",
    "print(time.strftime('start time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "# map(check_RAD_id, raw_RAD_id_list[:100])\n",
    "# print(time.strftime('start time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "RAD_id_list = list(pool.map(check_RAD_id, raw_RAD_id_list))\n",
    "RAD_id_list = [x for x in RAD_id_list if x is not None]\n",
    "print(time.strftime('end time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "print(time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time)))\n",
    "print(len(RAD_id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build time: 2018-09-05 21:54:40\n"
     ]
    }
   ],
   "source": [
    "step_size = 5\n",
    "\n",
    "class trainGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, nt, image_size, image_scalar, batch_size=32, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.nt = nt\n",
    "        self.image_size = image_size\n",
    "        self.image_scalar = image_scalar\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        X = np.empty((self.batch_size, self.nt, self.image_size, self.image_size, 1))\n",
    "#         y = np.empty((self.batch_size, self.image_size, self.image_size, 1))\n",
    "        # Generate data\n",
    "#         print(list_IDs_temp)\n",
    "        for i, RAD_id in enumerate(list_IDs_temp):\n",
    "            offset = random.randint(0, 61 - self.nt * step_size)\n",
    "            offset = random.randint(1, 60 - self.nt * step_size)\n",
    "            for j in range(self.nt):\n",
    "#                 X[i][j] = (np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset)).resize((self.image_size, self.image_size))).astype(np.int8).reshape((self.image_size, self.image_size, 1)) / self.image_scalar + \n",
    "#                     np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset + 1)).resize((self.image_size, self.image_size))).astype(np.int8).reshape((self.image_size, self.image_size, 1)) / self.image_scalar + \n",
    "#                     np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset - 1)).resize((self.image_size, self.image_size))).astype(np.int8).reshape((self.image_size, self.image_size, 1)) / self.image_scalar) / 3\n",
    "                X[i][j] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset)).resize((self.image_size, self.image_size))).astype(np.int8).reshape((self.image_size, self.image_size, 1)) / self.image_scalar\n",
    "#             y[i] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, (self.nt) * 1)).resize((self.image_size, self.image_size))).astype(np.int8).reshape((self.image_size, self.image_size, 1)) / self.image_scalar\n",
    "        y = np.zeros(self.batch_size, np.float32)\n",
    "        return X, y\n",
    "print(time.strftime('build time: %Y-%m-%d %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build time: 2018-09-05 21:54:42\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "image_size = 256\n",
    "nt = 6 # number of timesteps used for sequences in training\n",
    "image_scalar = 80\n",
    "vmin = -1\n",
    "vmax = 0.6 * image_scalar\n",
    "n_channels, im_height, im_width = (1, image_size, image_size)  # (3, 128, 160)\n",
    "input_shape = (n_channels, im_height, im_width) if K.image_data_format() == 'channels_first' else (im_height, im_width, n_channels)\n",
    "stack_sizes = (n_channels, 48, 96, 192)\n",
    "R_stack_sizes = stack_sizes\n",
    "A_filt_sizes = (3, 3, 3)\n",
    "Ahat_filt_sizes = (3, 3, 3, 3)\n",
    "R_filt_sizes = (3, 3, 3, 3)\n",
    "layer_loss_weights = np.array([1., 0., 0., 0.])  # weighting for each layer in final loss; \"L_0\" model:  [1, 0, 0, 0], \"L_all\": [1, 0.1, 0.1, 0.1]\n",
    "layer_loss_weights = np.expand_dims(layer_loss_weights, 1)\n",
    "time_loss_weights = 1./ (nt - 1) * np.ones((nt,1))  # equally weight all timesteps except the first\n",
    "time_loss_weights[0] = 0\n",
    "\n",
    "\n",
    "prednet = PredNet(stack_sizes, R_stack_sizes,\n",
    "                  A_filt_sizes, Ahat_filt_sizes, R_filt_sizes,\n",
    "                  A_activation=tf.nn.elu, error_activation=tf.nn.elu,\n",
    "                  LSTM_inner_activation='sigmoid',\n",
    "                  output_mode='error', return_sequences=True, extrap_start_time=3)\n",
    "\n",
    "inputs = Input(shape=(nt,) + input_shape)\n",
    "errors = prednet(inputs)  # errors will be (batch_size, nt, nb_layers)\n",
    "errors_by_time = TimeDistributed(Dense(1, trainable=False), weights=[layer_loss_weights, np.zeros(1)], trainable=False)(errors)  # calculate weighted error by layer\n",
    "errors_by_time = Flatten()(errors_by_time)  # will be (batch_size, nt)\n",
    "final_errors = Dense(1, weights=[time_loss_weights, np.zeros(1)], trainable=False)(errors_by_time)  # weight errors by time\n",
    "outputs = final_errors\n",
    "# outputs = np.empty(shape=(nt,) + input_shape)\n",
    "# for i in range(nt):\n",
    "#     temp_outputs = prednet(inputs)\n",
    "#     outputs[i] = temp_outputs\n",
    "#     for j in range(nt - 1):\n",
    "#         inputs[j] = inputs[j + 1]\n",
    "#     inputs[-1] = temp_outputs\n",
    "\n",
    "def my_loss(y_true, y_pred):\n",
    "#     w = tf.add(y_true, tf.constant(0.5))\n",
    "#     w = tf.add(y_pred, w)\n",
    "#     loss = tf.losses.mean_squared_error(y_true, y_pred, weights=w)\n",
    "#     loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "    loss = tf.losses.absolute_difference(y_true, y_pred)\n",
    "#     loss = tf.multiply(loss, tf.constant(1000000000.0))\n",
    "    return loss\n",
    "\n",
    "model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss=my_loss, optimizer=keras.optimizers.Adam())\n",
    "print(time.strftime('build time: %Y-%m-%d %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 6, 256, 256, 1)    0         \n",
      "_________________________________________________________________\n",
      "pred_net_1 (PredNet)         (None, 6, 4)              6909818   \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 6, 1)              5         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 6,909,830\n",
      "Trainable params: 6,909,818\n",
      "Non-trainable params: 12\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to use tensorboard\n",
    "\n",
    "```bash\n",
    "jupyter notebook --ip=192.168.2.101\n",
    "python3 /usr/local/lib/python3.5/dist-packages/tensorboard/main.py --logdir='/home/hadoop/Documents/Neutrino/tensorboard_logs' --host=192.168.2.101 --port=8900\n",
    "rm -rf /home/hadoop/Documents/Neutrino/tensorboard_logs\n",
    "scp \"C:/Users/Administrator/Downloads/SRAD2018_Test_1.zip\" hadoop@192.168.1.115:~/Documents/Neutrino/SRAD2018/SRAD2018_test\n",
    "mv -v ~/Documents/Neutrino/SRAD2018/SRAD2018_test/SRAD2018_Test_1* ~/Documents/Neutrino/SRAD2018/SRAD2018_test\n",
    "lsof /dev/nvidia0\n",
    "lsof -n -i4TCP:8888\n",
    "kill -9 -[PID]\n",
    "```\n",
    "\n",
    "`http://222.200.177.32:8900/#scalars&run=.&_smoothingWeight=0.8`\n",
    "\n",
    "`http://222.200.177.32:8888`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = lambda epoch: 0.001 if epoch < 20 else 0.0003    # start with lr of 0.001 and then drop to 0.0001 after 75 epochs\n",
    "tb_callback = keras.callbacks.TensorBoard(log_dir='/home/hadoop/Documents/Neutrino/tensorboard_logs', histogram_freq=0, write_graph=False)\n",
    "callbacks = [LearningRateScheduler(lr_schedule), tb_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(time.strftime('start time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "train_generator = trainGenerator(list_IDs=RAD_id_list[-4:-3], nt=nt, image_size=image_size, image_scalar=image_scalar, batch_size=1)\n",
    "valid_generator = trainGenerator(list_IDs=RAD_id_list[:12], nt=nt, image_size=image_size, image_scalar=image_scalar, batch_size=1, shuffle=False)\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=30, epochs=50, validation_data=valid_generator, validation_steps=12, use_multiprocessing=True, max_queue_size=20, callbacks=callbacks)\n",
    "print(time.strftime('end time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "print(time.strftime('It took  %H:%M:%S', time.gmtime(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = lambda epoch: 0.0003 if epoch < 20 else 0.00001    # start with lr of 0.001 and then drop to 0.0001 after 75 epochs\n",
    "tb_callback = keras.callbacks.TensorBoard(log_dir='/home/hadoop/Documents/Neutrino/tensorboard_logs', histogram_freq=0, write_graph=False)\n",
    "callbacks = [LearningRateScheduler(lr_schedule), tb_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 2018-09-05 21:55:39\n",
      "Epoch 1/200\n",
      "100/100 [==============================] - 34s 336ms/step - loss: 5.0468e-04 - val_loss: 3.7485e-04\n",
      "Epoch 2/200\n",
      "100/100 [==============================] - 31s 306ms/step - loss: 2.9860e-04 - val_loss: 3.6277e-04\n",
      "Epoch 3/200\n",
      "100/100 [==============================] - 31s 309ms/step - loss: 3.0705e-04 - val_loss: 2.9159e-04\n",
      "Epoch 4/200\n",
      "100/100 [==============================] - 32s 318ms/step - loss: 2.9143e-04 - val_loss: 3.2253e-04\n",
      "Epoch 5/200\n",
      "100/100 [==============================] - 32s 316ms/step - loss: 2.7724e-04 - val_loss: 3.6792e-04\n",
      "Epoch 6/200\n",
      "100/100 [==============================] - 32s 315ms/step - loss: 2.9011e-04 - val_loss: 2.7588e-04\n",
      "Epoch 7/200\n",
      "100/100 [==============================] - 32s 315ms/step - loss: 2.9984e-04 - val_loss: 2.7435e-04\n",
      "Epoch 8/200\n",
      "100/100 [==============================] - 32s 316ms/step - loss: 2.9337e-04 - val_loss: 2.5122e-04\n",
      "Epoch 9/200\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 2.4600e-04 - val_loss: 2.7510e-04\n",
      "Epoch 10/200\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 2.7142e-04 - val_loss: 2.3898e-04\n",
      "Epoch 11/200\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 2.3363e-04 - val_loss: 2.4941e-04\n",
      "Epoch 12/200\n",
      "100/100 [==============================] - 31s 312ms/step - loss: 2.4937e-04 - val_loss: 2.5940e-04\n",
      "Epoch 13/200\n",
      "100/100 [==============================] - 31s 312ms/step - loss: 2.4490e-04 - val_loss: 2.5716e-04\n",
      "Epoch 14/200\n",
      "100/100 [==============================] - 31s 314ms/step - loss: 2.6764e-04 - val_loss: 2.4484e-04\n",
      "Epoch 15/200\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 2.6820e-04 - val_loss: 2.4474e-04\n",
      "Epoch 16/200\n",
      "100/100 [==============================] - 31s 310ms/step - loss: 2.5953e-04 - val_loss: 2.4708e-04\n",
      "Epoch 17/200\n",
      "100/100 [==============================] - 31s 310ms/step - loss: 2.5487e-04 - val_loss: 2.5503e-04\n",
      "Epoch 18/200\n",
      "100/100 [==============================] - 31s 308ms/step - loss: 2.4250e-04 - val_loss: 2.4987e-04\n",
      "Epoch 19/200\n",
      "100/100 [==============================] - 31s 309ms/step - loss: 2.4550e-04 - val_loss: 2.5414e-04\n",
      "Epoch 20/200\n",
      "100/100 [==============================] - 31s 310ms/step - loss: 2.3969e-04 - val_loss: 2.4342e-04\n",
      "Epoch 21/200\n",
      "100/100 [==============================] - 31s 310ms/step - loss: 2.6729e-04 - val_loss: 2.3585e-04\n",
      "Epoch 22/200\n",
      "100/100 [==============================] - 31s 307ms/step - loss: 2.1780e-04 - val_loss: 2.4216e-04\n",
      "Epoch 23/200\n",
      "100/100 [==============================] - 31s 309ms/step - loss: 2.6727e-04 - val_loss: 2.2533e-04\n",
      "Epoch 24/200\n",
      "100/100 [==============================] - 31s 308ms/step - loss: 2.4949e-04 - val_loss: 2.2702e-04\n",
      "Epoch 25/200\n",
      "100/100 [==============================] - 31s 310ms/step - loss: 2.5011e-04 - val_loss: 2.3229e-04\n",
      "Epoch 26/200\n",
      "100/100 [==============================] - 31s 306ms/step - loss: 2.3292e-04 - val_loss: 2.3253e-04\n",
      "Epoch 27/200\n",
      "100/100 [==============================] - 31s 310ms/step - loss: 2.3747e-04 - val_loss: 2.4817e-04\n",
      "Epoch 28/200\n",
      "100/100 [==============================] - 31s 308ms/step - loss: 2.7453e-04 - val_loss: 2.2930e-04\n",
      "Epoch 29/200\n",
      "100/100 [==============================] - 31s 307ms/step - loss: 2.1791e-04 - val_loss: 2.2721e-04\n",
      "Epoch 30/200\n",
      "100/100 [==============================] - 31s 308ms/step - loss: 2.5553e-04 - val_loss: 2.4986e-04\n",
      "Epoch 31/200\n",
      "100/100 [==============================] - 31s 306ms/step - loss: 2.2796e-04 - val_loss: 2.3686e-04\n",
      "Epoch 32/200\n",
      "100/100 [==============================] - 37s 370ms/step - loss: 2.3107e-04 - val_loss: 2.2510e-04\n",
      "Epoch 33/200\n",
      "100/100 [==============================] - 34s 340ms/step - loss: 2.2263e-04 - val_loss: 2.2540e-04\n",
      "Epoch 34/200\n",
      "100/100 [==============================] - 31s 311ms/step - loss: 2.3908e-04 - val_loss: 2.3890e-04\n",
      "Epoch 35/200\n",
      "100/100 [==============================] - 31s 310ms/step - loss: 2.1282e-04 - val_loss: 2.3565e-04\n",
      "Epoch 36/200\n",
      "100/100 [==============================] - 31s 312ms/step - loss: 2.6041e-04 - val_loss: 2.4891e-04\n",
      "Epoch 37/200\n",
      "100/100 [==============================] - 31s 309ms/step - loss: 2.2313e-04 - val_loss: 2.3181e-04\n",
      "Epoch 38/200\n",
      "100/100 [==============================] - 31s 310ms/step - loss: 2.5457e-04 - val_loss: 2.2583e-04\n",
      "Epoch 39/200\n",
      "100/100 [==============================] - 31s 309ms/step - loss: 2.2938e-04 - val_loss: 2.2602e-04\n",
      "Epoch 40/200\n",
      "100/100 [==============================] - 31s 307ms/step - loss: 2.5223e-04 - val_loss: 2.2321e-04\n",
      "Epoch 41/200\n",
      "100/100 [==============================] - 31s 308ms/step - loss: 2.3612e-04 - val_loss: 2.3409e-04\n",
      "Epoch 42/200\n",
      "100/100 [==============================] - 31s 310ms/step - loss: 2.3822e-04 - val_loss: 2.3065e-04\n",
      "Epoch 43/200\n",
      "100/100 [==============================] - 31s 307ms/step - loss: 2.3090e-04 - val_loss: 2.4121e-04\n",
      "Epoch 44/200\n",
      "100/100 [==============================] - 31s 308ms/step - loss: 2.4174e-04 - val_loss: 2.2395e-04\n",
      "Epoch 45/200\n",
      "100/100 [==============================] - 31s 308ms/step - loss: 2.2910e-04 - val_loss: 2.3608e-04\n",
      "Epoch 46/200\n",
      "100/100 [==============================] - 31s 307ms/step - loss: 2.2248e-04 - val_loss: 2.4343e-04\n",
      "Epoch 47/200\n",
      "100/100 [==============================] - 31s 306ms/step - loss: 2.3487e-04 - val_loss: 2.3381e-04\n",
      "Epoch 48/200\n",
      " 12/100 [==>...........................] - ETA: 25s - loss: 1.3167e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-341c0af36a57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_IDs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRAD_id_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_scalar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_scalar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvalid_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_IDs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRAD_id_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_scalar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_scalar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'end time: %Y-%m-%d %H:%M:%S'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocaltime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'It took  %H:%M:%S'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgmtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2670\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2671\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2672\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2652\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2654\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2655\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(time.strftime('start time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "start_time = time.time()\n",
    "train_generator = trainGenerator(list_IDs=RAD_id_list[:6000], nt=nt, image_size=image_size, image_scalar=image_scalar, batch_size=1)\n",
    "valid_generator = trainGenerator(list_IDs=RAD_id_list[-20:], nt=nt, image_size=image_size, image_scalar=image_scalar, batch_size=1)\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=100, epochs=200, validation_data=valid_generator, validation_steps=20, callbacks=callbacks)\n",
    "print(time.strftime('end time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "print(time.strftime('It took  %H:%M:%S', time.gmtime(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime('start time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "start_time = time.time()\n",
    "train_generator = trainGenerator(list_IDs=RAD_id_list[:6000], nt=nt, image_size=image_size, image_scalar=image_scalar, batch_size=1)\n",
    "valid_generator = trainGenerator(list_IDs=RAD_id_list[-20:], nt=nt, image_size=image_size, image_scalar=image_scalar, batch_size=1)\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=100, epochs=50, validation_data=valid_generator, validation_steps=20, callbacks=callbacks)\n",
    "print(time.strftime('end time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "print(time.strftime('It took  %H:%M:%S', time.gmtime(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "## `return_sequences=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = model\n",
    "layer_config = train_model.layers[1].get_config()\n",
    "layer_config['output_mode'] = 'prediction'\n",
    "data_format = layer_config['data_format'] if 'data_format' in layer_config else layer_config['dim_ordering']\n",
    "test_prednet = PredNet(weights=train_model.layers[1].get_weights(), **layer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = Input(shape=(nt,) + input_shape)\n",
    "test_outputs = test_prednet(test_inputs)\n",
    "\n",
    "def my_loss(y_true, y_pred):\n",
    "#     w = tf.add(y_true, tf.constant(0.8))\n",
    "#     w = tf.add(y_pred, w)\n",
    "#     loss = tf.losses.mean_squared_error(y_true, y_pred, weights=w)\n",
    "    loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "    loss = tf.multiply(loss, tf.constant(10000000.0))\n",
    "    return loss\n",
    "\n",
    "test_model = keras.models.Model(inputs=test_inputs, outputs=test_outputs)\n",
    "test_model.compile(loss=my_loss, optimizer=keras.optimizers.Adam())\n",
    "print(time.strftime('build time: %Y-%m-%d %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class testGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, nt, image_size, image_scalar, batch_size=32, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.nt = nt\n",
    "        self.image_size = image_size\n",
    "        self.image_scalar = image_scalar\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        X = np.empty((self.batch_size, self.nt, self.image_size, self.image_size, 1))\n",
    "        y = np.empty((self.batch_size, self.image_size, self.image_size, 1))\n",
    "        # Generate data\n",
    "#         print(list_IDs_temp)\n",
    "        for i, RAD_id in enumerate(list_IDs_temp):\n",
    "            for j in range(self.nt):\n",
    "                X[i][j] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * 1)).resize((self.image_size, self.image_size))).astype(np.int8).reshape((self.image_size, self.image_size, 1)) / self.image_scalar\n",
    "            y[i] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, (self.nt) * 1)).resize((self.image_size, self.image_size))).astype(np.int8).reshape((self.image_size, self.image_size, 1)) / self.image_scalar\n",
    "#         y = np.zeros(self.batch_size, np.float32)\n",
    "        return X, y\n",
    "print(time.strftime('build time: %Y-%m-%d %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime('start time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "train_generator = testGenerator(list_IDs=RAD_id_list[:6000], nt=nt, image_size=image_size, image_scalar=image_scalar, batch_size=1)\n",
    "valid_generator = testGenerator(list_IDs=RAD_id_list[-1:], nt=nt, image_size=image_size, image_scalar=image_scalar, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_generator.on_epoch_end()\n",
    "# valid_generator.on_epoch_end()\n",
    "for data in [train_generator, valid_generator][1]:\n",
    "    x, y_ = data\n",
    "    break\n",
    "y = test_model.predict(x)\n",
    "\n",
    "plt.imshow(x[0][-1].reshape((image_size, image_size)) * image_scalar, cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "plt.show()\n",
    "print('')\n",
    "plt.imshow(y_[0].reshape((image_size, image_size)) * image_scalar, cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "plt.show()\n",
    "print('')\n",
    "plt.imshow(y[0][-1].reshape((image_size, image_size)) * image_scalar, cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "plt.show()\n",
    "print('')\n",
    "for i in range(y.shape[1]):\n",
    "    plt.imshow(y[0][i].reshape((image_size, image_size)) * image_scalar, cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_generator.on_epoch_end()\n",
    "# valid_generator.on_epoch_end()\n",
    "for data in [train_generator, valid_generator][1]:\n",
    "    x, y_ = data\n",
    "    break\n",
    "for i in range(30):\n",
    "    y = test_model.predict(x)\n",
    "    y = np.where(y<0.01, -0.01, y)\n",
    "    for j in range(nt - 1):\n",
    "        x[0][j] = x[0][j + 1]\n",
    "    x[0][-1] = y[0][-1]\n",
    "    print('%2d:' % i)\n",
    "    plt.imshow(y[0][-1].reshape((image_size, image_size)) * image_scalar, cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "    plt.show()\n",
    "\n",
    "# plt.imshow(x[0][-1].reshape((image_size, image_size)) * image_scalar, cmap=cm.gist_ncar_r)\n",
    "# plt.show()\n",
    "# print('')\n",
    "# plt.imshow(y_[0].reshape((image_size, image_size)) * image_scalar, cmap=cm.gist_ncar_r)\n",
    "# plt.show()\n",
    "# print('')\n",
    "# plt.imshow(y[0][-1].reshape((image_size, image_size)) * image_scalar, cmap=cm.gist_ncar_r)\n",
    "# plt.show()\n",
    "# print('')\n",
    "# for i in range(y.shape[1]):\n",
    "#     plt.imshow(y[0][i].reshape((image_size, image_size)) * image_scalar, cmap=cm.gist_ncar_r)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'SRAD2018_Neutrino_PredNet_v0.3.5.1_nt10_18.09.02.23.07'\n",
    "model.save('/home/hadoop/Documents/Neutrino/prednet/%s.hdf5' % version)\n",
    "model.save_weights('/home/hadoop/Documents/Neutrino/prednet/%s_weights.hdf5' % version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `return_sequences=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = model\n",
    "layer_config = train_model.layers[1].get_config()\n",
    "layer_config['output_mode'] = 'prediction'\n",
    "layer_config['return_sequences'] = False\n",
    "data_format = layer_config['data_format'] if 'data_format' in layer_config else layer_config['dim_ordering']\n",
    "test_prednet = PredNet(weights=train_model.layers[1].get_weights(), **layer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = Input(shape=(nt,) + input_shape)\n",
    "test_outputs = test_prednet(test_inputs)\n",
    "\n",
    "def my_loss(y_true, y_pred):\n",
    "#     w = tf.add(y_true, tf.constant(0.8))\n",
    "#     w = tf.add(y_pred, w)\n",
    "#     loss = tf.losses.mean_squared_error(y_true, y_pred, weights=w)\n",
    "    loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "    loss = tf.multiply(loss, tf.constant(10000000.0))\n",
    "    return loss\n",
    "\n",
    "test_model = keras.models.Model(inputs=test_inputs, outputs=test_outputs)\n",
    "test_model.compile(loss=my_loss, optimizer=keras.optimizers.Adam())\n",
    "print(time.strftime('build time: %Y-%m-%d %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class testGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, nt, image_size, image_scalar, batch_size=32, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.nt = nt\n",
    "        self.image_size = image_size\n",
    "        self.image_scalar = image_scalar\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        X = np.empty((self.batch_size, self.nt, self.image_size, self.image_size, 1))\n",
    "        y = np.empty((self.batch_size, self.image_size, self.image_size, 1))\n",
    "        # Generate data\n",
    "#         print(list_IDs_temp)\n",
    "        for i, RAD_id in enumerate(list_IDs_temp):\n",
    "            for j in range(self.nt):\n",
    "                X[i][j] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + 30 - (nt - 1) * step_size)).resize((self.image_size, self.image_size))).astype(np.int8).reshape((self.image_size, self.image_size, 1)) / self.image_scalar\n",
    "#             y[i] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, (self.nt) * 1)).resize((self.image_size, self.image_size))).astype(np.int8).reshape((self.image_size, self.image_size, 1)) / self.image_scalar\n",
    "            y[i] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, 60)).resize((self.image_size, self.image_size))).astype(np.int8).reshape((self.image_size, self.image_size, 1)) / self.image_scalar\n",
    "#         y = np.zeros(self.batch_size, np.float32)\n",
    "        return X, y\n",
    "print(time.strftime('build time: %Y-%m-%d %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime('start time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "train_generator = testGenerator(list_IDs=RAD_id_list[:1], nt=nt, image_size=image_size, image_scalar=image_scalar, batch_size=1)\n",
    "valid_generator = testGenerator(list_IDs=RAD_id_list[-4:-3], nt=nt, image_size=image_size, image_scalar=image_scalar, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_generator.on_epoch_end()\n",
    "valid_generator.on_epoch_end()\n",
    "for data in [train_generator, valid_generator][1]:\n",
    "    x, y_ = data\n",
    "    break\n",
    "y = test_model.predict(x)\n",
    "\n",
    "plt.imshow(x[0][-1].reshape((image_size, image_size)) * image_scalar, cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "plt.show()\n",
    "print('')\n",
    "plt.imshow(y_[0].reshape((image_size, image_size)) * image_scalar, cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "plt.show()\n",
    "print('')\n",
    "plt.imshow(y[0].reshape((image_size, image_size)) * image_scalar, cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_generator.on_epoch_end()\n",
    "# valid_generator.on_epoch_end()\n",
    "for data in [train_generator, valid_generator][1]:\n",
    "    x, y_ = data\n",
    "    break\n",
    "for i in range(30 // step_size):\n",
    "    y = test_model.predict(x)\n",
    "#     y = np.where(y<0.03, -0.01, y)\n",
    "#     y = np.where(y>0.6, 0.6, y)\n",
    "    for j in range(nt - 1):\n",
    "        x[0][j] = x[0][j + 1]\n",
    "    x[0][-1] = y[0]\n",
    "    print('%2d:' % i)\n",
    "    plt.imshow(y[0].reshape((image_size, image_size)) * image_scalar, cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(time.strftime(\"start time: %Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "model_loss = []\n",
    "last_frame_loss = []\n",
    "for RAD_id_counter, RAD_id in enumerate(RAD_id_list[-4:-3]):\n",
    "    x = np.empty((nt, image_size, image_size, 1))\n",
    "    y = np.empty((1, image_size, image_size, 1))\n",
    "    y_true_last = np.empty((1, image_size, image_size, 1))\n",
    "    for i in range(nt):\n",
    "        x[i] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, i * step_size + 30 - (nt - 1) * step_size)).resize((image_size, image_size))).astype(np.int8).reshape((image_size, image_size, 1)) / image_scalar\n",
    "    x = x.reshape((1, nt, image_size, image_size, 1))\n",
    "    for i in range(30 // step_size):\n",
    "        y = test_model.predict(x)\n",
    "        y = np.where(y<0.03, -1/image_scalar, y)\n",
    "        for j in range(nt - 1):\n",
    "            x[0][j] = x[0][j + 1]\n",
    "        x[0][-1] = y[0]\n",
    "    x_last = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, 30)).resize((image_size, image_size))).astype(np.int8).reshape((image_size, image_size))\n",
    "    y_true_last = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, 60)).resize((image_size, image_size))).astype(np.int8).reshape((image_size, image_size))\n",
    "    y_pred_last = y[0].reshape((image_size, image_size)) * image_scalar\n",
    "#     last_frame_loss.append(sklearn.metrics.mean_absolute_error(y_true_last, x_last))\n",
    "#     model_loss.append(sklearn.metrics.mean_absolute_error(y_true_last, y_pred_last))\n",
    "    last_frame_loss.append(sklearn.metrics.mean_squared_error(y_true_last, x_last))\n",
    "    model_loss.append(sklearn.metrics.mean_squared_error(y_true_last, y_pred_last))\n",
    "    if RAD_id_counter % 10 == 0:\n",
    "        print('RAD_id_counter=%5d\\t\\tIt takes %.2f' % (RAD_id_counter, time.time() - start_time))\n",
    "model_loss = np.array(model_loss)\n",
    "last_frame_loss = np.array(last_frame_loss)\n",
    "print('     model_loss=%.7f' % model_loss.mean())\n",
    "print('last_frame_loss=%.7f' % last_frame_loss.mean())\n",
    "print(time.strftime('It took  %H:%M:%S', time.gmtime(time.time() - start_time)))\n",
    "print(time.strftime(\"end time: %Y-%m-%d %H:%M:%S\", time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_last.reshape((image_size, image_size)), cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "plt.show()\n",
    "plt.imshow(y_true_last.reshape((image_size, image_size)), cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "plt.show()\n",
    "plt.imshow(y_pred_last.reshape((image_size, image_size)), cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAD_id_submit_list = os.listdir(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_test/\")\n",
    "print(len(RAD_id_submit_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAD_id = RAD_id_submit_list[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.empty((nt, image_size, image_size, 1))\n",
    "y = np.empty((1, image_size, image_size, 1))\n",
    "for i in range(nt):\n",
    "    x[i] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_test/%s/%s_%03d.png\" % (RAD_id, RAD_id, i + 31 - nt)).resize((image_size, image_size))).astype(np.int8).reshape((image_size, image_size, 1)) / image_scalar\n",
    "x = x.reshape((1, nt, image_size, image_size, 1))\n",
    "\n",
    "for i in range(30):\n",
    "    y = test_model.predict(x)\n",
    "    y = np.where(y<0.03, -0.01, y)\n",
    "    for j in range(nt - 1):\n",
    "        x[0][j] = x[0][j + 1]\n",
    "    x[0][-1] = y[0]\n",
    "    if (31 + i) % 5 == 0:\n",
    "        print('%2d:' % (31 + i))\n",
    "        print(i // 5 + 1)\n",
    "        plt.imshow(y[0].reshape((image_size, image_size)) * image_scalar, cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "version = 'SRAD2018_submit_Neutrino_PredNet_v0.3.2.0_18.09.01.14.07'\n",
    "start_time = time.time()\n",
    "print(time.strftime(\"start time: %Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "for RAD_id_counter, RAD_id in enumerate(RAD_id_submit_list):\n",
    "    x = np.empty((nt, image_size, image_size, 1))\n",
    "    y = np.empty((1, image_size, image_size, 1))\n",
    "    for i in range(nt):\n",
    "        x[i] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_test/%s/%s_%03d.png\" % (RAD_id, RAD_id, i + 31 - nt)).resize((image_size, image_size))).astype(np.int8).reshape((image_size, image_size, 1)) / image_scalar\n",
    "    x = x.reshape((1, nt, image_size, image_size, 1))\n",
    "    if not os.path.exists(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_submit/%s/%s\" % (version, RAD_id)):\n",
    "        os.makedirs(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_submit/%s/%s\" % (version, RAD_id))\n",
    "    for i in range(30):\n",
    "        y = test_model.predict(x)\n",
    "        y = np.where(y<0.03, -0.01, y)\n",
    "        for j in range(nt - 1):\n",
    "            x[0][j] = x[0][j + 1]\n",
    "        x[0][-1] = y[0]\n",
    "        if (31 + i) % 5 == 0:\n",
    "#             print('%2d:' % (31 + i))\n",
    "#             print(i // 5 + 1)\n",
    "            result = y[0].reshape((image_size, image_size)) * image_scalar\n",
    "#             plt.imshow(result, cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "#             plt.show()\n",
    "            result = result.astype(np.uint8)\n",
    "            result = np.where(result==0, 255, result)\n",
    "            result = PIL.Image.fromarray(result)\n",
    "            result = result.resize((501, 501))\n",
    "#             plt.imshow(result, cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "#             plt.show()\n",
    "            result.save(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_submit/%s/%s/%s_f%03d.png\" % (version, RAD_id, RAD_id, i // 5 + 1))\n",
    "    if RAD_id_counter % 10 == 0:\n",
    "        print('RAD_id_counter=%5d\\t\\tIt takes %.2f' % (RAD_id_counter, time.time() - start_time))\n",
    "print(time.strftime('It took  %H:%M:%S', time.gmtime(time.time() - start_time)))\n",
    "print(time.strftime(\"end time: %Y-%m-%d %H:%M:%S\", time.localtime()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "zip -r \"SRAD2018_submit_Neutrino_PredNet_v0.3.2.0_18.09.01.14.07.zip\" \"SRAD2018_submit_Neutrino_PredNet_v0.3.2.0_18.09.01.14.07\"\n",
    "ls | wc -l\n",
    "```\n",
    "18.09.02 15.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'SRAD2018_submit_Neutrino_PredNet_v0.3.1.0_18.09.01.13.38'\n",
    "start_time = time.time()\n",
    "print(time.strftime(\"start time: %Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "for RAD_id_counter, RAD_id in enumerate(RAD_id_submit_list[:1]):\n",
    "    x = np.empty((nt, image_size, image_size, 1))\n",
    "    y = np.empty((1, image_size, image_size, 1))\n",
    "    for i in range(nt):\n",
    "        x[i] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_test/%s/%s_%03d.png\" % (RAD_id, RAD_id, i + 31 - nt)).resize((image_size, image_size))).astype(np.int8).reshape((image_size, image_size, 1)) / image_scalar\n",
    "    x = x.reshape((1, nt, image_size, image_size, 1))\n",
    "    if not os.path.exists(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_submit/%s/%s\" % (version, RAD_id)):\n",
    "        os.makedirs(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_submit/%s/%s\" % (version, RAD_id))\n",
    "    for i in range(30):\n",
    "#         print('before predict %.2f' % (time.time() - start_time))\n",
    "        y = test_model.predict(x)  # takes 0.2s to predict\n",
    "#         print('after predict %.2f' % (time.time() - start_time))\n",
    "        y = np.where(y<0.03, -0.01, y)\n",
    "        for j in range(nt - 1):\n",
    "            x[0][j] = x[0][j + 1]\n",
    "        x[0][-1] = y[0]\n",
    "        if (31 + i) % 5 == 0:\n",
    "#             print('%2d:' % (31 + i))\n",
    "#             print(i // 5 + 1)\n",
    "            result = y[0].reshape((image_size, image_size)) * image_scalar\n",
    "#             plt.imshow(result, cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "#             plt.show()\n",
    "            result = result.astype(np.uint8)\n",
    "            result = np.where(result==0, 255, result)\n",
    "            result = PIL.Image.fromarray(result)\n",
    "            result = result.resize((501, 501))\n",
    "#             plt.imshow(result, cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "#             plt.show()\n",
    "            result.save(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_submit/%s/%s/%s_f%03d.png\" % (version, RAD_id, RAD_id, i // 5 + 1))\n",
    "    if RAD_id_counter % 10 == 0:\n",
    "        print('RAD_id_counter=%5d\\t\\tIt takes %.2f' % (RAD_id_counter, time.time() - start_time))\n",
    "print(time.strftime('It took  %H:%M:%S', time.gmtime(time.time() - start_time)))\n",
    "print(time.strftime(\"end time: %Y-%m-%d %H:%M:%S\", time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_test/%s/%s_%03d.png\" % (RAD_id, RAD_id, 0)).resize((image_size, image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_submit/%s/%s/%s_f%03d.png\" % (version, RAD_id, RAD_id, i // 5 + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
