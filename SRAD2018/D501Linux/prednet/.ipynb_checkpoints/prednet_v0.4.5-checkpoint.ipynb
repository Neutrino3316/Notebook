{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(123)\n",
    "from six.moves import cPickle\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "# from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# from prednet import PredNet\n",
    "# # from data_utils import SequenceGenerator\n",
    "# from kitti_settings import *\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import time\n",
    "import multiprocessing\n",
    "import random\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import skimage\n",
    "from skimage import restoration\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build time: 2018-09-22 10:19:08\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras import activations\n",
    "from keras.layers import Recurrent\n",
    "from keras.layers import Conv2D, UpSampling2D, MaxPooling2D\n",
    "from keras.engine import InputSpec\n",
    "# from keras_utils import legacy_prednet_support\n",
    "\n",
    "class PredNet(Recurrent):\n",
    "    '''PredNet architecture - Lotter 2016.\n",
    "        Stacked convolutional LSTM inspired by predictive coding principles.\n",
    "\n",
    "    # Arguments\n",
    "        stack_sizes: number of channels in targets (A) and predictions (Ahat) in each layer of the architecture.\n",
    "            Length is the number of layers in the architecture.\n",
    "            First element is the number of channels in the input.\n",
    "            Ex. (3, 16, 32) would correspond to a 3 layer architecture that takes in RGB images and has 16 and 32\n",
    "                channels in the second and third layers, respectively.\n",
    "        R_stack_sizes: number of channels in the representation (R) modules.\n",
    "            Length must equal length of stack_sizes, but the number of channels per layer can be different.\n",
    "        A_filt_sizes: filter sizes for the target (A) modules.\n",
    "            Has length of 1 - len(stack_sizes).\n",
    "            Ex. (3, 3) would mean that targets for layers 2 and 3 are computed by a 3x3 convolution of the errors (E)\n",
    "                from the layer below (followed by max-pooling)\n",
    "        Ahat_filt_sizes: filter sizes for the prediction (Ahat) modules.\n",
    "            Has length equal to length of stack_sizes.\n",
    "            Ex. (3, 3, 3) would mean that the predictions for each layer are computed by a 3x3 convolution of the\n",
    "                representation (R) modules at each layer.\n",
    "        R_filt_sizes: filter sizes for the representation (R) modules.\n",
    "            Has length equal to length of stack_sizes.\n",
    "            Corresponds to the filter sizes for all convolutions in the LSTM.\n",
    "        pixel_max: the maximum pixel value.\n",
    "            Used to clip the pixel-layer prediction.\n",
    "        error_activation: activation function for the error (E) units.\n",
    "        A_activation: activation function for the target (A) and prediction (A_hat) units.\n",
    "        LSTM_activation: activation function for the cell and hidden states of the LSTM.\n",
    "        LSTM_inner_activation: activation function for the gates in the LSTM.\n",
    "        output_mode: either 'error', 'prediction', 'all' or layer specification (ex. R2, see below).\n",
    "            Controls what is outputted by the PredNet.\n",
    "            If 'error', the mean response of the error (E) units of each layer will be outputted.\n",
    "                That is, the output shape will be (batch_size, nb_layers).\n",
    "            If 'prediction', the frame prediction will be outputted.\n",
    "            If 'all', the output will be the frame prediction concatenated with the mean layer errors.\n",
    "                The frame prediction is flattened before concatenation.\n",
    "                Nomenclature of 'all' is kept for backwards compatibility, but should not be confused with returning all of the layers of the model\n",
    "            For returning the features of a particular layer, output_mode should be of the form unit_type + layer_number.\n",
    "                For instance, to return the features of the LSTM \"representational\" units in the lowest layer, output_mode should be specificied as 'R0'.\n",
    "                The possible unit types are 'R', 'Ahat', 'A', and 'E' corresponding to the 'representation', 'prediction', 'target', and 'error' units respectively.\n",
    "        extrap_start_time: time step for which model will start extrapolating.\n",
    "            Starting at this time step, the prediction from the previous time step will be treated as the \"actual\"\n",
    "        data_format: 'channels_first' or 'channels_last'.\n",
    "            It defaults to the `image_data_format` value found in your\n",
    "            Keras config file at `~/.keras/keras.json`.\n",
    "\n",
    "    # References\n",
    "        - [Deep predictive coding networks for video prediction and unsupervised learning](https://arxiv.org/abs/1605.08104)\n",
    "        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "        - [Convolutional LSTM network: a machine learning approach for precipitation nowcasting](http://arxiv.org/abs/1506.04214)\n",
    "        - [Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects](http://www.nature.com/neuro/journal/v2/n1/pdf/nn0199_79.pdf)\n",
    "    '''\n",
    "#     @legacy_prednet_support\n",
    "    def __init__(self, stack_sizes, R_stack_sizes,\n",
    "                 A_filt_sizes, Ahat_filt_sizes, R_filt_sizes,\n",
    "                 pixel_max=1., error_activation='relu', A_activation='relu',\n",
    "                 LSTM_activation='tanh', LSTM_inner_activation='hard_sigmoid',\n",
    "                 conv_dropout=0.8,\n",
    "                 output_mode='error', extrap_start_time=None,\n",
    "                 data_format=K.image_data_format(), **kwargs):\n",
    "        self.stack_sizes = stack_sizes\n",
    "        self.nb_layers = len(stack_sizes)\n",
    "        assert len(R_stack_sizes) == self.nb_layers, 'len(R_stack_sizes) must equal len(stack_sizes)'\n",
    "        self.R_stack_sizes = R_stack_sizes\n",
    "        assert len(A_filt_sizes) == (self.nb_layers - 1), 'len(A_filt_sizes) must equal len(stack_sizes) - 1'\n",
    "        self.A_filt_sizes = A_filt_sizes\n",
    "        assert len(Ahat_filt_sizes) == self.nb_layers, 'len(Ahat_filt_sizes) must equal len(stack_sizes)'\n",
    "        self.Ahat_filt_sizes = Ahat_filt_sizes\n",
    "        assert len(R_filt_sizes) == (self.nb_layers), 'len(R_filt_sizes) must equal len(stack_sizes)'\n",
    "        self.R_filt_sizes = R_filt_sizes\n",
    "\n",
    "        self.pixel_max = pixel_max\n",
    "        self.error_activation = activations.get(error_activation)\n",
    "        self.A_activation = activations.get(A_activation)\n",
    "        self.LSTM_activation = activations.get(LSTM_activation)\n",
    "        self.LSTM_inner_activation = activations.get(LSTM_inner_activation)\n",
    "        self.conv_dropout = conv_dropout\n",
    "\n",
    "        default_output_modes = ['prediction', 'error', 'all']\n",
    "        layer_output_modes = [layer + str(n) for n in range(self.nb_layers) for layer in ['R', 'E', 'A', 'Ahat']]\n",
    "        assert output_mode in default_output_modes + layer_output_modes, 'Invalid output_mode: ' + str(output_mode)\n",
    "        self.output_mode = output_mode\n",
    "        if self.output_mode in layer_output_modes:\n",
    "            self.output_layer_type = self.output_mode[:-1]\n",
    "            self.output_layer_num = int(self.output_mode[-1])\n",
    "        else:\n",
    "            self.output_layer_type = None\n",
    "            self.output_layer_num = None\n",
    "        self.extrap_start_time = extrap_start_time\n",
    "\n",
    "        assert data_format in {'channels_last', 'channels_first'}, 'data_format must be in {channels_last, channels_first}'\n",
    "        self.data_format = data_format\n",
    "        self.channel_axis = -3 if data_format == 'channels_first' else -1\n",
    "        self.row_axis = -2 if data_format == 'channels_first' else -3\n",
    "        self.column_axis = -1 if data_format == 'channels_first' else -2\n",
    "        super(PredNet, self).__init__(**kwargs)\n",
    "        self.input_spec = [InputSpec(ndim=5)]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.output_mode == 'prediction':\n",
    "            out_shape = input_shape[2:]\n",
    "        elif self.output_mode == 'error':\n",
    "            out_shape = (self.nb_layers,)\n",
    "        elif self.output_mode == 'all':\n",
    "            out_shape = (np.prod(input_shape[2:]) + self.nb_layers,)\n",
    "        else:\n",
    "            stack_str = 'R_stack_sizes' if self.output_layer_type == 'R' else 'stack_sizes'\n",
    "            stack_mult = 2 if self.output_layer_type == 'E' else 1\n",
    "            out_stack_size = stack_mult * getattr(self, stack_str)[self.output_layer_num]\n",
    "            out_nb_row = input_shape[self.row_axis] / 2**self.output_layer_num\n",
    "            out_nb_col = input_shape[self.column_axis] / 2**self.output_layer_num\n",
    "            if self.data_format == 'channels_first':\n",
    "                out_shape = (out_stack_size, out_nb_row, out_nb_col)\n",
    "            else:\n",
    "                out_shape = (out_nb_row, out_nb_col, out_stack_size)\n",
    "\n",
    "        if self.return_sequences:\n",
    "            return (input_shape[0], input_shape[1]) + out_shape\n",
    "        else:\n",
    "            return (input_shape[0],) + out_shape\n",
    "\n",
    "    def get_initial_state(self, x):\n",
    "        input_shape = self.input_spec[0].shape\n",
    "        init_nb_row = input_shape[self.row_axis]\n",
    "        init_nb_col = input_shape[self.column_axis]\n",
    "\n",
    "        base_initial_state = K.zeros_like(x)  # (samples, timesteps) + image_shape\n",
    "        non_channel_axis = -1 if self.data_format == 'channels_first' else -2\n",
    "        for _ in range(2):\n",
    "            base_initial_state = K.sum(base_initial_state, axis=non_channel_axis)\n",
    "        base_initial_state = K.sum(base_initial_state, axis=1)  # (samples, nb_channels)\n",
    "\n",
    "        initial_states = []\n",
    "        states_to_pass = ['r', 'c', 'e']\n",
    "        nlayers_to_pass = {u: self.nb_layers for u in states_to_pass}\n",
    "        if self.extrap_start_time is not None:\n",
    "            states_to_pass.append('ahat')  # pass prediction in states so can use as actual for t+1 when extrapolating\n",
    "            nlayers_to_pass['ahat'] = 1\n",
    "        for u in states_to_pass:\n",
    "            for l in range(nlayers_to_pass[u]):\n",
    "                ds_factor = 2 ** l\n",
    "                nb_row = init_nb_row // ds_factor\n",
    "                nb_col = init_nb_col // ds_factor\n",
    "                if u in ['r', 'c']:\n",
    "                    stack_size = self.R_stack_sizes[l]\n",
    "                elif u == 'e':\n",
    "                    stack_size = 2 * self.stack_sizes[l]\n",
    "                elif u == 'ahat':\n",
    "                    stack_size = self.stack_sizes[l]\n",
    "                output_size = stack_size * nb_row * nb_col  # flattened size\n",
    "\n",
    "                reducer = K.zeros((input_shape[self.channel_axis], output_size)) # (nb_channels, output_size)\n",
    "                initial_state = K.dot(base_initial_state, reducer) # (samples, output_size)\n",
    "                if self.data_format == 'channels_first':\n",
    "                    output_shp = (-1, stack_size, nb_row, nb_col)\n",
    "                else:\n",
    "                    output_shp = (-1, nb_row, nb_col, stack_size)\n",
    "                initial_state = K.reshape(initial_state, output_shp)\n",
    "                initial_states += [initial_state]\n",
    "\n",
    "        if K._BACKEND == 'theano':\n",
    "            from theano import tensor as T\n",
    "            # There is a known issue in the Theano scan op when dealing with inputs whose shape is 1 along a dimension.\n",
    "            # In our case, this is a problem when training on grayscale images, and the below line fixes it.\n",
    "            initial_states = [T.unbroadcast(init_state, 0, 1) for init_state in initial_states]\n",
    "\n",
    "        if self.extrap_start_time is not None:\n",
    "            initial_states += [K.variable(0, int if K.backend() != 'tensorflow' else 'int32')]  # the last state will correspond to the current timestep\n",
    "        return initial_states\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.conv_layers = {c: [] for c in ['i', 'f', 'c', 'o', 'a', 'ahat']}\n",
    "\n",
    "        for l in range(self.nb_layers):\n",
    "            for c in ['i', 'f', 'c', 'o']:\n",
    "                act = self.LSTM_activation if c == 'c' else self.LSTM_inner_activation\n",
    "                self.conv_layers[c].append(Conv2D(self.R_stack_sizes[l], self.R_filt_sizes[l], bias_initializer='zeros', padding='same', activation=act, data_format=self.data_format))\n",
    "#                 self.conv_layers[c].append(keras.layers.Dropout(self.conv_dropout))\n",
    "\n",
    "            act = 'relu' if l == 0 else self.A_activation\n",
    "            self.conv_layers['ahat'].append(Conv2D(self.stack_sizes[l], self.Ahat_filt_sizes[l], bias_initializer='zeros', padding='same', activation=act, data_format=self.data_format))\n",
    "\n",
    "            if l < self.nb_layers - 1:\n",
    "                self.conv_layers['a'].append(Conv2D(self.stack_sizes[l+1], self.A_filt_sizes[l], bias_initializer='zeros', padding='same', activation=self.A_activation, data_format=self.data_format))\n",
    "\n",
    "        self.upsample = UpSampling2D(data_format=self.data_format)\n",
    "        self.pool = MaxPooling2D(data_format=self.data_format)\n",
    "\n",
    "        self.trainable_weights = []\n",
    "        nb_row, nb_col = (input_shape[-2], input_shape[-1]) if self.data_format == 'channels_first' else (input_shape[-3], input_shape[-2])\n",
    "        for c in sorted(self.conv_layers.keys()):\n",
    "            for l in range(len(self.conv_layers[c])):\n",
    "                ds_factor = 2 ** l\n",
    "                if c == 'ahat':\n",
    "                    nb_channels = self.R_stack_sizes[l]\n",
    "                elif c == 'a':\n",
    "                    nb_channels = 2 * self.R_stack_sizes[l]\n",
    "                else:\n",
    "                    nb_channels = self.stack_sizes[l] * 2 + self.R_stack_sizes[l]\n",
    "                    if l < self.nb_layers - 1:\n",
    "                        nb_channels += self.R_stack_sizes[l+1]\n",
    "                in_shape = (input_shape[0], nb_channels, nb_row // ds_factor, nb_col // ds_factor)\n",
    "                if self.data_format == 'channels_last': in_shape = (in_shape[0], in_shape[2], in_shape[3], in_shape[1])\n",
    "                with K.name_scope('layer_' + c + '_' + str(l)):\n",
    "                    self.conv_layers[c][l].build(in_shape)\n",
    "                self.trainable_weights += self.conv_layers[c][l].trainable_weights\n",
    "\n",
    "        self.states = [None] * self.nb_layers*3\n",
    "\n",
    "        if self.extrap_start_time is not None:\n",
    "            self.t_extrap = K.variable(self.extrap_start_time, int if K.backend() != 'tensorflow' else 'int32')\n",
    "            self.states += [None] * 2  # [previous frame prediction, timestep]\n",
    "\n",
    "    def step(self, a, states):\n",
    "        r_tm1 = states[:self.nb_layers]\n",
    "        c_tm1 = states[self.nb_layers:2*self.nb_layers]\n",
    "        e_tm1 = states[2*self.nb_layers:3*self.nb_layers]\n",
    "\n",
    "        if self.extrap_start_time is not None:\n",
    "            t = states[-1]\n",
    "            a = K.switch(t >= self.t_extrap, states[-2], a)  # if past self.extrap_start_time, the previous prediction will be treated as the actual\n",
    "\n",
    "        c = []\n",
    "        r = []\n",
    "        e = []\n",
    "\n",
    "        # Update R units starting from the top\n",
    "        for l in reversed(range(self.nb_layers)):\n",
    "            inputs = [r_tm1[l], e_tm1[l]]\n",
    "            if l < self.nb_layers - 1:\n",
    "                inputs.append(r_up)\n",
    "\n",
    "            inputs = K.concatenate(inputs, axis=self.channel_axis)\n",
    "            i = self.conv_layers['i'][l].call(inputs)\n",
    "            f = self.conv_layers['f'][l].call(inputs)\n",
    "            o = self.conv_layers['o'][l].call(inputs)\n",
    "            _c = f * c_tm1[l] + i * self.conv_layers['c'][l].call(inputs)\n",
    "            _r = o * self.LSTM_activation(_c)\n",
    "            c.insert(0, _c)\n",
    "            r.insert(0, _r)\n",
    "\n",
    "            if l > 0:\n",
    "                r_up = self.upsample.call(_r)\n",
    "\n",
    "        # Update feedforward path starting from the bottom\n",
    "        for l in range(self.nb_layers):\n",
    "            ahat = self.conv_layers['ahat'][l].call(r[l])\n",
    "            if l == 0:\n",
    "                ahat = K.minimum(ahat, self.pixel_max)\n",
    "                frame_prediction = ahat\n",
    "\n",
    "            # compute errors\n",
    "            e_up = self.error_activation(ahat - a)\n",
    "            e_down = self.error_activation(a - ahat)\n",
    "\n",
    "            e.append(K.concatenate((e_up, e_down), axis=self.channel_axis))\n",
    "\n",
    "            if self.output_layer_num == l:\n",
    "                if self.output_layer_type == 'A':\n",
    "                    output = a\n",
    "                elif self.output_layer_type == 'Ahat':\n",
    "                    output = ahat\n",
    "                elif self.output_layer_type == 'R':\n",
    "                    output = r[l]\n",
    "                elif self.output_layer_type == 'E':\n",
    "                    output = e[l]\n",
    "\n",
    "            if l < self.nb_layers - 1:\n",
    "                a = self.conv_layers['a'][l].call(e[l])\n",
    "                a = self.pool.call(a)  # target for next layer\n",
    "\n",
    "        if self.output_layer_type is None:\n",
    "            if self.output_mode == 'prediction':\n",
    "                output = frame_prediction\n",
    "            else:\n",
    "                for l in range(self.nb_layers):\n",
    "                    layer_error = K.mean(K.batch_flatten(e[l]), axis=-1, keepdims=True)\n",
    "                    all_error = layer_error if l == 0 else K.concatenate((all_error, layer_error), axis=-1)\n",
    "                if self.output_mode == 'error':\n",
    "                    output = all_error\n",
    "                else:\n",
    "                    output = K.concatenate((K.batch_flatten(frame_prediction), all_error), axis=-1)\n",
    "\n",
    "        states = r + c + e\n",
    "        if self.extrap_start_time is not None:\n",
    "            states += [frame_prediction, t + 1]\n",
    "        return output, states\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'stack_sizes': self.stack_sizes,\n",
    "                  'R_stack_sizes': self.R_stack_sizes,\n",
    "                  'A_filt_sizes': self.A_filt_sizes,\n",
    "                  'Ahat_filt_sizes': self.Ahat_filt_sizes,\n",
    "                  'R_filt_sizes': self.R_filt_sizes,\n",
    "                  'pixel_max': self.pixel_max,\n",
    "                  'error_activation': self.error_activation.__name__,\n",
    "                  'A_activation': self.A_activation.__name__,\n",
    "                  'LSTM_activation': self.LSTM_activation.__name__,\n",
    "                  'LSTM_inner_activation': self.LSTM_inner_activation.__name__,\n",
    "                  'data_format': self.data_format,\n",
    "                  'extrap_start_time': self.extrap_start_time,\n",
    "                  'output_mode': self.output_mode}\n",
    "        base_config = super(PredNet, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "print(time.strftime('build time: %Y-%m-%d %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n"
     ]
    }
   ],
   "source": [
    "raw_RAD_id_list = os.listdir('/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/')\n",
    "print(len(raw_RAD_id_list))\n",
    "RAD_id_list = raw_RAD_id_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_RAD_id(RAD_id):\n",
    "#     return RAD_id\n",
    "    mean_list = []\n",
    "    for k in range(61):\n",
    "        mean_list.append([RAD_id, k,\n",
    "                          np.array(PIL.Image.open('/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png'\n",
    "                          % (RAD_id, RAD_id,\n",
    "                          k))).astype(np.int8).ravel().mean()])\n",
    "    mean_list = np.array(mean_list)\n",
    "    return pd.DataFrame(columns=['RAD_id', 'time', 'signal'],data=mean_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 2018-09-22 10:48:49\n",
      "end time: 2018-09-22 10:51:19\n",
      "00:02:30\n",
      "15000\n"
     ]
    }
   ],
   "source": [
    "pool = multiprocessing.Pool()\n",
    "start_time = time.time()\n",
    "print(time.strftime('start time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "# map(check_RAD_id, raw_RAD_id_list[:100])\n",
    "# print(time.strftime('start time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "distribution_list = list(pool.map(distribution_RAD_id, raw_RAD_id_list))\n",
    "distribution_list = [x for x in distribution_list if x is not None]\n",
    "print(time.strftime('end time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "print(time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time)))\n",
    "print(len(distribution_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_array = np.array(distribution_list)\n",
    "distribution_df = pd.DataFrame(distribution_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri = sns.load_dataset(\"fmri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f8158a02d68>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(distribution_list.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(data=distribution_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_df = copy.deepcopy(distribution_list[0])\n",
    "for i in range(10):\n",
    "    distribution_df = distribution_df.append(distribution_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RAD_id</th>\n",
       "      <th>time</th>\n",
       "      <th>signal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>0</td>\n",
       "      <td>1.7473316839375141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>1</td>\n",
       "      <td>1.7561125254481058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>2</td>\n",
       "      <td>1.7834430938522157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>3</td>\n",
       "      <td>1.7338735702248198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>4</td>\n",
       "      <td>1.782498874506476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>5</td>\n",
       "      <td>1.7607778455065917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>6</td>\n",
       "      <td>1.7334871175812048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>7</td>\n",
       "      <td>1.7969649523308673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>8</td>\n",
       "      <td>1.7930486332723774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>9</td>\n",
       "      <td>1.8119250520914258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>10</td>\n",
       "      <td>1.8023155286233918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>11</td>\n",
       "      <td>1.8389886892880905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>12</td>\n",
       "      <td>1.851912940585894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>13</td>\n",
       "      <td>1.8834506635431731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>14</td>\n",
       "      <td>1.9499762949151598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>15</td>\n",
       "      <td>1.9706256150373902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>16</td>\n",
       "      <td>2.0450356771486966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>17</td>\n",
       "      <td>2.0235576750690236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>18</td>\n",
       "      <td>2.0677726383560224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>19</td>\n",
       "      <td>2.0719399524304682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>20</td>\n",
       "      <td>2.0167130808243794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>21</td>\n",
       "      <td>2.016011888398851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>22</td>\n",
       "      <td>1.9777769809682033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>23</td>\n",
       "      <td>1.9368966657503357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>24</td>\n",
       "      <td>1.8494189266178223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>25</td>\n",
       "      <td>1.8334508627455668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>26</td>\n",
       "      <td>1.8648969526017825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>27</td>\n",
       "      <td>1.8066183003254968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>28</td>\n",
       "      <td>1.9046298620324222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>RAD_436482474222544</td>\n",
       "      <td>29</td>\n",
       "      <td>1.7854271496926306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>31</td>\n",
       "      <td>-0.9830120198724308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>32</td>\n",
       "      <td>-0.9892111983617595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>33</td>\n",
       "      <td>-0.9888406819096338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>34</td>\n",
       "      <td>-0.9880080159043191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>35</td>\n",
       "      <td>-0.9870877008458133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>36</td>\n",
       "      <td>-0.9877769411277246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>37</td>\n",
       "      <td>-0.9862510507926263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>38</td>\n",
       "      <td>-0.9827928972394532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>39</td>\n",
       "      <td>-0.9809203947394632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>40</td>\n",
       "      <td>-0.9731395492448237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>41</td>\n",
       "      <td>-0.9716813877235548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.9662431623778391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>43</td>\n",
       "      <td>-0.9727770008884427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>44</td>\n",
       "      <td>-0.9737849650001394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>45</td>\n",
       "      <td>-0.9719483189310003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>46</td>\n",
       "      <td>-0.966267066665073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>47</td>\n",
       "      <td>-0.9497252998992036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>48</td>\n",
       "      <td>-0.9519723028991917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>49</td>\n",
       "      <td>-0.950267130409839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>50</td>\n",
       "      <td>-0.948382675766232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>51</td>\n",
       "      <td>-0.9553069509683229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>52</td>\n",
       "      <td>-0.9527412241385492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>53</td>\n",
       "      <td>-0.9499165341970749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>54</td>\n",
       "      <td>-0.9618089170959478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>55</td>\n",
       "      <td>-0.9579443906597982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>56</td>\n",
       "      <td>-0.9699204385639898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>57</td>\n",
       "      <td>-0.9625140935693484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>58</td>\n",
       "      <td>-0.9611674853885044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>59</td>\n",
       "      <td>-0.9579443906597982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>RAD_336482464232533</td>\n",
       "      <td>60</td>\n",
       "      <td>-0.9645379898884865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>671 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 RAD_id time               signal\n",
       "0   RAD_436482474222544    0   1.7473316839375141\n",
       "1   RAD_436482474222544    1   1.7561125254481058\n",
       "2   RAD_436482474222544    2   1.7834430938522157\n",
       "3   RAD_436482474222544    3   1.7338735702248198\n",
       "4   RAD_436482474222544    4    1.782498874506476\n",
       "5   RAD_436482474222544    5   1.7607778455065917\n",
       "6   RAD_436482474222544    6   1.7334871175812048\n",
       "7   RAD_436482474222544    7   1.7969649523308673\n",
       "8   RAD_436482474222544    8   1.7930486332723774\n",
       "9   RAD_436482474222544    9   1.8119250520914258\n",
       "10  RAD_436482474222544   10   1.8023155286233918\n",
       "11  RAD_436482474222544   11   1.8389886892880905\n",
       "12  RAD_436482474222544   12    1.851912940585894\n",
       "13  RAD_436482474222544   13   1.8834506635431731\n",
       "14  RAD_436482474222544   14   1.9499762949151598\n",
       "15  RAD_436482474222544   15   1.9706256150373902\n",
       "16  RAD_436482474222544   16   2.0450356771486966\n",
       "17  RAD_436482474222544   17   2.0235576750690236\n",
       "18  RAD_436482474222544   18   2.0677726383560224\n",
       "19  RAD_436482474222544   19   2.0719399524304682\n",
       "20  RAD_436482474222544   20   2.0167130808243794\n",
       "21  RAD_436482474222544   21    2.016011888398851\n",
       "22  RAD_436482474222544   22   1.9777769809682033\n",
       "23  RAD_436482474222544   23   1.9368966657503357\n",
       "24  RAD_436482474222544   24   1.8494189266178223\n",
       "25  RAD_436482474222544   25   1.8334508627455668\n",
       "26  RAD_436482474222544   26   1.8648969526017825\n",
       "27  RAD_436482474222544   27   1.8066183003254968\n",
       "28  RAD_436482474222544   28   1.9046298620324222\n",
       "29  RAD_436482474222544   29   1.7854271496926306\n",
       "..                  ...  ...                  ...\n",
       "31  RAD_336482464232533   31  -0.9830120198724308\n",
       "32  RAD_336482464232533   32  -0.9892111983617595\n",
       "33  RAD_336482464232533   33  -0.9888406819096338\n",
       "34  RAD_336482464232533   34  -0.9880080159043191\n",
       "35  RAD_336482464232533   35  -0.9870877008458133\n",
       "36  RAD_336482464232533   36  -0.9877769411277246\n",
       "37  RAD_336482464232533   37  -0.9862510507926263\n",
       "38  RAD_336482464232533   38  -0.9827928972394532\n",
       "39  RAD_336482464232533   39  -0.9809203947394632\n",
       "40  RAD_336482464232533   40  -0.9731395492448237\n",
       "41  RAD_336482464232533   41  -0.9716813877235548\n",
       "42  RAD_336482464232533   42  -0.9662431623778391\n",
       "43  RAD_336482464232533   43  -0.9727770008884427\n",
       "44  RAD_336482464232533   44  -0.9737849650001394\n",
       "45  RAD_336482464232533   45  -0.9719483189310003\n",
       "46  RAD_336482464232533   46   -0.966267066665073\n",
       "47  RAD_336482464232533   47  -0.9497252998992036\n",
       "48  RAD_336482464232533   48  -0.9519723028991917\n",
       "49  RAD_336482464232533   49   -0.950267130409839\n",
       "50  RAD_336482464232533   50   -0.948382675766232\n",
       "51  RAD_336482464232533   51  -0.9553069509683229\n",
       "52  RAD_336482464232533   52  -0.9527412241385492\n",
       "53  RAD_336482464232533   53  -0.9499165341970749\n",
       "54  RAD_336482464232533   54  -0.9618089170959478\n",
       "55  RAD_336482464232533   55  -0.9579443906597982\n",
       "56  RAD_336482464232533   56  -0.9699204385639898\n",
       "57  RAD_336482464232533   57  -0.9625140935693484\n",
       "58  RAD_336482464232533   58  -0.9611674853885044\n",
       "59  RAD_336482464232533   59  -0.9579443906597982\n",
       "60  RAD_336482464232533   60  -0.9645379898884865\n",
       "\n",
       "[671 rows x 3 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.747332</td>\n",
       "      <td>1.756113</td>\n",
       "      <td>1.783443</td>\n",
       "      <td>1.733874</td>\n",
       "      <td>1.782499</td>\n",
       "      <td>1.760778</td>\n",
       "      <td>1.733487</td>\n",
       "      <td>1.796965</td>\n",
       "      <td>1.793049</td>\n",
       "      <td>1.811925</td>\n",
       "      <td>...</td>\n",
       "      <td>2.012721</td>\n",
       "      <td>1.943383</td>\n",
       "      <td>1.967673</td>\n",
       "      <td>1.933721</td>\n",
       "      <td>1.930773</td>\n",
       "      <td>1.897056</td>\n",
       "      <td>1.879423</td>\n",
       "      <td>1.845945</td>\n",
       "      <td>1.766120</td>\n",
       "      <td>1.654005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.347915</td>\n",
       "      <td>-0.329309</td>\n",
       "      <td>-0.320321</td>\n",
       "      <td>-0.300955</td>\n",
       "      <td>-0.335807</td>\n",
       "      <td>-0.310580</td>\n",
       "      <td>-0.337604</td>\n",
       "      <td>-0.345903</td>\n",
       "      <td>-0.325298</td>\n",
       "      <td>-0.339612</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.913941</td>\n",
       "      <td>-0.915395</td>\n",
       "      <td>-0.936435</td>\n",
       "      <td>-0.915443</td>\n",
       "      <td>-0.928152</td>\n",
       "      <td>-0.918000</td>\n",
       "      <td>-0.916056</td>\n",
       "      <td>-0.913570</td>\n",
       "      <td>-0.907610</td>\n",
       "      <td>-0.920678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.982657</td>\n",
       "      <td>-0.989279</td>\n",
       "      <td>-0.988785</td>\n",
       "      <td>-0.989422</td>\n",
       "      <td>-0.986673</td>\n",
       "      <td>-0.988729</td>\n",
       "      <td>-0.991582</td>\n",
       "      <td>-0.991008</td>\n",
       "      <td>-0.989745</td>\n",
       "      <td>-0.984263</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.986944</td>\n",
       "      <td>-0.990143</td>\n",
       "      <td>-0.988367</td>\n",
       "      <td>-0.988303</td>\n",
       "      <td>-0.985980</td>\n",
       "      <td>-0.990470</td>\n",
       "      <td>-0.989203</td>\n",
       "      <td>-0.991705</td>\n",
       "      <td>-0.989072</td>\n",
       "      <td>-0.991291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.871435</td>\n",
       "      <td>-0.888646</td>\n",
       "      <td>-0.875710</td>\n",
       "      <td>-0.876518</td>\n",
       "      <td>-0.856152</td>\n",
       "      <td>-0.858080</td>\n",
       "      <td>-0.860475</td>\n",
       "      <td>-0.863614</td>\n",
       "      <td>-0.872228</td>\n",
       "      <td>-0.879745</td>\n",
       "      <td>...</td>\n",
       "      <td>1.619332</td>\n",
       "      <td>1.763838</td>\n",
       "      <td>1.847650</td>\n",
       "      <td>1.868283</td>\n",
       "      <td>1.978976</td>\n",
       "      <td>2.027354</td>\n",
       "      <td>2.109880</td>\n",
       "      <td>2.181175</td>\n",
       "      <td>2.186740</td>\n",
       "      <td>2.185190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.987502</td>\n",
       "      <td>-0.989183</td>\n",
       "      <td>-0.990673</td>\n",
       "      <td>-0.989060</td>\n",
       "      <td>-0.992171</td>\n",
       "      <td>-0.992498</td>\n",
       "      <td>-0.991235</td>\n",
       "      <td>-0.991745</td>\n",
       "      <td>-0.993466</td>\n",
       "      <td>-0.992928</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.997805</td>\n",
       "      <td>-0.998323</td>\n",
       "      <td>-0.993857</td>\n",
       "      <td>-0.987183</td>\n",
       "      <td>-0.989303</td>\n",
       "      <td>-0.990394</td>\n",
       "      <td>-0.989677</td>\n",
       "      <td>-0.982371</td>\n",
       "      <td>-0.973813</td>\n",
       "      <td>-0.981024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.999386</td>\n",
       "      <td>-0.999590</td>\n",
       "      <td>-0.999765</td>\n",
       "      <td>-0.999614</td>\n",
       "      <td>-0.999462</td>\n",
       "      <td>-0.999578</td>\n",
       "      <td>-0.999709</td>\n",
       "      <td>-0.999590</td>\n",
       "      <td>-0.999725</td>\n",
       "      <td>-0.999697</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.997952</td>\n",
       "      <td>-0.984841</td>\n",
       "      <td>-0.972060</td>\n",
       "      <td>-0.932853</td>\n",
       "      <td>-0.889694</td>\n",
       "      <td>-0.827503</td>\n",
       "      <td>-0.800061</td>\n",
       "      <td>-0.759184</td>\n",
       "      <td>-0.642033</td>\n",
       "      <td>-0.512743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.811347</td>\n",
       "      <td>7.725216</td>\n",
       "      <td>7.585767</td>\n",
       "      <td>7.585906</td>\n",
       "      <td>7.461285</td>\n",
       "      <td>7.488974</td>\n",
       "      <td>7.522751</td>\n",
       "      <td>7.650599</td>\n",
       "      <td>7.842706</td>\n",
       "      <td>8.800746</td>\n",
       "      <td>...</td>\n",
       "      <td>3.030251</td>\n",
       "      <td>3.010283</td>\n",
       "      <td>2.928144</td>\n",
       "      <td>2.842527</td>\n",
       "      <td>2.654376</td>\n",
       "      <td>2.648627</td>\n",
       "      <td>2.655695</td>\n",
       "      <td>2.699806</td>\n",
       "      <td>2.705945</td>\n",
       "      <td>2.736443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.739854</td>\n",
       "      <td>-0.754208</td>\n",
       "      <td>-0.733854</td>\n",
       "      <td>-0.726555</td>\n",
       "      <td>-0.730543</td>\n",
       "      <td>-0.720611</td>\n",
       "      <td>-0.726778</td>\n",
       "      <td>-0.731244</td>\n",
       "      <td>-0.742913</td>\n",
       "      <td>-0.737961</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.866391</td>\n",
       "      <td>-0.859359</td>\n",
       "      <td>-0.862184</td>\n",
       "      <td>-0.857040</td>\n",
       "      <td>-0.853522</td>\n",
       "      <td>-0.832005</td>\n",
       "      <td>-0.804734</td>\n",
       "      <td>-0.811168</td>\n",
       "      <td>-0.786108</td>\n",
       "      <td>-0.788156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.818355</td>\n",
       "      <td>0.943853</td>\n",
       "      <td>0.977016</td>\n",
       "      <td>1.048777</td>\n",
       "      <td>1.039964</td>\n",
       "      <td>1.108968</td>\n",
       "      <td>1.134999</td>\n",
       "      <td>1.106984</td>\n",
       "      <td>1.182752</td>\n",
       "      <td>1.236306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.791571</td>\n",
       "      <td>0.825367</td>\n",
       "      <td>0.869208</td>\n",
       "      <td>0.855459</td>\n",
       "      <td>0.863670</td>\n",
       "      <td>0.857538</td>\n",
       "      <td>0.862758</td>\n",
       "      <td>0.858475</td>\n",
       "      <td>0.889367</td>\n",
       "      <td>0.967211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.977311</td>\n",
       "      <td>-0.980136</td>\n",
       "      <td>-0.972865</td>\n",
       "      <td>-0.973677</td>\n",
       "      <td>-0.971885</td>\n",
       "      <td>-0.978594</td>\n",
       "      <td>-0.977546</td>\n",
       "      <td>-0.974024</td>\n",
       "      <td>-0.972040</td>\n",
       "      <td>-0.976163</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.955307</td>\n",
       "      <td>-0.952741</td>\n",
       "      <td>-0.949917</td>\n",
       "      <td>-0.961809</td>\n",
       "      <td>-0.957944</td>\n",
       "      <td>-0.969920</td>\n",
       "      <td>-0.962514</td>\n",
       "      <td>-0.961167</td>\n",
       "      <td>-0.957944</td>\n",
       "      <td>-0.964538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.441588</td>\n",
       "      <td>-0.457548</td>\n",
       "      <td>-0.432939</td>\n",
       "      <td>-0.345748</td>\n",
       "      <td>-0.512484</td>\n",
       "      <td>-0.529524</td>\n",
       "      <td>-0.523719</td>\n",
       "      <td>-0.537850</td>\n",
       "      <td>-0.567488</td>\n",
       "      <td>-0.599069</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.844746</td>\n",
       "      <td>-0.815590</td>\n",
       "      <td>-0.757981</td>\n",
       "      <td>-0.740678</td>\n",
       "      <td>-0.767435</td>\n",
       "      <td>-0.785786</td>\n",
       "      <td>-0.795798</td>\n",
       "      <td>-0.639790</td>\n",
       "      <td>-0.587890</td>\n",
       "      <td>-0.718662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.196732</td>\n",
       "      <td>3.008940</td>\n",
       "      <td>2.806041</td>\n",
       "      <td>2.678515</td>\n",
       "      <td>2.521125</td>\n",
       "      <td>2.399194</td>\n",
       "      <td>2.206370</td>\n",
       "      <td>2.130625</td>\n",
       "      <td>1.959347</td>\n",
       "      <td>1.814527</td>\n",
       "      <td>...</td>\n",
       "      <td>4.345070</td>\n",
       "      <td>4.409046</td>\n",
       "      <td>4.532333</td>\n",
       "      <td>4.686216</td>\n",
       "      <td>4.766583</td>\n",
       "      <td>4.893132</td>\n",
       "      <td>5.124342</td>\n",
       "      <td>5.232087</td>\n",
       "      <td>5.361724</td>\n",
       "      <td>5.417361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.844845</td>\n",
       "      <td>-0.862399</td>\n",
       "      <td>-0.879937</td>\n",
       "      <td>-0.895024</td>\n",
       "      <td>-0.925227</td>\n",
       "      <td>-0.916857</td>\n",
       "      <td>-0.935450</td>\n",
       "      <td>-0.944470</td>\n",
       "      <td>-0.957399</td>\n",
       "      <td>-0.947036</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.976494</td>\n",
       "      <td>-0.968036</td>\n",
       "      <td>-0.971403</td>\n",
       "      <td>-0.969518</td>\n",
       "      <td>-0.967132</td>\n",
       "      <td>-0.975893</td>\n",
       "      <td>-0.966988</td>\n",
       "      <td>-0.967948</td>\n",
       "      <td>-0.965769</td>\n",
       "      <td>-0.966418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.973570</td>\n",
       "      <td>-0.979140</td>\n",
       "      <td>-0.980594</td>\n",
       "      <td>-0.982522</td>\n",
       "      <td>-0.978088</td>\n",
       "      <td>-0.976805</td>\n",
       "      <td>-0.971530</td>\n",
       "      <td>-0.982072</td>\n",
       "      <td>-0.961602</td>\n",
       "      <td>-0.975721</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.464209</td>\n",
       "      <td>-0.495867</td>\n",
       "      <td>-0.497910</td>\n",
       "      <td>-0.469329</td>\n",
       "      <td>-0.474560</td>\n",
       "      <td>-0.544329</td>\n",
       "      <td>-0.548213</td>\n",
       "      <td>-0.626826</td>\n",
       "      <td>-0.625069</td>\n",
       "      <td>-0.719021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.999187</td>\n",
       "      <td>-0.997928</td>\n",
       "      <td>-0.999275</td>\n",
       "      <td>-0.997665</td>\n",
       "      <td>-0.999454</td>\n",
       "      <td>-0.998606</td>\n",
       "      <td>-0.999359</td>\n",
       "      <td>-0.997386</td>\n",
       "      <td>-0.997040</td>\n",
       "      <td>-0.998594</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999618</td>\n",
       "      <td>-0.999745</td>\n",
       "      <td>-0.999147</td>\n",
       "      <td>-0.998677</td>\n",
       "      <td>-0.999673</td>\n",
       "      <td>-0.999530</td>\n",
       "      <td>-0.999960</td>\n",
       "      <td>-0.996546</td>\n",
       "      <td>-0.999179</td>\n",
       "      <td>-0.999876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.995701</td>\n",
       "      <td>-0.996880</td>\n",
       "      <td>-0.997303</td>\n",
       "      <td>-0.996052</td>\n",
       "      <td>-0.996916</td>\n",
       "      <td>-0.995765</td>\n",
       "      <td>-0.997673</td>\n",
       "      <td>-0.995773</td>\n",
       "      <td>-0.998570</td>\n",
       "      <td>-0.996745</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.881686</td>\n",
       "      <td>-0.874562</td>\n",
       "      <td>-0.864574</td>\n",
       "      <td>-0.864750</td>\n",
       "      <td>-0.884865</td>\n",
       "      <td>-0.886957</td>\n",
       "      <td>-0.886666</td>\n",
       "      <td>-0.887335</td>\n",
       "      <td>-0.891247</td>\n",
       "      <td>-0.899781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.948809</td>\n",
       "      <td>-0.955809</td>\n",
       "      <td>-0.952506</td>\n",
       "      <td>-0.965757</td>\n",
       "      <td>-0.959171</td>\n",
       "      <td>-0.956024</td>\n",
       "      <td>-0.961044</td>\n",
       "      <td>-0.962144</td>\n",
       "      <td>-0.937709</td>\n",
       "      <td>-0.959777</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.765734</td>\n",
       "      <td>-0.753395</td>\n",
       "      <td>-0.729678</td>\n",
       "      <td>-0.745248</td>\n",
       "      <td>-0.745013</td>\n",
       "      <td>-0.731822</td>\n",
       "      <td>-0.734551</td>\n",
       "      <td>-0.755407</td>\n",
       "      <td>-0.669033</td>\n",
       "      <td>-0.715443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.281369</td>\n",
       "      <td>1.303102</td>\n",
       "      <td>1.339995</td>\n",
       "      <td>1.384038</td>\n",
       "      <td>1.548799</td>\n",
       "      <td>1.615324</td>\n",
       "      <td>1.683220</td>\n",
       "      <td>1.692184</td>\n",
       "      <td>1.697846</td>\n",
       "      <td>1.731300</td>\n",
       "      <td>...</td>\n",
       "      <td>3.363584</td>\n",
       "      <td>3.554380</td>\n",
       "      <td>3.645707</td>\n",
       "      <td>3.806630</td>\n",
       "      <td>3.913339</td>\n",
       "      <td>3.972140</td>\n",
       "      <td>4.190282</td>\n",
       "      <td>4.394233</td>\n",
       "      <td>4.536189</td>\n",
       "      <td>4.639248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.428006</td>\n",
       "      <td>0.471373</td>\n",
       "      <td>0.527281</td>\n",
       "      <td>0.590759</td>\n",
       "      <td>0.723073</td>\n",
       "      <td>0.755862</td>\n",
       "      <td>0.813001</td>\n",
       "      <td>0.839136</td>\n",
       "      <td>0.940825</td>\n",
       "      <td>0.946331</td>\n",
       "      <td>...</td>\n",
       "      <td>1.963307</td>\n",
       "      <td>1.953307</td>\n",
       "      <td>1.964136</td>\n",
       "      <td>1.825443</td>\n",
       "      <td>1.803917</td>\n",
       "      <td>1.675902</td>\n",
       "      <td>1.555512</td>\n",
       "      <td>1.414500</td>\n",
       "      <td>1.285684</td>\n",
       "      <td>1.157318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.851877</td>\n",
       "      <td>-0.796774</td>\n",
       "      <td>-0.734766</td>\n",
       "      <td>-0.691949</td>\n",
       "      <td>-0.589281</td>\n",
       "      <td>-0.523787</td>\n",
       "      <td>-0.454695</td>\n",
       "      <td>-0.410305</td>\n",
       "      <td>-0.358540</td>\n",
       "      <td>-0.332333</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.110358</td>\n",
       "      <td>-0.167669</td>\n",
       "      <td>-0.200406</td>\n",
       "      <td>-0.214378</td>\n",
       "      <td>-0.373293</td>\n",
       "      <td>-0.408839</td>\n",
       "      <td>-0.415441</td>\n",
       "      <td>-0.450516</td>\n",
       "      <td>-0.456078</td>\n",
       "      <td>-0.458281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.259700</td>\n",
       "      <td>-0.281122</td>\n",
       "      <td>-0.308676</td>\n",
       "      <td>-0.356740</td>\n",
       "      <td>-0.333393</td>\n",
       "      <td>-0.360803</td>\n",
       "      <td>-0.365544</td>\n",
       "      <td>-0.405947</td>\n",
       "      <td>-0.383385</td>\n",
       "      <td>-0.404293</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.670579</td>\n",
       "      <td>-0.651985</td>\n",
       "      <td>-0.635153</td>\n",
       "      <td>-0.620810</td>\n",
       "      <td>-0.580599</td>\n",
       "      <td>-0.581691</td>\n",
       "      <td>-0.581719</td>\n",
       "      <td>-0.570520</td>\n",
       "      <td>-0.579715</td>\n",
       "      <td>-0.558472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.030713</td>\n",
       "      <td>-0.074159</td>\n",
       "      <td>-0.120585</td>\n",
       "      <td>-0.193912</td>\n",
       "      <td>-0.248852</td>\n",
       "      <td>-0.288879</td>\n",
       "      <td>-0.349162</td>\n",
       "      <td>-0.402544</td>\n",
       "      <td>-0.438257</td>\n",
       "      <td>-0.466779</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.487209</td>\n",
       "      <td>-0.432608</td>\n",
       "      <td>-0.461524</td>\n",
       "      <td>-0.457787</td>\n",
       "      <td>-0.435500</td>\n",
       "      <td>-0.466962</td>\n",
       "      <td>-0.364664</td>\n",
       "      <td>-0.419696</td>\n",
       "      <td>-0.376823</td>\n",
       "      <td>-0.337178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.984725</td>\n",
       "      <td>-0.989299</td>\n",
       "      <td>-0.992147</td>\n",
       "      <td>-0.991518</td>\n",
       "      <td>-0.990016</td>\n",
       "      <td>-0.992892</td>\n",
       "      <td>-0.994904</td>\n",
       "      <td>-0.991259</td>\n",
       "      <td>-0.990618</td>\n",
       "      <td>-0.994837</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.815547</td>\n",
       "      <td>-0.745156</td>\n",
       "      <td>-0.719113</td>\n",
       "      <td>-0.621005</td>\n",
       "      <td>-0.580563</td>\n",
       "      <td>-0.558815</td>\n",
       "      <td>-0.436285</td>\n",
       "      <td>-0.317385</td>\n",
       "      <td>-0.295262</td>\n",
       "      <td>-0.257318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.195378</td>\n",
       "      <td>0.126924</td>\n",
       "      <td>0.075506</td>\n",
       "      <td>0.017044</td>\n",
       "      <td>-0.012251</td>\n",
       "      <td>-0.030713</td>\n",
       "      <td>-0.074159</td>\n",
       "      <td>-0.120585</td>\n",
       "      <td>-0.193912</td>\n",
       "      <td>-0.248852</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.558109</td>\n",
       "      <td>-0.555046</td>\n",
       "      <td>-0.543468</td>\n",
       "      <td>-0.548799</td>\n",
       "      <td>-0.459604</td>\n",
       "      <td>-0.487209</td>\n",
       "      <td>-0.432608</td>\n",
       "      <td>-0.461524</td>\n",
       "      <td>-0.457787</td>\n",
       "      <td>-0.435500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.992430</td>\n",
       "      <td>1.913905</td>\n",
       "      <td>1.795272</td>\n",
       "      <td>1.760077</td>\n",
       "      <td>1.698910</td>\n",
       "      <td>1.644519</td>\n",
       "      <td>1.629974</td>\n",
       "      <td>1.566448</td>\n",
       "      <td>1.536010</td>\n",
       "      <td>1.419775</td>\n",
       "      <td>...</td>\n",
       "      <td>1.748682</td>\n",
       "      <td>1.838116</td>\n",
       "      <td>1.851977</td>\n",
       "      <td>1.783021</td>\n",
       "      <td>1.828323</td>\n",
       "      <td>1.876136</td>\n",
       "      <td>1.858295</td>\n",
       "      <td>1.838056</td>\n",
       "      <td>1.815060</td>\n",
       "      <td>1.899008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.899008</td>\n",
       "      <td>1.910128</td>\n",
       "      <td>2.018781</td>\n",
       "      <td>2.016183</td>\n",
       "      <td>2.083641</td>\n",
       "      <td>2.066442</td>\n",
       "      <td>2.136804</td>\n",
       "      <td>2.151187</td>\n",
       "      <td>2.178928</td>\n",
       "      <td>2.157171</td>\n",
       "      <td>...</td>\n",
       "      <td>5.040880</td>\n",
       "      <td>5.003382</td>\n",
       "      <td>5.135748</td>\n",
       "      <td>5.277330</td>\n",
       "      <td>5.364154</td>\n",
       "      <td>5.462911</td>\n",
       "      <td>5.555819</td>\n",
       "      <td>5.667149</td>\n",
       "      <td>5.741324</td>\n",
       "      <td>5.912514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.899271</td>\n",
       "      <td>0.976239</td>\n",
       "      <td>1.091115</td>\n",
       "      <td>1.181079</td>\n",
       "      <td>1.259310</td>\n",
       "      <td>1.296804</td>\n",
       "      <td>1.307628</td>\n",
       "      <td>1.393939</td>\n",
       "      <td>1.422349</td>\n",
       "      <td>1.409524</td>\n",
       "      <td>...</td>\n",
       "      <td>3.265306</td>\n",
       "      <td>3.360134</td>\n",
       "      <td>3.436134</td>\n",
       "      <td>3.468185</td>\n",
       "      <td>3.582898</td>\n",
       "      <td>3.595950</td>\n",
       "      <td>3.674734</td>\n",
       "      <td>3.743559</td>\n",
       "      <td>3.788622</td>\n",
       "      <td>3.806539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.473707</td>\n",
       "      <td>-0.484974</td>\n",
       "      <td>-0.468887</td>\n",
       "      <td>-0.484731</td>\n",
       "      <td>-0.462034</td>\n",
       "      <td>-0.442648</td>\n",
       "      <td>-0.410783</td>\n",
       "      <td>-0.362106</td>\n",
       "      <td>-0.330134</td>\n",
       "      <td>-0.261354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.483193</td>\n",
       "      <td>0.488811</td>\n",
       "      <td>0.460536</td>\n",
       "      <td>0.468464</td>\n",
       "      <td>0.500584</td>\n",
       "      <td>0.450899</td>\n",
       "      <td>0.456014</td>\n",
       "      <td>0.470142</td>\n",
       "      <td>0.456743</td>\n",
       "      <td>0.454787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3.902192</td>\n",
       "      <td>3.911797</td>\n",
       "      <td>4.129980</td>\n",
       "      <td>4.085478</td>\n",
       "      <td>4.178465</td>\n",
       "      <td>4.179517</td>\n",
       "      <td>4.147191</td>\n",
       "      <td>4.181553</td>\n",
       "      <td>4.226625</td>\n",
       "      <td>4.224330</td>\n",
       "      <td>...</td>\n",
       "      <td>2.671077</td>\n",
       "      <td>2.569579</td>\n",
       "      <td>2.406174</td>\n",
       "      <td>2.404409</td>\n",
       "      <td>2.202103</td>\n",
       "      <td>2.094163</td>\n",
       "      <td>1.973936</td>\n",
       "      <td>1.613400</td>\n",
       "      <td>1.190426</td>\n",
       "      <td>0.734941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.953642</td>\n",
       "      <td>-0.952897</td>\n",
       "      <td>-0.952104</td>\n",
       "      <td>-0.964893</td>\n",
       "      <td>-0.961399</td>\n",
       "      <td>-0.971422</td>\n",
       "      <td>-0.968367</td>\n",
       "      <td>-0.964371</td>\n",
       "      <td>-0.969972</td>\n",
       "      <td>-0.969359</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.920976</td>\n",
       "      <td>-0.903861</td>\n",
       "      <td>-0.920136</td>\n",
       "      <td>-0.900339</td>\n",
       "      <td>-0.898741</td>\n",
       "      <td>-0.882722</td>\n",
       "      <td>-0.886052</td>\n",
       "      <td>-0.882538</td>\n",
       "      <td>-0.881933</td>\n",
       "      <td>-0.846427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14970</th>\n",
       "      <td>2.416237</td>\n",
       "      <td>2.459301</td>\n",
       "      <td>2.451305</td>\n",
       "      <td>2.436066</td>\n",
       "      <td>2.451329</td>\n",
       "      <td>2.472739</td>\n",
       "      <td>2.533500</td>\n",
       "      <td>2.395377</td>\n",
       "      <td>2.438460</td>\n",
       "      <td>2.448520</td>\n",
       "      <td>...</td>\n",
       "      <td>3.089139</td>\n",
       "      <td>3.083892</td>\n",
       "      <td>3.104151</td>\n",
       "      <td>3.161545</td>\n",
       "      <td>3.183187</td>\n",
       "      <td>3.209370</td>\n",
       "      <td>3.285592</td>\n",
       "      <td>3.422747</td>\n",
       "      <td>3.439962</td>\n",
       "      <td>3.462162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14971</th>\n",
       "      <td>-0.031366</td>\n",
       "      <td>-0.013809</td>\n",
       "      <td>-0.059797</td>\n",
       "      <td>-0.084800</td>\n",
       "      <td>-0.054554</td>\n",
       "      <td>-0.046378</td>\n",
       "      <td>0.032657</td>\n",
       "      <td>0.028119</td>\n",
       "      <td>0.018410</td>\n",
       "      <td>0.035235</td>\n",
       "      <td>...</td>\n",
       "      <td>1.162099</td>\n",
       "      <td>1.092661</td>\n",
       "      <td>1.024912</td>\n",
       "      <td>0.978586</td>\n",
       "      <td>0.955203</td>\n",
       "      <td>1.031163</td>\n",
       "      <td>1.051207</td>\n",
       "      <td>1.124482</td>\n",
       "      <td>1.131478</td>\n",
       "      <td>1.159613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14972</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-0.999956</td>\n",
       "      <td>-0.999976</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999988</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999988</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14973</th>\n",
       "      <td>-0.943777</td>\n",
       "      <td>-0.946574</td>\n",
       "      <td>-0.952315</td>\n",
       "      <td>-0.950733</td>\n",
       "      <td>-0.947168</td>\n",
       "      <td>-0.948116</td>\n",
       "      <td>-0.948809</td>\n",
       "      <td>-0.957371</td>\n",
       "      <td>-0.957243</td>\n",
       "      <td>-0.943932</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.673639</td>\n",
       "      <td>-0.697288</td>\n",
       "      <td>-0.701412</td>\n",
       "      <td>-0.734188</td>\n",
       "      <td>-0.729941</td>\n",
       "      <td>-0.734041</td>\n",
       "      <td>-0.761069</td>\n",
       "      <td>-0.769718</td>\n",
       "      <td>-0.760790</td>\n",
       "      <td>-0.785674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14974</th>\n",
       "      <td>-0.974936</td>\n",
       "      <td>-0.976972</td>\n",
       "      <td>-0.982283</td>\n",
       "      <td>-0.987928</td>\n",
       "      <td>-0.979279</td>\n",
       "      <td>-0.978056</td>\n",
       "      <td>-0.975092</td>\n",
       "      <td>-0.966634</td>\n",
       "      <td>-0.951490</td>\n",
       "      <td>-0.931674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346441</td>\n",
       "      <td>0.332899</td>\n",
       "      <td>0.312620</td>\n",
       "      <td>0.313286</td>\n",
       "      <td>0.293915</td>\n",
       "      <td>0.315433</td>\n",
       "      <td>0.304007</td>\n",
       "      <td>0.290501</td>\n",
       "      <td>0.283334</td>\n",
       "      <td>0.260832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14975</th>\n",
       "      <td>-0.997542</td>\n",
       "      <td>-0.998179</td>\n",
       "      <td>-0.997331</td>\n",
       "      <td>-0.997693</td>\n",
       "      <td>-0.996709</td>\n",
       "      <td>-0.995470</td>\n",
       "      <td>-0.991940</td>\n",
       "      <td>-0.991108</td>\n",
       "      <td>-0.986498</td>\n",
       "      <td>-0.983136</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999745</td>\n",
       "      <td>-0.988359</td>\n",
       "      <td>-0.989773</td>\n",
       "      <td>-0.988187</td>\n",
       "      <td>-0.992777</td>\n",
       "      <td>-0.992809</td>\n",
       "      <td>-0.990992</td>\n",
       "      <td>-0.989944</td>\n",
       "      <td>-0.990904</td>\n",
       "      <td>-0.987960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14976</th>\n",
       "      <td>-0.503460</td>\n",
       "      <td>-0.530460</td>\n",
       "      <td>-0.459070</td>\n",
       "      <td>-0.466241</td>\n",
       "      <td>-0.457464</td>\n",
       "      <td>-0.467827</td>\n",
       "      <td>-0.448162</td>\n",
       "      <td>-0.456424</td>\n",
       "      <td>-0.425273</td>\n",
       "      <td>-0.400222</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.555787</td>\n",
       "      <td>-0.575201</td>\n",
       "      <td>-0.538074</td>\n",
       "      <td>0.021382</td>\n",
       "      <td>0.034697</td>\n",
       "      <td>0.097239</td>\n",
       "      <td>0.116776</td>\n",
       "      <td>0.074797</td>\n",
       "      <td>0.241557</td>\n",
       "      <td>0.303628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14977</th>\n",
       "      <td>-0.992940</td>\n",
       "      <td>-0.992036</td>\n",
       "      <td>-0.995171</td>\n",
       "      <td>-0.995255</td>\n",
       "      <td>-0.997096</td>\n",
       "      <td>-0.994534</td>\n",
       "      <td>-0.996916</td>\n",
       "      <td>-0.994829</td>\n",
       "      <td>-0.996869</td>\n",
       "      <td>-0.999785</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114661</td>\n",
       "      <td>0.145880</td>\n",
       "      <td>0.139529</td>\n",
       "      <td>0.249812</td>\n",
       "      <td>0.228633</td>\n",
       "      <td>0.202099</td>\n",
       "      <td>0.161230</td>\n",
       "      <td>0.167493</td>\n",
       "      <td>0.232497</td>\n",
       "      <td>0.210035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14978</th>\n",
       "      <td>-0.811084</td>\n",
       "      <td>-0.799833</td>\n",
       "      <td>-0.805921</td>\n",
       "      <td>-0.802929</td>\n",
       "      <td>-0.806758</td>\n",
       "      <td>-0.768881</td>\n",
       "      <td>-0.757515</td>\n",
       "      <td>-0.704599</td>\n",
       "      <td>-0.619762</td>\n",
       "      <td>-0.499871</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.541775</td>\n",
       "      <td>-0.505902</td>\n",
       "      <td>-0.510870</td>\n",
       "      <td>-0.509333</td>\n",
       "      <td>-0.493922</td>\n",
       "      <td>-0.541360</td>\n",
       "      <td>-0.523655</td>\n",
       "      <td>-0.556882</td>\n",
       "      <td>-0.502273</td>\n",
       "      <td>-0.544735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14979</th>\n",
       "      <td>0.697252</td>\n",
       "      <td>0.835463</td>\n",
       "      <td>1.005203</td>\n",
       "      <td>1.127609</td>\n",
       "      <td>1.199673</td>\n",
       "      <td>1.373544</td>\n",
       "      <td>1.409090</td>\n",
       "      <td>1.559225</td>\n",
       "      <td>1.547460</td>\n",
       "      <td>1.653894</td>\n",
       "      <td>...</td>\n",
       "      <td>5.913347</td>\n",
       "      <td>5.849694</td>\n",
       "      <td>5.579034</td>\n",
       "      <td>5.766599</td>\n",
       "      <td>5.852076</td>\n",
       "      <td>6.264688</td>\n",
       "      <td>6.346712</td>\n",
       "      <td>6.591671</td>\n",
       "      <td>6.784164</td>\n",
       "      <td>6.960940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14980</th>\n",
       "      <td>-0.851264</td>\n",
       "      <td>-0.859722</td>\n",
       "      <td>-0.871144</td>\n",
       "      <td>-0.892243</td>\n",
       "      <td>-0.851746</td>\n",
       "      <td>-0.848674</td>\n",
       "      <td>-0.853789</td>\n",
       "      <td>-0.847877</td>\n",
       "      <td>-0.826503</td>\n",
       "      <td>-0.841857</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.947964</td>\n",
       "      <td>-0.946160</td>\n",
       "      <td>-0.950610</td>\n",
       "      <td>-0.955140</td>\n",
       "      <td>-0.975092</td>\n",
       "      <td>-0.970673</td>\n",
       "      <td>-0.964084</td>\n",
       "      <td>-0.978056</td>\n",
       "      <td>-0.976243</td>\n",
       "      <td>-0.982231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14981</th>\n",
       "      <td>2.249370</td>\n",
       "      <td>2.189322</td>\n",
       "      <td>2.056844</td>\n",
       "      <td>1.946263</td>\n",
       "      <td>1.962578</td>\n",
       "      <td>1.812802</td>\n",
       "      <td>1.833363</td>\n",
       "      <td>1.798790</td>\n",
       "      <td>1.804969</td>\n",
       "      <td>1.676961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182736</td>\n",
       "      <td>0.299533</td>\n",
       "      <td>0.378843</td>\n",
       "      <td>0.333525</td>\n",
       "      <td>0.349421</td>\n",
       "      <td>0.329138</td>\n",
       "      <td>0.391915</td>\n",
       "      <td>0.358696</td>\n",
       "      <td>0.374624</td>\n",
       "      <td>0.342744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14982</th>\n",
       "      <td>0.858204</td>\n",
       "      <td>0.939100</td>\n",
       "      <td>1.068207</td>\n",
       "      <td>1.173736</td>\n",
       "      <td>1.248648</td>\n",
       "      <td>1.349544</td>\n",
       "      <td>1.353505</td>\n",
       "      <td>1.406779</td>\n",
       "      <td>1.487118</td>\n",
       "      <td>1.509604</td>\n",
       "      <td>...</td>\n",
       "      <td>2.125768</td>\n",
       "      <td>2.046661</td>\n",
       "      <td>1.863076</td>\n",
       "      <td>1.817606</td>\n",
       "      <td>1.678240</td>\n",
       "      <td>1.585671</td>\n",
       "      <td>1.556627</td>\n",
       "      <td>1.447632</td>\n",
       "      <td>0.614615</td>\n",
       "      <td>0.464695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14983</th>\n",
       "      <td>-0.411576</td>\n",
       "      <td>-0.389552</td>\n",
       "      <td>-0.334859</td>\n",
       "      <td>-0.292031</td>\n",
       "      <td>-0.255198</td>\n",
       "      <td>-0.212067</td>\n",
       "      <td>-0.188947</td>\n",
       "      <td>-0.104561</td>\n",
       "      <td>-0.067151</td>\n",
       "      <td>-0.028470</td>\n",
       "      <td>...</td>\n",
       "      <td>1.592950</td>\n",
       "      <td>1.567663</td>\n",
       "      <td>1.544285</td>\n",
       "      <td>1.458074</td>\n",
       "      <td>1.393652</td>\n",
       "      <td>1.292943</td>\n",
       "      <td>1.180577</td>\n",
       "      <td>1.092904</td>\n",
       "      <td>0.983343</td>\n",
       "      <td>0.882008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14984</th>\n",
       "      <td>10.621822</td>\n",
       "      <td>10.389887</td>\n",
       "      <td>10.390724</td>\n",
       "      <td>10.170473</td>\n",
       "      <td>10.251768</td>\n",
       "      <td>10.233007</td>\n",
       "      <td>10.277636</td>\n",
       "      <td>10.217876</td>\n",
       "      <td>10.097235</td>\n",
       "      <td>10.081506</td>\n",
       "      <td>...</td>\n",
       "      <td>1.724160</td>\n",
       "      <td>1.699559</td>\n",
       "      <td>1.640025</td>\n",
       "      <td>1.065621</td>\n",
       "      <td>0.995151</td>\n",
       "      <td>0.884439</td>\n",
       "      <td>0.907024</td>\n",
       "      <td>0.919486</td>\n",
       "      <td>0.904785</td>\n",
       "      <td>0.896359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14985</th>\n",
       "      <td>4.273971</td>\n",
       "      <td>4.307983</td>\n",
       "      <td>4.340740</td>\n",
       "      <td>4.274075</td>\n",
       "      <td>4.280863</td>\n",
       "      <td>4.128446</td>\n",
       "      <td>4.104506</td>\n",
       "      <td>4.069565</td>\n",
       "      <td>4.033231</td>\n",
       "      <td>3.826268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252318</td>\n",
       "      <td>0.101502</td>\n",
       "      <td>0.204402</td>\n",
       "      <td>0.178987</td>\n",
       "      <td>0.172203</td>\n",
       "      <td>0.189222</td>\n",
       "      <td>0.096960</td>\n",
       "      <td>0.097406</td>\n",
       "      <td>0.074518</td>\n",
       "      <td>0.094446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14986</th>\n",
       "      <td>-0.994490</td>\n",
       "      <td>-0.995665</td>\n",
       "      <td>-0.995231</td>\n",
       "      <td>-0.996080</td>\n",
       "      <td>-0.996179</td>\n",
       "      <td>-0.996482</td>\n",
       "      <td>-0.995550</td>\n",
       "      <td>-0.993952</td>\n",
       "      <td>-0.995522</td>\n",
       "      <td>-0.992542</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.993518</td>\n",
       "      <td>-0.994470</td>\n",
       "      <td>-0.994956</td>\n",
       "      <td>-0.996108</td>\n",
       "      <td>-0.996052</td>\n",
       "      <td>-0.993379</td>\n",
       "      <td>-0.994020</td>\n",
       "      <td>-0.994801</td>\n",
       "      <td>-0.994175</td>\n",
       "      <td>-0.993251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14987</th>\n",
       "      <td>-0.445580</td>\n",
       "      <td>-0.420767</td>\n",
       "      <td>-0.434329</td>\n",
       "      <td>-0.421086</td>\n",
       "      <td>-0.473982</td>\n",
       "      <td>-0.538396</td>\n",
       "      <td>-0.550428</td>\n",
       "      <td>-0.555137</td>\n",
       "      <td>-0.610631</td>\n",
       "      <td>-0.610974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.971590</td>\n",
       "      <td>-0.974877</td>\n",
       "      <td>-0.991649</td>\n",
       "      <td>-0.998426</td>\n",
       "      <td>-0.999669</td>\n",
       "      <td>-0.999849</td>\n",
       "      <td>-0.999845</td>\n",
       "      <td>-0.999853</td>\n",
       "      <td>-0.999908</td>\n",
       "      <td>-0.999817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14988</th>\n",
       "      <td>-0.961219</td>\n",
       "      <td>-0.954399</td>\n",
       "      <td>-0.957498</td>\n",
       "      <td>-0.956140</td>\n",
       "      <td>-0.962430</td>\n",
       "      <td>-0.961928</td>\n",
       "      <td>-0.966801</td>\n",
       "      <td>-0.969486</td>\n",
       "      <td>-0.951454</td>\n",
       "      <td>-0.969211</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.998036</td>\n",
       "      <td>-0.997629</td>\n",
       "      <td>-0.999474</td>\n",
       "      <td>-0.998291</td>\n",
       "      <td>-0.998980</td>\n",
       "      <td>-0.998490</td>\n",
       "      <td>-0.999398</td>\n",
       "      <td>-0.997697</td>\n",
       "      <td>-0.995829</td>\n",
       "      <td>-0.998255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14989</th>\n",
       "      <td>-0.930952</td>\n",
       "      <td>-0.926753</td>\n",
       "      <td>-0.920721</td>\n",
       "      <td>-0.907196</td>\n",
       "      <td>-0.899909</td>\n",
       "      <td>-0.876108</td>\n",
       "      <td>-0.851104</td>\n",
       "      <td>-0.810487</td>\n",
       "      <td>-0.772292</td>\n",
       "      <td>-0.742889</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.548137</td>\n",
       "      <td>-0.523213</td>\n",
       "      <td>-0.522488</td>\n",
       "      <td>-0.541580</td>\n",
       "      <td>-0.517062</td>\n",
       "      <td>-0.539659</td>\n",
       "      <td>-0.525500</td>\n",
       "      <td>-0.523520</td>\n",
       "      <td>-0.514153</td>\n",
       "      <td>-0.503942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14990</th>\n",
       "      <td>-0.021809</td>\n",
       "      <td>0.085219</td>\n",
       "      <td>0.145948</td>\n",
       "      <td>0.270003</td>\n",
       "      <td>0.262338</td>\n",
       "      <td>0.379313</td>\n",
       "      <td>0.403138</td>\n",
       "      <td>0.446488</td>\n",
       "      <td>0.436456</td>\n",
       "      <td>0.575783</td>\n",
       "      <td>...</td>\n",
       "      <td>2.667037</td>\n",
       "      <td>2.465755</td>\n",
       "      <td>2.434711</td>\n",
       "      <td>2.244230</td>\n",
       "      <td>2.139302</td>\n",
       "      <td>1.869738</td>\n",
       "      <td>1.632452</td>\n",
       "      <td>1.420692</td>\n",
       "      <td>1.242565</td>\n",
       "      <td>1.090047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14991</th>\n",
       "      <td>3.868403</td>\n",
       "      <td>4.033219</td>\n",
       "      <td>4.030048</td>\n",
       "      <td>4.099227</td>\n",
       "      <td>3.993924</td>\n",
       "      <td>4.180896</td>\n",
       "      <td>4.109908</td>\n",
       "      <td>4.182609</td>\n",
       "      <td>4.101996</td>\n",
       "      <td>4.141944</td>\n",
       "      <td>...</td>\n",
       "      <td>4.454556</td>\n",
       "      <td>4.510508</td>\n",
       "      <td>4.529488</td>\n",
       "      <td>4.479564</td>\n",
       "      <td>4.566834</td>\n",
       "      <td>4.566559</td>\n",
       "      <td>4.583615</td>\n",
       "      <td>4.660364</td>\n",
       "      <td>4.592053</td>\n",
       "      <td>4.899566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14992</th>\n",
       "      <td>-0.645647</td>\n",
       "      <td>-0.697914</td>\n",
       "      <td>-0.723439</td>\n",
       "      <td>-0.755559</td>\n",
       "      <td>-0.778909</td>\n",
       "      <td>-0.787351</td>\n",
       "      <td>-0.795216</td>\n",
       "      <td>-0.800638</td>\n",
       "      <td>-0.807582</td>\n",
       "      <td>-0.793571</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.230864</td>\n",
       "      <td>-0.185613</td>\n",
       "      <td>-0.123410</td>\n",
       "      <td>-0.103705</td>\n",
       "      <td>-0.064729</td>\n",
       "      <td>-0.015466</td>\n",
       "      <td>-0.015621</td>\n",
       "      <td>-0.002916</td>\n",
       "      <td>0.016450</td>\n",
       "      <td>0.025601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14993</th>\n",
       "      <td>10.542854</td>\n",
       "      <td>10.658619</td>\n",
       "      <td>10.757838</td>\n",
       "      <td>11.039179</td>\n",
       "      <td>11.090972</td>\n",
       "      <td>11.003048</td>\n",
       "      <td>11.239927</td>\n",
       "      <td>11.367652</td>\n",
       "      <td>11.317278</td>\n",
       "      <td>11.330967</td>\n",
       "      <td>...</td>\n",
       "      <td>4.974709</td>\n",
       "      <td>4.905933</td>\n",
       "      <td>4.621348</td>\n",
       "      <td>4.488353</td>\n",
       "      <td>4.337437</td>\n",
       "      <td>4.176043</td>\n",
       "      <td>4.079860</td>\n",
       "      <td>3.806188</td>\n",
       "      <td>3.723212</td>\n",
       "      <td>3.496791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14994</th>\n",
       "      <td>-0.726089</td>\n",
       "      <td>-0.765403</td>\n",
       "      <td>-0.794750</td>\n",
       "      <td>-0.814925</td>\n",
       "      <td>-0.827116</td>\n",
       "      <td>-0.842993</td>\n",
       "      <td>-0.835535</td>\n",
       "      <td>-0.910961</td>\n",
       "      <td>-0.902745</td>\n",
       "      <td>-0.905741</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.476440</td>\n",
       "      <td>-0.341955</td>\n",
       "      <td>-0.210617</td>\n",
       "      <td>-0.073119</td>\n",
       "      <td>0.078119</td>\n",
       "      <td>0.215589</td>\n",
       "      <td>0.373006</td>\n",
       "      <td>0.494659</td>\n",
       "      <td>2.396596</td>\n",
       "      <td>2.999382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>-0.686256</td>\n",
       "      <td>-0.658683</td>\n",
       "      <td>-0.612488</td>\n",
       "      <td>-0.579372</td>\n",
       "      <td>-0.592994</td>\n",
       "      <td>-0.553942</td>\n",
       "      <td>-0.554520</td>\n",
       "      <td>-0.523448</td>\n",
       "      <td>-0.505181</td>\n",
       "      <td>-0.430536</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.565715</td>\n",
       "      <td>-0.604476</td>\n",
       "      <td>-0.620838</td>\n",
       "      <td>-0.649806</td>\n",
       "      <td>-0.659898</td>\n",
       "      <td>-0.704340</td>\n",
       "      <td>-0.723726</td>\n",
       "      <td>-0.746698</td>\n",
       "      <td>-0.754156</td>\n",
       "      <td>-0.782583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>-0.990781</td>\n",
       "      <td>-0.992988</td>\n",
       "      <td>-0.989279</td>\n",
       "      <td>-0.993904</td>\n",
       "      <td>-0.992295</td>\n",
       "      <td>-0.995414</td>\n",
       "      <td>-0.994422</td>\n",
       "      <td>-0.996100</td>\n",
       "      <td>-0.993474</td>\n",
       "      <td>-0.993745</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999044</td>\n",
       "      <td>-0.998888</td>\n",
       "      <td>-0.999430</td>\n",
       "      <td>-0.998339</td>\n",
       "      <td>-0.998610</td>\n",
       "      <td>-0.999012</td>\n",
       "      <td>-0.999016</td>\n",
       "      <td>-0.998769</td>\n",
       "      <td>-0.997996</td>\n",
       "      <td>-0.997498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>-0.962323</td>\n",
       "      <td>-0.952044</td>\n",
       "      <td>-0.968689</td>\n",
       "      <td>-0.976391</td>\n",
       "      <td>-0.973510</td>\n",
       "      <td>-0.967861</td>\n",
       "      <td>-0.965825</td>\n",
       "      <td>-0.975594</td>\n",
       "      <td>-0.973279</td>\n",
       "      <td>-0.977653</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.981155</td>\n",
       "      <td>-0.989582</td>\n",
       "      <td>-0.988801</td>\n",
       "      <td>-0.979395</td>\n",
       "      <td>-0.984359</td>\n",
       "      <td>-0.974669</td>\n",
       "      <td>-0.974534</td>\n",
       "      <td>-0.992940</td>\n",
       "      <td>-0.982251</td>\n",
       "      <td>-0.982558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>1.158175</td>\n",
       "      <td>0.982885</td>\n",
       "      <td>0.930419</td>\n",
       "      <td>0.897482</td>\n",
       "      <td>0.899546</td>\n",
       "      <td>0.880060</td>\n",
       "      <td>0.923104</td>\n",
       "      <td>0.977064</td>\n",
       "      <td>0.958136</td>\n",
       "      <td>0.910024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146398</td>\n",
       "      <td>0.030207</td>\n",
       "      <td>-0.009964</td>\n",
       "      <td>-0.070231</td>\n",
       "      <td>-0.059414</td>\n",
       "      <td>-0.050896</td>\n",
       "      <td>-0.043478</td>\n",
       "      <td>-0.042641</td>\n",
       "      <td>-0.009032</td>\n",
       "      <td>-0.020052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>-0.031438</td>\n",
       "      <td>0.005203</td>\n",
       "      <td>0.047904</td>\n",
       "      <td>0.099585</td>\n",
       "      <td>0.130545</td>\n",
       "      <td>0.163294</td>\n",
       "      <td>0.241812</td>\n",
       "      <td>0.316321</td>\n",
       "      <td>0.362867</td>\n",
       "      <td>0.446158</td>\n",
       "      <td>...</td>\n",
       "      <td>1.212641</td>\n",
       "      <td>1.206931</td>\n",
       "      <td>1.231955</td>\n",
       "      <td>1.221649</td>\n",
       "      <td>1.246027</td>\n",
       "      <td>1.156645</td>\n",
       "      <td>1.133362</td>\n",
       "      <td>1.156250</td>\n",
       "      <td>1.133645</td>\n",
       "      <td>1.115669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1          2          3          4          5   \\\n",
       "0       1.747332   1.756113   1.783443   1.733874   1.782499   1.760778   \n",
       "1      -0.347915  -0.329309  -0.320321  -0.300955  -0.335807  -0.310580   \n",
       "2      -0.982657  -0.989279  -0.988785  -0.989422  -0.986673  -0.988729   \n",
       "3      -0.871435  -0.888646  -0.875710  -0.876518  -0.856152  -0.858080   \n",
       "4      -0.987502  -0.989183  -0.990673  -0.989060  -0.992171  -0.992498   \n",
       "5      -0.999386  -0.999590  -0.999765  -0.999614  -0.999462  -0.999578   \n",
       "6       7.811347   7.725216   7.585767   7.585906   7.461285   7.488974   \n",
       "7      -0.739854  -0.754208  -0.733854  -0.726555  -0.730543  -0.720611   \n",
       "8       0.818355   0.943853   0.977016   1.048777   1.039964   1.108968   \n",
       "9      -0.977311  -0.980136  -0.972865  -0.973677  -0.971885  -0.978594   \n",
       "10     -0.441588  -0.457548  -0.432939  -0.345748  -0.512484  -0.529524   \n",
       "11      3.196732   3.008940   2.806041   2.678515   2.521125   2.399194   \n",
       "12     -0.844845  -0.862399  -0.879937  -0.895024  -0.925227  -0.916857   \n",
       "13     -0.973570  -0.979140  -0.980594  -0.982522  -0.978088  -0.976805   \n",
       "14     -0.999187  -0.997928  -0.999275  -0.997665  -0.999454  -0.998606   \n",
       "15     -0.995701  -0.996880  -0.997303  -0.996052  -0.996916  -0.995765   \n",
       "16     -0.948809  -0.955809  -0.952506  -0.965757  -0.959171  -0.956024   \n",
       "17      1.281369   1.303102   1.339995   1.384038   1.548799   1.615324   \n",
       "18      0.428006   0.471373   0.527281   0.590759   0.723073   0.755862   \n",
       "19     -0.851877  -0.796774  -0.734766  -0.691949  -0.589281  -0.523787   \n",
       "20     -0.259700  -0.281122  -0.308676  -0.356740  -0.333393  -0.360803   \n",
       "21     -0.030713  -0.074159  -0.120585  -0.193912  -0.248852  -0.288879   \n",
       "22     -0.984725  -0.989299  -0.992147  -0.991518  -0.990016  -0.992892   \n",
       "23      0.195378   0.126924   0.075506   0.017044  -0.012251  -0.030713   \n",
       "24      1.992430   1.913905   1.795272   1.760077   1.698910   1.644519   \n",
       "25      1.899008   1.910128   2.018781   2.016183   2.083641   2.066442   \n",
       "26      0.899271   0.976239   1.091115   1.181079   1.259310   1.296804   \n",
       "27     -0.473707  -0.484974  -0.468887  -0.484731  -0.462034  -0.442648   \n",
       "28      3.902192   3.911797   4.129980   4.085478   4.178465   4.179517   \n",
       "29     -0.953642  -0.952897  -0.952104  -0.964893  -0.961399  -0.971422   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "14970   2.416237   2.459301   2.451305   2.436066   2.451329   2.472739   \n",
       "14971  -0.031366  -0.013809  -0.059797  -0.084800  -0.054554  -0.046378   \n",
       "14972  -1.000000  -1.000000  -1.000000  -1.000000  -1.000000  -0.999992   \n",
       "14973  -0.943777  -0.946574  -0.952315  -0.950733  -0.947168  -0.948116   \n",
       "14974  -0.974936  -0.976972  -0.982283  -0.987928  -0.979279  -0.978056   \n",
       "14975  -0.997542  -0.998179  -0.997331  -0.997693  -0.996709  -0.995470   \n",
       "14976  -0.503460  -0.530460  -0.459070  -0.466241  -0.457464  -0.467827   \n",
       "14977  -0.992940  -0.992036  -0.995171  -0.995255  -0.997096  -0.994534   \n",
       "14978  -0.811084  -0.799833  -0.805921  -0.802929  -0.806758  -0.768881   \n",
       "14979   0.697252   0.835463   1.005203   1.127609   1.199673   1.373544   \n",
       "14980  -0.851264  -0.859722  -0.871144  -0.892243  -0.851746  -0.848674   \n",
       "14981   2.249370   2.189322   2.056844   1.946263   1.962578   1.812802   \n",
       "14982   0.858204   0.939100   1.068207   1.173736   1.248648   1.349544   \n",
       "14983  -0.411576  -0.389552  -0.334859  -0.292031  -0.255198  -0.212067   \n",
       "14984  10.621822  10.389887  10.390724  10.170473  10.251768  10.233007   \n",
       "14985   4.273971   4.307983   4.340740   4.274075   4.280863   4.128446   \n",
       "14986  -0.994490  -0.995665  -0.995231  -0.996080  -0.996179  -0.996482   \n",
       "14987  -0.445580  -0.420767  -0.434329  -0.421086  -0.473982  -0.538396   \n",
       "14988  -0.961219  -0.954399  -0.957498  -0.956140  -0.962430  -0.961928   \n",
       "14989  -0.930952  -0.926753  -0.920721  -0.907196  -0.899909  -0.876108   \n",
       "14990  -0.021809   0.085219   0.145948   0.270003   0.262338   0.379313   \n",
       "14991   3.868403   4.033219   4.030048   4.099227   3.993924   4.180896   \n",
       "14992  -0.645647  -0.697914  -0.723439  -0.755559  -0.778909  -0.787351   \n",
       "14993  10.542854  10.658619  10.757838  11.039179  11.090972  11.003048   \n",
       "14994  -0.726089  -0.765403  -0.794750  -0.814925  -0.827116  -0.842993   \n",
       "14995  -0.686256  -0.658683  -0.612488  -0.579372  -0.592994  -0.553942   \n",
       "14996  -0.990781  -0.992988  -0.989279  -0.993904  -0.992295  -0.995414   \n",
       "14997  -0.962323  -0.952044  -0.968689  -0.976391  -0.973510  -0.967861   \n",
       "14998   1.158175   0.982885   0.930419   0.897482   0.899546   0.880060   \n",
       "14999  -0.031438   0.005203   0.047904   0.099585   0.130545   0.163294   \n",
       "\n",
       "              6          7          8          9     ...           51  \\\n",
       "0       1.733487   1.796965   1.793049   1.811925    ...     2.012721   \n",
       "1      -0.337604  -0.345903  -0.325298  -0.339612    ...    -0.913941   \n",
       "2      -0.991582  -0.991008  -0.989745  -0.984263    ...    -0.986944   \n",
       "3      -0.860475  -0.863614  -0.872228  -0.879745    ...     1.619332   \n",
       "4      -0.991235  -0.991745  -0.993466  -0.992928    ...    -0.997805   \n",
       "5      -0.999709  -0.999590  -0.999725  -0.999697    ...    -0.997952   \n",
       "6       7.522751   7.650599   7.842706   8.800746    ...     3.030251   \n",
       "7      -0.726778  -0.731244  -0.742913  -0.737961    ...    -0.866391   \n",
       "8       1.134999   1.106984   1.182752   1.236306    ...     0.791571   \n",
       "9      -0.977546  -0.974024  -0.972040  -0.976163    ...    -0.955307   \n",
       "10     -0.523719  -0.537850  -0.567488  -0.599069    ...    -0.844746   \n",
       "11      2.206370   2.130625   1.959347   1.814527    ...     4.345070   \n",
       "12     -0.935450  -0.944470  -0.957399  -0.947036    ...    -0.976494   \n",
       "13     -0.971530  -0.982072  -0.961602  -0.975721    ...    -0.464209   \n",
       "14     -0.999359  -0.997386  -0.997040  -0.998594    ...    -0.999618   \n",
       "15     -0.997673  -0.995773  -0.998570  -0.996745    ...    -0.881686   \n",
       "16     -0.961044  -0.962144  -0.937709  -0.959777    ...    -0.765734   \n",
       "17      1.683220   1.692184   1.697846   1.731300    ...     3.363584   \n",
       "18      0.813001   0.839136   0.940825   0.946331    ...     1.963307   \n",
       "19     -0.454695  -0.410305  -0.358540  -0.332333    ...    -0.110358   \n",
       "20     -0.365544  -0.405947  -0.383385  -0.404293    ...    -0.670579   \n",
       "21     -0.349162  -0.402544  -0.438257  -0.466779    ...    -0.487209   \n",
       "22     -0.994904  -0.991259  -0.990618  -0.994837    ...    -0.815547   \n",
       "23     -0.074159  -0.120585  -0.193912  -0.248852    ...    -0.558109   \n",
       "24      1.629974   1.566448   1.536010   1.419775    ...     1.748682   \n",
       "25      2.136804   2.151187   2.178928   2.157171    ...     5.040880   \n",
       "26      1.307628   1.393939   1.422349   1.409524    ...     3.265306   \n",
       "27     -0.410783  -0.362106  -0.330134  -0.261354    ...     0.483193   \n",
       "28      4.147191   4.181553   4.226625   4.224330    ...     2.671077   \n",
       "29     -0.968367  -0.964371  -0.969972  -0.969359    ...    -0.920976   \n",
       "...          ...        ...        ...        ...    ...          ...   \n",
       "14970   2.533500   2.395377   2.438460   2.448520    ...     3.089139   \n",
       "14971   0.032657   0.028119   0.018410   0.035235    ...     1.162099   \n",
       "14972  -0.999956  -0.999976  -1.000000  -0.999988    ...    -1.000000   \n",
       "14973  -0.948809  -0.957371  -0.957243  -0.943932    ...    -0.673639   \n",
       "14974  -0.975092  -0.966634  -0.951490  -0.931674    ...     0.346441   \n",
       "14975  -0.991940  -0.991108  -0.986498  -0.983136    ...    -0.999745   \n",
       "14976  -0.448162  -0.456424  -0.425273  -0.400222    ...    -0.555787   \n",
       "14977  -0.996916  -0.994829  -0.996869  -0.999785    ...     0.114661   \n",
       "14978  -0.757515  -0.704599  -0.619762  -0.499871    ...    -0.541775   \n",
       "14979   1.409090   1.559225   1.547460   1.653894    ...     5.913347   \n",
       "14980  -0.853789  -0.847877  -0.826503  -0.841857    ...    -0.947964   \n",
       "14981   1.833363   1.798790   1.804969   1.676961    ...     0.182736   \n",
       "14982   1.353505   1.406779   1.487118   1.509604    ...     2.125768   \n",
       "14983  -0.188947  -0.104561  -0.067151  -0.028470    ...     1.592950   \n",
       "14984  10.277636  10.217876  10.097235  10.081506    ...     1.724160   \n",
       "14985   4.104506   4.069565   4.033231   3.826268    ...     0.252318   \n",
       "14986  -0.995550  -0.993952  -0.995522  -0.992542    ...    -0.993518   \n",
       "14987  -0.550428  -0.555137  -0.610631  -0.610974    ...    -0.971590   \n",
       "14988  -0.966801  -0.969486  -0.951454  -0.969211    ...    -0.998036   \n",
       "14989  -0.851104  -0.810487  -0.772292  -0.742889    ...    -0.548137   \n",
       "14990   0.403138   0.446488   0.436456   0.575783    ...     2.667037   \n",
       "14991   4.109908   4.182609   4.101996   4.141944    ...     4.454556   \n",
       "14992  -0.795216  -0.800638  -0.807582  -0.793571    ...    -0.230864   \n",
       "14993  11.239927  11.367652  11.317278  11.330967    ...     4.974709   \n",
       "14994  -0.835535  -0.910961  -0.902745  -0.905741    ...    -0.476440   \n",
       "14995  -0.554520  -0.523448  -0.505181  -0.430536    ...    -0.565715   \n",
       "14996  -0.994422  -0.996100  -0.993474  -0.993745    ...    -0.999044   \n",
       "14997  -0.965825  -0.975594  -0.973279  -0.977653    ...    -0.981155   \n",
       "14998   0.923104   0.977064   0.958136   0.910024    ...     0.146398   \n",
       "14999   0.241812   0.316321   0.362867   0.446158    ...     1.212641   \n",
       "\n",
       "             52        53        54        55        56        57        58  \\\n",
       "0      1.943383  1.967673  1.933721  1.930773  1.897056  1.879423  1.845945   \n",
       "1     -0.915395 -0.936435 -0.915443 -0.928152 -0.918000 -0.916056 -0.913570   \n",
       "2     -0.990143 -0.988367 -0.988303 -0.985980 -0.990470 -0.989203 -0.991705   \n",
       "3      1.763838  1.847650  1.868283  1.978976  2.027354  2.109880  2.181175   \n",
       "4     -0.998323 -0.993857 -0.987183 -0.989303 -0.990394 -0.989677 -0.982371   \n",
       "5     -0.984841 -0.972060 -0.932853 -0.889694 -0.827503 -0.800061 -0.759184   \n",
       "6      3.010283  2.928144  2.842527  2.654376  2.648627  2.655695  2.699806   \n",
       "7     -0.859359 -0.862184 -0.857040 -0.853522 -0.832005 -0.804734 -0.811168   \n",
       "8      0.825367  0.869208  0.855459  0.863670  0.857538  0.862758  0.858475   \n",
       "9     -0.952741 -0.949917 -0.961809 -0.957944 -0.969920 -0.962514 -0.961167   \n",
       "10    -0.815590 -0.757981 -0.740678 -0.767435 -0.785786 -0.795798 -0.639790   \n",
       "11     4.409046  4.532333  4.686216  4.766583  4.893132  5.124342  5.232087   \n",
       "12    -0.968036 -0.971403 -0.969518 -0.967132 -0.975893 -0.966988 -0.967948   \n",
       "13    -0.495867 -0.497910 -0.469329 -0.474560 -0.544329 -0.548213 -0.626826   \n",
       "14    -0.999745 -0.999147 -0.998677 -0.999673 -0.999530 -0.999960 -0.996546   \n",
       "15    -0.874562 -0.864574 -0.864750 -0.884865 -0.886957 -0.886666 -0.887335   \n",
       "16    -0.753395 -0.729678 -0.745248 -0.745013 -0.731822 -0.734551 -0.755407   \n",
       "17     3.554380  3.645707  3.806630  3.913339  3.972140  4.190282  4.394233   \n",
       "18     1.953307  1.964136  1.825443  1.803917  1.675902  1.555512  1.414500   \n",
       "19    -0.167669 -0.200406 -0.214378 -0.373293 -0.408839 -0.415441 -0.450516   \n",
       "20    -0.651985 -0.635153 -0.620810 -0.580599 -0.581691 -0.581719 -0.570520   \n",
       "21    -0.432608 -0.461524 -0.457787 -0.435500 -0.466962 -0.364664 -0.419696   \n",
       "22    -0.745156 -0.719113 -0.621005 -0.580563 -0.558815 -0.436285 -0.317385   \n",
       "23    -0.555046 -0.543468 -0.548799 -0.459604 -0.487209 -0.432608 -0.461524   \n",
       "24     1.838116  1.851977  1.783021  1.828323  1.876136  1.858295  1.838056   \n",
       "25     5.003382  5.135748  5.277330  5.364154  5.462911  5.555819  5.667149   \n",
       "26     3.360134  3.436134  3.468185  3.582898  3.595950  3.674734  3.743559   \n",
       "27     0.488811  0.460536  0.468464  0.500584  0.450899  0.456014  0.470142   \n",
       "28     2.569579  2.406174  2.404409  2.202103  2.094163  1.973936  1.613400   \n",
       "29    -0.903861 -0.920136 -0.900339 -0.898741 -0.882722 -0.886052 -0.882538   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "14970  3.083892  3.104151  3.161545  3.183187  3.209370  3.285592  3.422747   \n",
       "14971  1.092661  1.024912  0.978586  0.955203  1.031163  1.051207  1.124482   \n",
       "14972 -1.000000 -1.000000 -1.000000 -1.000000 -1.000000 -0.999988 -1.000000   \n",
       "14973 -0.697288 -0.701412 -0.734188 -0.729941 -0.734041 -0.761069 -0.769718   \n",
       "14974  0.332899  0.312620  0.313286  0.293915  0.315433  0.304007  0.290501   \n",
       "14975 -0.988359 -0.989773 -0.988187 -0.992777 -0.992809 -0.990992 -0.989944   \n",
       "14976 -0.575201 -0.538074  0.021382  0.034697  0.097239  0.116776  0.074797   \n",
       "14977  0.145880  0.139529  0.249812  0.228633  0.202099  0.161230  0.167493   \n",
       "14978 -0.505902 -0.510870 -0.509333 -0.493922 -0.541360 -0.523655 -0.556882   \n",
       "14979  5.849694  5.579034  5.766599  5.852076  6.264688  6.346712  6.591671   \n",
       "14980 -0.946160 -0.950610 -0.955140 -0.975092 -0.970673 -0.964084 -0.978056   \n",
       "14981  0.299533  0.378843  0.333525  0.349421  0.329138  0.391915  0.358696   \n",
       "14982  2.046661  1.863076  1.817606  1.678240  1.585671  1.556627  1.447632   \n",
       "14983  1.567663  1.544285  1.458074  1.393652  1.292943  1.180577  1.092904   \n",
       "14984  1.699559  1.640025  1.065621  0.995151  0.884439  0.907024  0.919486   \n",
       "14985  0.101502  0.204402  0.178987  0.172203  0.189222  0.096960  0.097406   \n",
       "14986 -0.994470 -0.994956 -0.996108 -0.996052 -0.993379 -0.994020 -0.994801   \n",
       "14987 -0.974877 -0.991649 -0.998426 -0.999669 -0.999849 -0.999845 -0.999853   \n",
       "14988 -0.997629 -0.999474 -0.998291 -0.998980 -0.998490 -0.999398 -0.997697   \n",
       "14989 -0.523213 -0.522488 -0.541580 -0.517062 -0.539659 -0.525500 -0.523520   \n",
       "14990  2.465755  2.434711  2.244230  2.139302  1.869738  1.632452  1.420692   \n",
       "14991  4.510508  4.529488  4.479564  4.566834  4.566559  4.583615  4.660364   \n",
       "14992 -0.185613 -0.123410 -0.103705 -0.064729 -0.015466 -0.015621 -0.002916   \n",
       "14993  4.905933  4.621348  4.488353  4.337437  4.176043  4.079860  3.806188   \n",
       "14994 -0.341955 -0.210617 -0.073119  0.078119  0.215589  0.373006  0.494659   \n",
       "14995 -0.604476 -0.620838 -0.649806 -0.659898 -0.704340 -0.723726 -0.746698   \n",
       "14996 -0.998888 -0.999430 -0.998339 -0.998610 -0.999012 -0.999016 -0.998769   \n",
       "14997 -0.989582 -0.988801 -0.979395 -0.984359 -0.974669 -0.974534 -0.992940   \n",
       "14998  0.030207 -0.009964 -0.070231 -0.059414 -0.050896 -0.043478 -0.042641   \n",
       "14999  1.206931  1.231955  1.221649  1.246027  1.156645  1.133362  1.156250   \n",
       "\n",
       "             59        60  \n",
       "0      1.766120  1.654005  \n",
       "1     -0.907610 -0.920678  \n",
       "2     -0.989072 -0.991291  \n",
       "3      2.186740  2.185190  \n",
       "4     -0.973813 -0.981024  \n",
       "5     -0.642033 -0.512743  \n",
       "6      2.705945  2.736443  \n",
       "7     -0.786108 -0.788156  \n",
       "8      0.889367  0.967211  \n",
       "9     -0.957944 -0.964538  \n",
       "10    -0.587890 -0.718662  \n",
       "11     5.361724  5.417361  \n",
       "12    -0.965769 -0.966418  \n",
       "13    -0.625069 -0.719021  \n",
       "14    -0.999179 -0.999876  \n",
       "15    -0.891247 -0.899781  \n",
       "16    -0.669033 -0.715443  \n",
       "17     4.536189  4.639248  \n",
       "18     1.285684  1.157318  \n",
       "19    -0.456078 -0.458281  \n",
       "20    -0.579715 -0.558472  \n",
       "21    -0.376823 -0.337178  \n",
       "22    -0.295262 -0.257318  \n",
       "23    -0.457787 -0.435500  \n",
       "24     1.815060  1.899008  \n",
       "25     5.741324  5.912514  \n",
       "26     3.788622  3.806539  \n",
       "27     0.456743  0.454787  \n",
       "28     1.190426  0.734941  \n",
       "29    -0.881933 -0.846427  \n",
       "...         ...       ...  \n",
       "14970  3.439962  3.462162  \n",
       "14971  1.131478  1.159613  \n",
       "14972 -1.000000 -1.000000  \n",
       "14973 -0.760790 -0.785674  \n",
       "14974  0.283334  0.260832  \n",
       "14975 -0.990904 -0.987960  \n",
       "14976  0.241557  0.303628  \n",
       "14977  0.232497  0.210035  \n",
       "14978 -0.502273 -0.544735  \n",
       "14979  6.784164  6.960940  \n",
       "14980 -0.976243 -0.982231  \n",
       "14981  0.374624  0.342744  \n",
       "14982  0.614615  0.464695  \n",
       "14983  0.983343  0.882008  \n",
       "14984  0.904785  0.896359  \n",
       "14985  0.074518  0.094446  \n",
       "14986 -0.994175 -0.993251  \n",
       "14987 -0.999908 -0.999817  \n",
       "14988 -0.995829 -0.998255  \n",
       "14989 -0.514153 -0.503942  \n",
       "14990  1.242565  1.090047  \n",
       "14991  4.592053  4.899566  \n",
       "14992  0.016450  0.025601  \n",
       "14993  3.723212  3.496791  \n",
       "14994  2.396596  2.999382  \n",
       "14995 -0.754156 -0.782583  \n",
       "14996 -0.997996 -0.997498  \n",
       "14997 -0.982251 -0.982558  \n",
       "14998 -0.009032 -0.020052  \n",
       "14999  1.133645  1.115669  \n",
       "\n",
       "[15000 rows x 61 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f81585e5a90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x=0,y=1,data=distribution_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8158685b00>,\n",
       " <matplotlib.lines.Line2D at 0x7f8158685c50>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(distribution_list[:,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_RAD_id(RAD_id):\n",
    "#     return RAD_id\n",
    "    mean_list = []\n",
    "    for k in range(61):\n",
    "        mean_list.append(np.array(PIL.Image.open('/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png'\n",
    "                         % (RAD_id, RAD_id,\n",
    "                        k))).astype(np.int8).ravel().mean())\n",
    "    mean_list = np.array(mean_list)\n",
    "    if mean_list.mean() < -0.5:\n",
    "        return None\n",
    "    for k in range(59):\n",
    "        if abs(mean_list[k] + mean_list[k + 2] - 2 * mean_list[k + 1]) > 2:\n",
    "            return None\n",
    "    return RAD_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool()\n",
    "start_time = time.time()\n",
    "print(time.strftime('start time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "# map(check_RAD_id, raw_RAD_id_list[:100])\n",
    "# print(time.strftime('start time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "RAD_id_list = list(pool.map(check_RAD_id, raw_RAD_id_list))\n",
    "RAD_id_list = [x for x in RAD_id_list if x is not None]\n",
    "print(time.strftime('end time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "print(time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time)))\n",
    "print(len(RAD_id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 5\n",
    "\n",
    "class trainGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, nt, image_size, image_scalar, batch_size=32, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.nt = nt\n",
    "        self.image_size = image_size\n",
    "        self.image_scalar = image_scalar\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "#         'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "#         X = np.empty((self.batch_size, self.nt, self.image_size, self.image_size, 3))\n",
    "# #         y = np.empty((self.batch_size, self.image_size, self.image_size, 1))\n",
    "#         for i, RAD_id in enumerate(list_IDs_temp):\n",
    "#             offset = random.randint(0, 61 - self.nt * step_size)\n",
    "#             offset = random.randint(2, 59 - self.nt * step_size)\n",
    "#             for j in range(self.nt):\n",
    "# #                 X[i][j] = (np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset)).resize((self.image_size, self.image_size))).astype(np.int8).reshape((self.image_size, self.image_size, 1)) / self.image_scalar + \n",
    "# #                     np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset + 1)).resize((self.image_size, self.image_size))).astype(np.int8).reshape((self.image_size, self.image_size, 1)) / self.image_scalar + \n",
    "# #                     np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset - 1)).resize((self.image_size, self.image_size))).astype(np.int8).reshape((self.image_size, self.image_size, 1)) / self.image_scalar) / 3\n",
    "#                 temp_matrix = np.empty((3, self.image_size, self.image_size))\n",
    "#                 temp_matrix[0] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset - 1)).resize((self.image_size, self.image_size))).astype(np.int8) / self.image_scalar + \\\n",
    "#                                  np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset - 2)).resize((self.image_size, self.image_size))).astype(np.int8) / self.image_scalar\n",
    "#                 temp_matrix[1] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset)).resize((self.image_size, self.image_size))).astype(np.int8) / self.image_scalar * 2\n",
    "#                 temp_matrix[2] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset + 1)).resize((self.image_size, self.image_size))).astype(np.int8) / self.image_scalar + \\\n",
    "#                                  np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset + 2)).resize((self.image_size, self.image_size))).astype(np.int8) / self.image_scalar\n",
    "# #                 temp_matrix[2] = temp_matrix[2] - temp_matrix[1]\n",
    "# #                 temp_matrix[0] = temp_matrix[1] - temp_matrix[0]\n",
    "#                 temp_matrix[0] = cv2.GaussianBlur(temp_matrix[1], (5, 5), 0)\n",
    "#                 temp_matrix[1] = cv2.GaussianBlur(temp_matrix[1], (9, 9), 0)\n",
    "#                 temp_matrix[2] = cv2.GaussianBlur(temp_matrix[1], (13, 13), 0)\n",
    "#                 temp_matrix = np.rollaxis(temp_matrix, 0, 3)\n",
    "#                 X[i][j] = temp_matrix\n",
    "# #             y[i] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, (self.nt) * 1)).resize((self.image_size, self.image_size))).astype(np.int8).reshape((self.image_size, self.image_size, 1)) / self.image_scalar\n",
    "        X, y = data_generation(list_IDs_temp, batch_size=self.batch_size, image_size=self.image_size, nt=self.nt, step_size=step_size, image_scalar=self.image_scalar)\n",
    "        y = np.zeros(self.batch_size, np.float32)\n",
    "        return X, y\n",
    "print(time.strftime('build time: %Y-%m-%d %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = 4\n",
    "def data_generation(list_IDs_temp, batch_size, image_size, nt, step_size, image_scalar, offset=None, path='/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train'):\n",
    "    '''\n",
    "    nt * step_size + offset = 60\n",
    "    '''\n",
    "    X = np.empty((batch_size, nt, image_size, image_size, n_channels))\n",
    "    y = np.empty((batch_size, image_size, image_size, n_channels))\n",
    "    for i, RAD_id in enumerate(list_IDs_temp):\n",
    "#         offset = random.randint(0, 61 - nt * step_size)\n",
    "        if offset == None:\n",
    "            offset = random.randint(2, 59 - nt * step_size)\n",
    "        for j in range(nt):\n",
    "            temp_matrix = np.empty((n_channels, image_size, image_size))\n",
    "#             temp_matrix[0] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset - 1)).resize((image_size, image_size))).astype(np.int8) / image_scalar + \\\n",
    "#                              np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset - 2)).resize((image_size, image_size))).astype(np.int8) / image_scalar\n",
    "            temp_matrix[0] = np.array(PIL.Image.open(\"%s/%s/%s_%03d.png\" % (path, RAD_id, RAD_id, j * step_size + offset)).resize((image_size, image_size))).astype(np.int8) / image_scalar\n",
    "#             temp_matrix[2] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset + 1)).resize((image_size, image_size))).astype(np.int8) / image_scalar + \\\n",
    "#                              np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset + 2)).resize((image_size, image_size))).astype(np.int8) / image_scalar\n",
    "            temp_matrix[1] = cv2.GaussianBlur(temp_matrix[0], (5, 5), 0)\n",
    "            temp_matrix[2] = cv2.GaussianBlur(temp_matrix[0], (9, 9), 0)\n",
    "            temp_matrix[3] = cv2.GaussianBlur(temp_matrix[0], (13, 13), 0)\n",
    "            temp_matrix = np.rollaxis(temp_matrix, 0, 3)\n",
    "            X[i][j] = temp_matrix\n",
    "        temp_matrix = np.empty((n_channels, image_size, image_size))\n",
    "        temp_matrix[0] = np.array(PIL.Image.open(\"%s/%s/%s_%03d.png\" % (path, RAD_id, RAD_id, nt * step_size + offset)).resize((image_size, image_size))).astype(np.int8) / image_scalar\n",
    "        temp_matrix[1] = cv2.GaussianBlur(temp_matrix[0], (5, 5), 0)\n",
    "        temp_matrix[2] = cv2.GaussianBlur(temp_matrix[0], (9, 9), 0)\n",
    "        temp_matrix[3] = cv2.GaussianBlur(temp_matrix[0], (13, 13), 0)\n",
    "        temp_matrix = np.rollaxis(temp_matrix, 0, 3)\n",
    "        y[i] = temp_matrix\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "image_size = 512\n",
    "nt = 6 # number of timesteps used for sequences in training\n",
    "image_scalar = 80\n",
    "vmin = -1\n",
    "vmax = 0.6 * image_scalar\n",
    "n_channels, im_height, im_width = (n_channels, image_size, image_size)  # (3, 128, 160)\n",
    "input_shape = (n_channels, im_height, im_width) if K.image_data_format() == 'channels_first' else (im_height, im_width, n_channels)\n",
    "stack_sizes = (n_channels, 48, 96)  # 48, 96, 192\n",
    "R_stack_sizes = stack_sizes\n",
    "A_filt_sizes = (4, 4)\n",
    "Ahat_filt_sizes = (4, 4, 4)\n",
    "R_filt_sizes = (4, 4, 4)\n",
    "layer_loss_weights = np.array([1., 0.1, 0.1])  # weighting for each layer in final loss; \"L_0\" model:  [1, 0, 0, 0], \"L_all\": [1, 0.1, 0.1, 0.1]\n",
    "layer_loss_weights = np.expand_dims(layer_loss_weights, 1)\n",
    "time_loss_weights = 1./ (nt - 1) * np.ones((nt,1))  # equally weight all timesteps except the first\n",
    "time_loss_weights[0] = 0\n",
    "\n",
    "\n",
    "prednet = PredNet(stack_sizes, R_stack_sizes,\n",
    "                  A_filt_sizes, Ahat_filt_sizes, R_filt_sizes,\n",
    "                  A_activation=tf.nn.elu, error_activation=tf.nn.elu,\n",
    "                  LSTM_inner_activation='sigmoid',\n",
    "                  output_mode='error', return_sequences=True, extrap_start_time=2)\n",
    "\n",
    "inputs = Input(shape=(nt,) + input_shape)\n",
    "errors = prednet(inputs)  # errors will be (batch_size, nt, nb_layers)\n",
    "errors_by_time = TimeDistributed(Dense(1, trainable=False), weights=[layer_loss_weights, np.zeros(1)], trainable=False)(errors)  # calculate weighted error by layer\n",
    "errors_by_time = Flatten()(errors_by_time)  # will be (batch_size, nt)\n",
    "final_errors = Dense(1, weights=[time_loss_weights, np.zeros(1)], trainable=False)(errors_by_time)  # weight errors by time\n",
    "outputs = final_errors\n",
    "# outputs = np.empty(shape=(nt,) + input_shape)\n",
    "# for i in range(nt):\n",
    "#     temp_outputs = prednet(inputs)\n",
    "#     outputs[i] = temp_outputs\n",
    "#     for j in range(nt - 1):\n",
    "#         inputs[j] = inputs[j + 1]\n",
    "#     inputs[-1] = temp_outputs\n",
    "\n",
    "def my_loss(y_true, y_pred):\n",
    "#     w = tf.add(y_true, tf.constant(0.5))\n",
    "#     w = tf.add(y_pred, w)\n",
    "#     loss = tf.losses.mean_squared_error(y_true, y_pred, weights=w)\n",
    "#     loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "    loss = tf.losses.absolute_difference(y_true, y_pred)\n",
    "#     loss = tf.losses.log_loss(y_true, y_pred)\n",
    "#     loss = tf.multiply(loss, tf.constant(1000000000.0))\n",
    "    return loss\n",
    "\n",
    "model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss=my_loss, optimizer=keras.optimizers.Adam())\n",
    "print(time.strftime('build time: %Y-%m-%d %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to use tensorboard\n",
    "\n",
    "```bash\n",
    "jupyter notebook --ip=192.168.2.101\n",
    "python3 /usr/local/lib/python3.5/dist-packages/tensorboard/main.py --logdir='/home/hadoop/Documents/Neutrino/tensorboard_logs' --host=192.168.2.101 --port=8900\n",
    "rm -rf /home/hadoop/Documents/Neutrino/tensorboard_logs\n",
    "scp \"C:/Users/Administrator/Downloads/SRAD2018_Test_1.zip\" hadoop@192.168.1.115:~/Documents/Neutrino/SRAD2018/SRAD2018_test\n",
    "mv -v ~/Documents/Neutrino/SRAD2018/SRAD2018_test/SRAD2018_Test_1* ~/Documents/Neutrino/SRAD2018/SRAD2018_test\n",
    "lsof /dev/nvidia0\n",
    "lsof -n -i4TCP:8888\n",
    "kill -9 -[PID]\n",
    "```\n",
    "\n",
    "`http://222.200.177.32:8900/#scalars&run=.&_smoothingWeight=0.8`\n",
    "\n",
    "`http://222.200.177.32:8888`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = lambda epoch: 0.001 if epoch < 20 else 0.0003    # start with lr of 0.001 and then drop to 0.0001 after 75 epochs\n",
    "tb_callback = keras.callbacks.TensorBoard(log_dir='/home/hadoop/Documents/Neutrino/tensorboard_logs', histogram_freq=0, write_graph=False)\n",
    "callbacks = [LearningRateScheduler(lr_schedule), tb_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(time.strftime('start time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "train_generator = trainGenerator(list_IDs=RAD_id_list[-4:-3], nt=nt, image_size=image_size, image_scalar=image_scalar, batch_size=1)\n",
    "valid_generator = trainGenerator(list_IDs=RAD_id_list[:12], nt=nt, image_size=image_size, image_scalar=image_scalar, batch_size=1, shuffle=False)\n",
    "# history = model.fit_generator(train_generator, steps_per_epoch=30, epochs=50, validation_data=valid_generator, validation_steps=12, use_multiprocessing=True, max_queue_size=20, callbacks=callbacks)\n",
    "print(time.strftime('end time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "print(time.strftime('It took  %H:%M:%S', time.gmtime(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = lambda epoch: 0.0003 if epoch < 20 else 0.00003    # start with lr of 0.001 and then drop to 0.0001 after 75 epochs\n",
    "tb_callback = keras.callbacks.TensorBoard(log_dir='/home/hadoop/Documents/Neutrino/tensorboard_logs', histogram_freq=0, write_graph=False)\n",
    "callbacks = [LearningRateScheduler(lr_schedule), tb_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(time.strftime('start time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "start_time = time.time()\n",
    "train_generator = trainGenerator(list_IDs=RAD_id_list[:6000], nt=nt, image_size=image_size, image_scalar=image_scalar, batch_size=1)\n",
    "valid_generator = trainGenerator(list_IDs=RAD_id_list[-20:], nt=nt, image_size=image_size, image_scalar=image_scalar, batch_size=1)\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=100, epochs=1000, validation_data=valid_generator, validation_steps=20, callbacks=callbacks)\n",
    "print(time.strftime('end time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "print(time.strftime('It took  %H:%M:%S', time.gmtime(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(time.strftime('start time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "start_time = time.time()\n",
    "train_generator = trainGenerator(list_IDs=RAD_id_list[:6000], nt=nt, image_size=image_size, image_scalar=image_scalar, batch_size=5)\n",
    "valid_generator = trainGenerator(list_IDs=RAD_id_list[-20:], nt=nt, image_size=image_size, image_scalar=image_scalar, batch_size=5)\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=20, epochs=50, validation_data=valid_generator, validation_steps=20, callbacks=callbacks)\n",
    "print(time.strftime('end time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "print(time.strftime('It took  %H:%M:%S', time.gmtime(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'SRAD2018_Neutrino_PredNet_v0.4.2.1_18.09.07.22.45'\n",
    "model.save('/home/hadoop/Documents/Neutrino/prednet/%s.hdf5' % version)\n",
    "model.save_weights('/home/hadoop/Documents/Neutrino/prednet/%s_weights.hdf5' % version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `return_sequences=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = model\n",
    "layer_config = train_model.layers[1].get_config()\n",
    "layer_config['output_mode'] = 'prediction'\n",
    "layer_config['return_sequences'] = False\n",
    "data_format = layer_config['data_format'] if 'data_format' in layer_config else layer_config['dim_ordering']\n",
    "test_prednet = PredNet(weights=train_model.layers[1].get_weights(), **layer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = Input(shape=(nt,) + input_shape)\n",
    "test_outputs = test_prednet(test_inputs)\n",
    "\n",
    "def my_loss(y_true, y_pred):\n",
    "#     w = tf.add(y_true, tf.constant(0.8))\n",
    "#     w = tf.add(y_pred, w)\n",
    "#     loss = tf.losses.mean_squared_error(y_true, y_pred, weights=w)\n",
    "    loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "    loss = tf.multiply(loss, tf.constant(10000000.0))\n",
    "    return loss\n",
    "\n",
    "test_model = keras.models.Model(inputs=test_inputs, outputs=test_outputs)\n",
    "test_model.compile(loss=my_loss, optimizer=keras.optimizers.Adam())\n",
    "print(time.strftime('build time: %Y-%m-%d %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class testGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, nt, image_size, image_scalar, batch_size=32, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.nt = nt\n",
    "        self.image_size = image_size\n",
    "        self.image_scalar = image_scalar\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        X, y = data_generation(list_IDs_temp, batch_size=self.batch_size, image_size=self.image_size, nt=self.nt, step_size=step_size, image_scalar=self.image_scalar)\n",
    "        y = np.zeros(self.batch_size, np.float32)\n",
    "        return X, y\n",
    "print(time.strftime('build time: %Y-%m-%d %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime('start time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "train_generator = testGenerator(list_IDs=RAD_id_list[:1], nt=nt, image_size=image_size, image_scalar=image_scalar, batch_size=1)\n",
    "valid_generator = testGenerator(list_IDs=RAD_id_list[-4:-3], nt=nt, image_size=image_size, image_scalar=image_scalar, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for data in [train_generator, valid_generator][0]:\n",
    "    x, y_ = data\n",
    "    break\n",
    "c = 2\n",
    "for i in range(6):\n",
    "    plt.imshow(np.rollaxis(x[0][i], 2, 0)[c] * image_scalar, cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_generator.on_epoch_end()\n",
    "# valid_generator.on_epoch_end()\n",
    "for data in [train_generator, valid_generator][0]:\n",
    "    x, y_ = data\n",
    "    break\n",
    "for i in range(30 // step_size):\n",
    "    y = test_model.predict(x)\n",
    "#     temp_matrix = np.rollaxis(y[0], 2, 0)\n",
    "#     for k in range(3):\n",
    "#         psf_size = 15\n",
    "#         psf = np.ones((psf_size, psf_size)) / psf_size ** 2\n",
    "#         temp_matrix[k] = restoration.wiener(temp_matrix[k], psf, 5)\n",
    "#     y[0] = np.rollaxis(temp_matrix, 0, 3)\n",
    "    y = np.where(y<0.03, -0.01, y)\n",
    "#     y = np.where(y>0.6, 0.6, y)\n",
    "    for j in range(nt - 1):\n",
    "        x[0][j] = x[0][j + 1]\n",
    "    x[0][-1] = y[0]\n",
    "    for k in range(n_channels):\n",
    "        print(i, k)\n",
    "        plt.imshow(np.rollaxis(y[0], 2, 0)[k] * image_scalar, cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(true, pred, split=0):\n",
    "#     true, pred = x_last.reshape(image_size, image_size), y_pred_last.reshape(image_size, image_size)\n",
    "    true_index = np.argsort(true, axis=None)\n",
    "    # print(true_index)\n",
    "    true_r = true.ravel()\n",
    "    pred_r = pred.ravel()\n",
    "    split = pred_r.max() * split\n",
    "    true_r = np.delete(true_r, np.where(true_r<0))\n",
    "    true_r.sort()\n",
    "    # print(true_r)\n",
    "    pred_index = np.argsort(pred, axis=None)\n",
    "    pred_new = np.empty(pred.shape).ravel()\n",
    "    if pred_r.max() <= split:\n",
    "        return pred\n",
    "    distribution_scalar = (len(true_r) - 1) / len(np.delete(pred_r, np.where(pred_r<split)))\n",
    "    position_offset = len(pred_r) - len(np.delete(pred_r, np.where(pred_r<split)))\n",
    "    for i in range(len(pred_new)):\n",
    "        position = (np.where(pred_index==i)[0][0] - position_offset) * distribution_scalar\n",
    "        if pred_r[i] < split:\n",
    "            pred_new[i] = -1\n",
    "        else:\n",
    "            pred_new[i] = true_r[int(position)] * (1+int(position)-position) + true_r[int(position+1)] * (position-int(position))\n",
    "    ans = pred_new.reshape(pred.shape)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "true, pred = x_last.reshape(image_size, image_size), y_pred_last.reshape(image_size, image_size)\n",
    "true_index = np.argsort(true, axis=None)\n",
    "# print(true_index)\n",
    "true_r = true.ravel()\n",
    "pred_r = pred.ravel()\n",
    "true_r = np.delete(true_r, np.where(true_r<0))\n",
    "true_r.sort()\n",
    "# print(true_r)\n",
    "pred_index = np.argsort(pred, axis=None)\n",
    "pred_new = np.empty(pred.shape).ravel()\n",
    "distribution_scalar = (len(true_r) - 1) / len(np.delete(pred_r, np.where(pred_r<0)))\n",
    "position_offset = len(pred_r) - len(np.delete(pred_r, np.where(pred_r<0)))\n",
    "for i in range(len(pred_new)):\n",
    "    position = (np.where(pred_index==i)[0][0] - position_offset) * distribution_scalar\n",
    "    if pred_r[i] < 0:\n",
    "        pred_new[i] = -1\n",
    "    else:\n",
    "        pred_new[i] = true_r[int(position)] * (1+int(position)-position) + true_r[int(position+1)] * (position-int(position))\n",
    "ans = pred_new.reshape(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = rescale(x_last, y_pred_last, split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = x_last.ravel()\n",
    "d1 = np.delete(arr, np.where(arr<0))\n",
    "sns.distplot(d1)\n",
    "plt.show\n",
    "arr = ans.ravel()\n",
    "# arr = arr * 2\n",
    "d2 = np.delete(arr, np.where(arr<0))\n",
    "sns.distplot(d2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y_true_last, cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "plt.show()\n",
    "plt.imshow(ans, cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(time.strftime(\"start time: %Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "model_abs_loss = []\n",
    "model_sqr_loss = []\n",
    "last_frame_abs_loss = []\n",
    "last_frame_sqr_loss = []\n",
    "for RAD_id_counter, RAD_id in enumerate(RAD_id_list[-50:]):\n",
    "    x, y = data_generation([RAD_id], batch_size=1, image_size=image_size, image_scalar=image_scalar, nt=nt, step_size=step_size, offset=60-nt*step_size)\n",
    "    x_last = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, 30)).resize((image_size, image_size))).astype(np.int8).reshape((image_size, image_size))\n",
    "    y_true_last = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, 60)).resize((image_size, image_size))).astype(np.int8).reshape((image_size, image_size))\n",
    "    for i in range(30 // step_size):\n",
    "        y = test_model.predict(x)\n",
    "        temp_matrix = np.rollaxis(y[0], 2, 0)\n",
    "#         for k in range(n_channels):\n",
    "#             temp_matrix[k] = rescale(x_last, temp_matrix[k], split=0.25)\n",
    "        y[0] = np.rollaxis(temp_matrix, 0, 3)\n",
    "        y = np.where(y<0.03, -0.01, y)\n",
    "    #     y = np.where(y>0.6, 0.6, y)\n",
    "        for j in range(nt - 1):\n",
    "            x[0][j] = x[0][j + 1]\n",
    "        x[0][-1] = y[0]\n",
    "    x, y = x[0], y[0]\n",
    "    y_pred_last = np.rollaxis(y, 2, 0)[0] * image_scalar + np.rollaxis(y, 2, 0)[1] * image_scalar + np.rollaxis(y, 2, 0)[2] * image_scalar# + np.rollaxis(y, 2, 0)[3] * image_scalar\n",
    "    y_pred_last = y_pred_last / 3\n",
    "#     y_pred_last = rescale(x_last, y_pred_last, split=0.25)\n",
    "    last_frame_abs_loss.append(sklearn.metrics.mean_absolute_error(y_true_last, x_last))\n",
    "    model_abs_loss.append(sklearn.metrics.mean_absolute_error(y_true_last, y_pred_last))\n",
    "    last_frame_sqr_loss.append(sklearn.metrics.mean_squared_error(y_true_last, x_last))\n",
    "    model_sqr_loss.append(sklearn.metrics.mean_squared_error(y_true_last, y_pred_last))\n",
    "    if RAD_id_counter % 10 == 0:\n",
    "        print('RAD_id_counter=%5d\\t\\tIt takes %.2f' % (RAD_id_counter, time.time() - start_time))\n",
    "model_abs_loss = np.array(model_abs_loss)\n",
    "last_frame_abs_loss = np.array(last_frame_abs_loss)\n",
    "model_sqr_loss = np.array(model_sqr_loss)\n",
    "last_frame_sqr_loss = np.array(last_frame_sqr_loss)\n",
    "print('     model_abs_loss=%.7f' % model_abs_loss.mean())\n",
    "print('last_frame_abs_loss=%.7f' % last_frame_abs_loss.mean())\n",
    "print('     model_sqr_loss=%.7f' % model_sqr_loss.mean())\n",
    "print('last_frame_sqr_loss=%.7f' % last_frame_sqr_loss.mean())\n",
    "print(time.strftime('It took  %H:%M:%S', time.gmtime(time.time() - start_time)))\n",
    "print(time.strftime(\"end time: %Y-%m-%d %H:%M:%S\", time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_abs_loss = np.array(model_abs_loss)\n",
    "last_frame_abs_loss = np.array(last_frame_abs_loss)\n",
    "model_sqr_loss = np.array(model_sqr_loss)\n",
    "last_frame_sqr_loss = np.array(last_frame_sqr_loss)\n",
    "print('     model_abs_loss=%.7f' % model_abs_loss.mean())\n",
    "print('last_frame_abs_loss=%.7f' % last_frame_abs_loss.mean())\n",
    "print('     model_sqr_loss=%.7f' % model_sqr_loss.mean())\n",
    "print('last_frame_sqr_loss=%.7f' % last_frame_sqr_loss.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqr:\n",
    "\n",
    "15.3290176 0\n",
    "15.3366709 1\n",
    "15.2775345 2\n",
    "15.2723169 average\n",
    "\n",
    "27.9708243 0\n",
    "24.8304856 0.1\n",
    "21.4680993 0.2\n",
    "21.1976629 0.25\n",
    "21.4486480 0.3\n",
    "23.0941604 0.4\n",
    "\n",
    "abs:\n",
    "\n",
    "1.4878454 average\n",
    "\n",
    "1.4255825 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_last = x_last.reshape(image_size, image_size)\n",
    "plt.imshow(x_last, cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "plt.show()\n",
    "y_true_last = y_true_last.reshape(image_size, image_size)\n",
    "plt.imshow(y_true_last, cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "plt.show()\n",
    "y_pred_last = y_pred_last.reshape(image_size, image_size)\n",
    "plt.imshow(y_pred_last, cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "plt.show()\n",
    "plt.imshow(rescale(x_last, y_pred_last), cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用模型预测数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAD_id_submit_list = os.listdir(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_test/\")\n",
    "print(len(RAD_id_submit_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAD_id = RAD_id_submit_list[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, y = data_generation([RAD_id], batch_size=1, image_size=image_size, image_scalar=image_scalar, nt=nt, step_size=step_size, offset=0, path='/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_test')\n",
    "\n",
    "for i in range(30 // step_size):\n",
    "    y = test_model.predict(x)\n",
    "#     temp_matrix = np.rollaxis(y[0], 2, 0)\n",
    "#     for k in range(3):\n",
    "#         psf_size = 15\n",
    "#         psf = np.ones((psf_size, psf_size)) / psf_size ** 2\n",
    "#         temp_matrix[k] = restoration.wiener(temp_matrix[k], psf, 5)\n",
    "#     y[0] = np.rollaxis(temp_matrix, 0, 3)\n",
    "    y = np.where(y<0.03, -0.01, y)\n",
    "#     y = np.where(y>0.6, 0.6, y)\n",
    "    for j in range(nt - 1):\n",
    "        x[0][j] = x[0][j + 1]\n",
    "    x[0][-1] = y[0]\n",
    "    for k in range(n_channels):\n",
    "        print(i, k)\n",
    "        plt.imshow(np.rollaxis(y[0], 2, 0)[k] * image_scalar, cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.rollaxis(y[0], 2, 0)[k].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "version = 'SRAD2018_submit_Neutrino_PredNet_v0.4.1.0_18.09.07.02.07'\n",
    "start_time = time.time()\n",
    "print(time.strftime(\"start time: %Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "for RAD_id_counter, RAD_id in enumerate(RAD_id_submit_list):\n",
    "    x, y = data_generation([RAD_id], batch_size=1, image_size=image_size, image_scalar=image_scalar, nt=nt, step_size=step_size, offset=0, path='/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_test')\n",
    "    if not os.path.exists(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_submit/%s/%s\" % (version, RAD_id)):\n",
    "        os.makedirs(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_submit/%s/%s\" % (version, RAD_id))\n",
    "    for i in range(30 // step_size):\n",
    "        y = test_model.predict(x)\n",
    "    #     temp_matrix = np.rollaxis(y[0], 2, 0)\n",
    "    #     for k in range(3):\n",
    "    #         psf_size = 15\n",
    "    #         psf = np.ones((psf_size, psf_size)) / psf_size ** 2\n",
    "    #         temp_matrix[k] = restoration.wiener(temp_matrix[k], psf, 5)\n",
    "    #     y[0] = np.rollaxis(temp_matrix, 0, 3)\n",
    "        y = np.where(y<0.03, -0.01, y)\n",
    "        y = np.where(y>0.33, 0.33, y)\n",
    "        for j in range(nt - 1):\n",
    "            x[0][j] = x[0][j + 1]\n",
    "        x[0][-1] = y[0]\n",
    "        result = np.empty((image_size, image_size))\n",
    "        for k in range(n_channels):\n",
    "#             print(i, k)\n",
    "            result = result + np.rollaxis(y[0], 2, 0)[k] * image_scalar\n",
    "#             plt.imshow(np.rollaxis(y[0], 2, 0)[k] * image_scalar, cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "#             plt.show()\n",
    "        result = result / 3\n",
    "#         print(i)\n",
    "#         plt.imshow(result, cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "#         plt.show()\n",
    "        result = result.astype(np.uint8)\n",
    "        result = np.where(result==0, 255, result)\n",
    "        result = PIL.Image.fromarray(result)\n",
    "        result = result.resize((501, 501))\n",
    "#         plt.imshow(result, cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "#         plt.show()\n",
    "        result.save(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_submit/%s/%s/%s_f%03d.png\" % (version, RAD_id, RAD_id, i + 1))\n",
    "    if RAD_id_counter % 10 == 0:\n",
    "        print('RAD_id_counter=%5d\\t\\tIt takes %.2f' % (RAD_id_counter, time.time() - start_time))\n",
    "print(time.strftime('It took  %H:%M:%S', time.gmtime(time.time() - start_time)))\n",
    "print(time.strftime(\"end time: %Y-%m-%d %H:%M:%S\", time.localtime()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "zip -r \"SRAD2018_submit_Neutrino_PredNet_v0.3.2.0_18.09.01.14.07.zip\" \"SRAD2018_submit_Neutrino_PredNet_v0.3.2.0_18.09.01.14.07\"\n",
    "ls | wc -l\n",
    "```\n",
    "18.09.02 15.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'SRAD2018_submit_Neutrino_PredNet_v0.3.1.0_18.09.01.13.38'\n",
    "start_time = time.time()\n",
    "print(time.strftime(\"start time: %Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "for RAD_id_counter, RAD_id in enumerate(RAD_id_submit_list[:1]):\n",
    "    x = np.empty((nt, image_size, image_size, 1))\n",
    "    y = np.empty((1, image_size, image_size, 1))\n",
    "    for i in range(nt):\n",
    "        x[i] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_test/%s/%s_%03d.png\" % (RAD_id, RAD_id, i + 31 - nt)).resize((image_size, image_size))).astype(np.int8).reshape((image_size, image_size, 1)) / image_scalar\n",
    "    x = x.reshape((1, nt, image_size, image_size, 1))\n",
    "    if not os.path.exists(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_submit/%s/%s\" % (version, RAD_id)):\n",
    "        os.makedirs(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_submit/%s/%s\" % (version, RAD_id))\n",
    "    for i in range(30):\n",
    "#         print('before predict %.2f' % (time.time() - start_time))\n",
    "        y = test_model.predict(x)  # takes 0.2s to predict\n",
    "#         print('after predict %.2f' % (time.time() - start_time))\n",
    "        y = np.where(y<0.03, -0.01, y)\n",
    "        for j in range(nt - 1):\n",
    "            x[0][j] = x[0][j + 1]\n",
    "        x[0][-1] = y[0]\n",
    "        if (31 + i) % 5 == 0:\n",
    "#             print('%2d:' % (31 + i))\n",
    "#             print(i // 5 + 1)\n",
    "            result = y[0].reshape((image_size, image_size)) * image_scalar\n",
    "#             plt.imshow(result, cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "#             plt.show()\n",
    "            result = result.astype(np.uint8)\n",
    "            result = np.where(result==0, 255, result)\n",
    "            result = PIL.Image.fromarray(result)\n",
    "            result = result.resize((501, 501))\n",
    "#             plt.imshow(result, cmap=cm.gist_ncar_r, vmin=vmin, vmax=vmax)\n",
    "#             plt.show()\n",
    "            result.save(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_submit/%s/%s/%s_f%03d.png\" % (version, RAD_id, RAD_id, i // 5 + 1))\n",
    "    if RAD_id_counter % 10 == 0:\n",
    "        print('RAD_id_counter=%5d\\t\\tIt takes %.2f' % (RAD_id_counter, time.time() - start_time))\n",
    "print(time.strftime('It took  %H:%M:%S', time.gmtime(time.time() - start_time)))\n",
    "print(time.strftime(\"end time: %Y-%m-%d %H:%M:%S\", time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
