{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train PredNet on KITTI sequences. (Geiger et al. 2013, http://www.cvlibs.net/datasets/kitti/)\n",
    "'''\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "from six.moves import cPickle\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from prednet import PredNet\n",
    "# from data_utils import SequenceGenerator\n",
    "from kitti_settings import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from data_utils import SequenceGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hickle as hkl\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import Iterator\n",
    "\n",
    "# Data generator that creates sequences for input into PredNet.\n",
    "class SequenceGenerator(Iterator):\n",
    "    def __init__(self, data_file, source_file, nt,\n",
    "                 batch_size=8, shuffle=False, seed=None,\n",
    "                 output_mode='error', sequence_start_mode='all', N_seq=None,\n",
    "                 data_format=K.image_data_format()):\n",
    "        self.X = hkl.load(data_file)  # X will be like (n_images, nb_cols, nb_rows, nb_channels)\n",
    "        self.sources = hkl.load(source_file) # source for each image so when creating sequences can assure that consecutive frames are from same video\n",
    "        self.nt = nt\n",
    "        self.batch_size = batch_size\n",
    "        self.data_format = data_format\n",
    "        assert sequence_start_mode in {'all', 'unique'}, 'sequence_start_mode must be in {all, unique}'\n",
    "        self.sequence_start_mode = sequence_start_mode\n",
    "        assert output_mode in {'error', 'prediction'}, 'output_mode must be in {error, prediction}'\n",
    "        self.output_mode = output_mode\n",
    "\n",
    "        if self.data_format == 'channels_first':\n",
    "            self.X = np.transpose(self.X, (0, 3, 1, 2))\n",
    "        self.im_shape = self.X[0].shape\n",
    "\n",
    "        if self.sequence_start_mode == 'all':  # allow for any possible sequence, starting from any frame\n",
    "            self.possible_starts = np.array([i for i in range(self.X.shape[0] - self.nt) if self.sources[i] == self.sources[i + self.nt - 1]])\n",
    "        elif self.sequence_start_mode == 'unique':  #create sequences where each unique frame is in at most one sequence\n",
    "            curr_location = 0\n",
    "            possible_starts = []\n",
    "            while curr_location < self.X.shape[0] - self.nt + 1:\n",
    "                if self.sources[curr_location] == self.sources[curr_location + self.nt - 1]:\n",
    "                    possible_starts.append(curr_location)\n",
    "                    curr_location += self.nt\n",
    "                else:\n",
    "                    curr_location += 1\n",
    "            self.possible_starts = possible_starts\n",
    "\n",
    "        if shuffle:\n",
    "            self.possible_starts = np.random.permutation(self.possible_starts)\n",
    "        if N_seq is not None and len(self.possible_starts) > N_seq:  # select a subset of sequences if want to\n",
    "            self.possible_starts = self.possible_starts[:N_seq]\n",
    "        self.N_sequences = len(self.possible_starts)\n",
    "        super(SequenceGenerator, self).__init__(len(self.possible_starts), batch_size, shuffle, seed)\n",
    "\n",
    "    def next(self):\n",
    "        with self.lock:\n",
    "            index_array, current_index, current_batch_size = next(self.index_generator)\n",
    "        batch_x = np.zeros((current_batch_size, self.nt) + self.im_shape, np.float32)\n",
    "        for i, idx in enumerate(index_array):\n",
    "            idx = self.possible_starts[idx]\n",
    "            batch_x[i] = self.preprocess(self.X[idx:idx+self.nt])\n",
    "        if self.output_mode == 'error':  # model outputs errors, so y should be zeros\n",
    "            batch_y = np.zeros(current_batch_size, np.float32)\n",
    "        elif self.output_mode == 'prediction':  # output actual pixels\n",
    "            batch_y = batch_x\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def preprocess(self, X):\n",
    "        return X.astype(np.float32) / 255\n",
    "\n",
    "    def create_all(self):\n",
    "        X_all = np.zeros((self.N_sequences, self.nt) + self.im_shape, np.float32)\n",
    "        for i, idx in enumerate(self.possible_starts):\n",
    "            X_all[i] = self.preprocess(self.X[idx:idx+self.nt])\n",
    "        return X_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = True  # if weights will be saved\n",
    "weights_file = os.path.join(WEIGHTS_DIR, 'prednet_kitti_weights.hdf5')  # where weights will be saved\n",
    "json_file = os.path.join(WEIGHTS_DIR, 'prednet_kitti_model.json')\n",
    "\n",
    "# Data files\n",
    "train_file = os.path.join(DATA_DIR, 'X_train.hkl')\n",
    "train_sources = os.path.join(DATA_DIR, 'sources_train.hkl')\n",
    "val_file = os.path.join(DATA_DIR, 'X_val.hkl')\n",
    "val_sources = os.path.join(DATA_DIR, 'sources_val.hkl')\n",
    "\n",
    "# Training parameters\n",
    "nb_epoch = 150\n",
    "batch_size = 4\n",
    "samples_per_epoch = 500\n",
    "N_seq_val = 100  # number of sequences to use for validation\n",
    "\n",
    "# Model parameters\n",
    "n_channels, im_height, im_width = (3, 128, 160)\n",
    "input_shape = (n_channels, im_height, im_width) if K.image_data_format() == 'channels_first' else (im_height, im_width, n_channels)\n",
    "stack_sizes = (n_channels, 48, 96, 192)\n",
    "R_stack_sizes = stack_sizes\n",
    "A_filt_sizes = (3, 3, 3)\n",
    "Ahat_filt_sizes = (3, 3, 3, 3)\n",
    "R_filt_sizes = (3, 3, 3, 3)\n",
    "layer_loss_weights = np.array([1., 0., 0., 0.])  # weighting for each layer in final loss; \"L_0\" model:  [1, 0, 0, 0], \"L_all\": [1, 0.1, 0.1, 0.1]\n",
    "layer_loss_weights = np.expand_dims(layer_loss_weights, 1)\n",
    "nt = 10  # number of timesteps used for sequences in training\n",
    "time_loss_weights = 1./ (nt - 1) * np.ones((nt,1))  # equally weight all timesteps except the first\n",
    "time_loss_weights[0] = 0\n",
    "\n",
    "\n",
    "prednet = PredNet(stack_sizes, R_stack_sizes,\n",
    "                  A_filt_sizes, Ahat_filt_sizes, R_filt_sizes,\n",
    "                  output_mode='error', return_sequences=True)\n",
    "\n",
    "inputs = Input(shape=(nt,) + input_shape)\n",
    "errors = prednet(inputs)  # errors will be (batch_size, nt, nb_layers)\n",
    "errors_by_time = TimeDistributed(Dense(1, trainable=False), weights=[layer_loss_weights, np.zeros(1)], trainable=False)(errors)  # calculate weighted error by layer\n",
    "errors_by_time = Flatten()(errors_by_time)  # will be (batch_size, nt)\n",
    "final_errors = Dense(1, weights=[time_loss_weights, np.zeros(1)], trainable=False)(errors_by_time)  # weight errors by time\n",
    "model = Model(inputs=inputs, outputs=final_errors)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "\n",
    "train_generator = SequenceGenerator(train_file, train_sources, nt, batch_size=batch_size, shuffle=True)\n",
    "val_generator = SequenceGenerator(val_file, val_sources, nt, batch_size=batch_size, N_seq=N_seq_val)\n",
    "\n",
    "lr_schedule = lambda epoch: 0.001 if epoch < 75 else 0.0001    # start with lr of 0.001 and then drop to 0.0001 after 75 epochs\n",
    "callbacks = [LearningRateScheduler(lr_schedule)]\n",
    "if save_model:\n",
    "    if not os.path.exists(WEIGHTS_DIR): os.mkdir(WEIGHTS_DIR)\n",
    "    callbacks.append(ModelCheckpoint(filepath=weights_file, monitor='val_loss', save_best_only=True))\n",
    "\n",
    "history = model.fit_generator(train_generator, samples_per_epoch / batch_size, nb_epoch, callbacks=callbacks,\n",
    "                validation_data=val_generator, validation_steps=N_seq_val / batch_size)\n",
    "\n",
    "if save_model:\n",
    "    json_string = model.to_json()\n",
    "    with open(json_file, \"w\") as f:\n",
    "        f.write(json_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
