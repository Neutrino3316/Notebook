{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import PIL\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import gc\n",
    "import sys\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "from six.moves import cPickle\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "# from keras.models import Model\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# from prednet import PredNet\n",
    "# # from data_utils import SequenceGenerator\n",
    "# from kitti_settings import *\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import time\n",
    "import multiprocessing\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import skimage\n",
    "from skimage import restoration\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n"
     ]
    }
   ],
   "source": [
    "raw_RAD_id_list = os.listdir('/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/')\n",
    "print(len(raw_RAD_id_list))\n",
    "RAD_id_list = raw_RAD_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    }
   ],
   "source": [
    "RAD_id_submit_list = os.listdir(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_Test_2/\")\n",
    "print(len(RAD_id_submit_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_RAD_id(RAD_id):\n",
    "#     return RAD_id\n",
    "    mean_list = []\n",
    "    for k in range(61):\n",
    "        mean_list.append(np.array(PIL.Image.open('/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png'\n",
    "                         % (RAD_id, RAD_id,\n",
    "                        k))).astype(np.int8).ravel().mean())\n",
    "    mean_list = np.array(mean_list)\n",
    "    if mean_list.mean() < -0.5:\n",
    "        return None\n",
    "    for k in range(59):\n",
    "        if abs(mean_list[k] + mean_list[k + 2] - 2 * mean_list[k + 1]) > 2:\n",
    "            return None\n",
    "    return RAD_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 2018-10-04 00:37:35\n",
      "end time: 2018-10-04 00:38:35\n",
      "00:01:00\n",
      "6562\n"
     ]
    }
   ],
   "source": [
    "pool = multiprocessing.Pool()\n",
    "start_time = time.time()\n",
    "print(time.strftime('start time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "# map(check_RAD_id, raw_RAD_id_list[:100])\n",
    "# print(time.strftime('start time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "RAD_id_list = list(pool.map(check_RAD_id, raw_RAD_id_list))\n",
    "RAD_id_list = [x for x in RAD_id_list if x is not None]\n",
    "print(time.strftime('end time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "print(time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time)))\n",
    "print(len(RAD_id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build time: 2018-10-04 00:38:35\n"
     ]
    }
   ],
   "source": [
    "step_size = 3\n",
    "\n",
    "class trainGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, nt, image_size, image_scalar, batch_size=32, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.nt = nt\n",
    "        self.image_size = image_size\n",
    "        self.image_scalar = image_scalar\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "#         'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "#         X = np.empty((self.batch_size, self.nt, self.image_size, self.image_size, 3))\n",
    "# #         y = np.empty((self.batch_size, self.image_size, self.image_size, 1))\n",
    "#         for i, RAD_id in enumerate(list_IDs_temp):\n",
    "#             offset = random.randint(0, 61 - self.nt * step_size)\n",
    "#             offset = random.randint(2, 59 - self.nt * step_size)\n",
    "#             for j in range(self.nt):\n",
    "# #                 X[i][j] = (np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset)).resize((self.image_size, self.image_size))).astype(np.int8).reshape((self.image_size, self.image_size, 1)) / self.image_scalar + \n",
    "# #                     np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset + 1)).resize((self.image_size, self.image_size))).astype(np.int8).reshape((self.image_size, self.image_size, 1)) / self.image_scalar + \n",
    "# #                     np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset - 1)).resize((self.image_size, self.image_size))).astype(np.int8).reshape((self.image_size, self.image_size, 1)) / self.image_scalar) / 3\n",
    "#                 temp_matrix = np.empty((3, self.image_size, self.image_size))\n",
    "#                 temp_matrix[0] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset - 1)).resize((self.image_size, self.image_size))).astype(np.int8) / self.image_scalar + \\\n",
    "#                                  np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset - 2)).resize((self.image_size, self.image_size))).astype(np.int8) / self.image_scalar\n",
    "#                 temp_matrix[1] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset)).resize((self.image_size, self.image_size))).astype(np.int8) / self.image_scalar * 2\n",
    "#                 temp_matrix[2] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset + 1)).resize((self.image_size, self.image_size))).astype(np.int8) / self.image_scalar + \\\n",
    "#                                  np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset + 2)).resize((self.image_size, self.image_size))).astype(np.int8) / self.image_scalar\n",
    "# #                 temp_matrix[2] = temp_matrix[2] - temp_matrix[1]\n",
    "# #                 temp_matrix[0] = temp_matrix[1] - temp_matrix[0]\n",
    "#                 temp_matrix[0] = cv2.GaussianBlur(temp_matrix[1], (5, 5), 0)\n",
    "#                 temp_matrix[1] = cv2.GaussianBlur(temp_matrix[1], (9, 9), 0)\n",
    "#                 temp_matrix[2] = cv2.GaussianBlur(temp_matrix[1], (13, 13), 0)\n",
    "#                 temp_matrix = np.rollaxis(temp_matrix, 0, 3)\n",
    "#                 X[i][j] = temp_matrix\n",
    "# #             y[i] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, (self.nt) * 1)).resize((self.image_size, self.image_size))).astype(np.int8).reshape((self.image_size, self.image_size, 1)) / self.image_scalar\n",
    "        X, y = data_generation(list_IDs_temp, batch_size=self.batch_size, image_size=self.image_size, nt=self.nt, step_size=step_size, image_scalar=self.image_scalar)\n",
    "        y = np.zeros(self.batch_size, np.float32)\n",
    "        return X, y\n",
    "print(time.strftime('build time: %Y-%m-%d %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = 1\n",
    "def data_generation(list_IDs_temp, batch_size, image_size, nt, step_size, image_scalar, offset=None, path='/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train'):\n",
    "    '''\n",
    "    nt * step_size + offset = 60\n",
    "    '''\n",
    "    X = np.empty((batch_size, nt, image_size, image_size, n_channels))\n",
    "    y = np.empty((batch_size, image_size, image_size, n_channels))\n",
    "    for i, RAD_id in enumerate(list_IDs_temp):\n",
    "        if offset == None:\n",
    "            offset = random.randint(2, 59 - nt * step_size)\n",
    "#             offset = random.randint(0, 61 - nt * step_size)\n",
    "        for j in range(nt):\n",
    "            temp_matrix = np.empty((n_channels, image_size, image_size))\n",
    "#             temp_matrix[0] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset - 1)).resize((image_size, image_size))).astype(np.int8) / image_scalar + \\\n",
    "#                              np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset - 2)).resize((image_size, image_size))).astype(np.int8) / image_scalar\n",
    "            temp_matrix[0] = np.array(PIL.Image.open(\"%s/%s/%s_%03d.png\" % (path, RAD_id, RAD_id, j * step_size + offset)).resize((image_size, image_size))).astype(np.int8) / image_scalar\n",
    "#             temp_matrix[2] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset + 1)).resize((image_size, image_size))).astype(np.int8) / image_scalar + \\\n",
    "#                              np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, j * step_size + offset + 2)).resize((image_size, image_size))).astype(np.int8) / image_scalar\n",
    "#             temp_matrix[1] = cv2.GaussianBlur(temp_matrix[0], (5, 5), 0)\n",
    "#             temp_matrix[2] = cv2.GaussianBlur(temp_matrix[0], (9, 9), 0)\n",
    "            temp_matrix[0] = cv2.GaussianBlur(temp_matrix[0], (13, 13), 0)\n",
    "            temp_matrix = np.rollaxis(temp_matrix, 0, 3)\n",
    "            X[i][j] = temp_matrix\n",
    "        temp_matrix = np.empty((n_channels, image_size, image_size))\n",
    "        temp_matrix[0] = np.array(PIL.Image.open(\"%s/%s/%s_%03d.png\" % (path, RAD_id, RAD_id, nt * step_size + offset)).resize((image_size, image_size))).astype(np.int8) / image_scalar\n",
    "#         temp_matrix[1] = cv2.GaussianBlur(temp_matrix[0], (5, 5), 0)\n",
    "#         temp_matrix[2] = cv2.GaussianBlur(temp_matrix[0], (9, 9), 0)\n",
    "        temp_matrix[0] = cv2.GaussianBlur(temp_matrix[0], (13, 13), 0)\n",
    "        temp_matrix = np.rollaxis(temp_matrix, 0, 3)\n",
    "        y[i] = temp_matrix\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Graph disconnected: cannot obtain value for tensor Tensor(\"input_4:0\", shape=(?, 4, 256, 256, 1), dtype=float32) at layer \"input_4\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-952c00653b42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'build time: %Y-%m-%d %H:%M:%S'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocaltime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m# Graph network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         nodes, nodes_by_depth, layers, layers_by_depth = _map_graph_network(\n\u001b[0;32m--> 237\u001b[0;31m             self.inputs, self.outputs)\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                                          \u001b[0;34m'The following previous layers '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                                          \u001b[0;34m'were accessed without issue: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                                          str(layers_with_complete_input))\n\u001b[0m\u001b[1;32m   1431\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m                     \u001b[0mcomputable_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor Tensor(\"input_4:0\", shape=(?, 4, 256, 256, 1), dtype=float32) at layer \"input_4\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "image_size = 256\n",
    "nt = 4 # number of timesteps used for sequences in training\n",
    "image_scalar = 80\n",
    "vmin = -1\n",
    "vmax = 0.6 * image_scalar\n",
    "n_channels, im_height, im_width = (n_channels, image_size, image_size)\n",
    "input_shape = (n_channels, im_height, im_width) if K.image_data_format() == 'channels_first' else (im_height, im_width, n_channels)\n",
    "\n",
    "network = keras.layers.Input(shape=(nt,) + input_shape)\n",
    "network = keras.layers.ConvLSTM2D(filters=32, kernel_size=(3, 3), input_shape=network.shape, padding='same', return_sequences=False, dropout=0.8, recurrent_dropout=0.9)(network)\n",
    "outputs = network\n",
    "\n",
    "def my_loss(y_true, y_pred):\n",
    "#     w = tf.add(y_true, tf.constant(0.5))\n",
    "#     w = tf.add(y_pred, w)\n",
    "#     loss = tf.losses.mean_squared_error(y_true, y_pred, weights=w)\n",
    "#     loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "    y_true = tf.multiply(y_true, tf.constant(1.0*image_scalar))\n",
    "    y_pred = tf.multiply(y_pred, tf.constant(1.0*image_scalar))\n",
    "    loss = tf.losses.absolute_difference(y_true, y_pred)\n",
    "#     loss = tf.losses.log_loss(y_true, y_pred)\n",
    "#     loss = tf.multiply(loss, tf.constant(1000000000.0))\n",
    "    return loss\n",
    "\n",
    "model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss=my_loss, optimizer=keras.optimizers.Adam())\n",
    "print(time.strftime('build time: %Y-%m-%d %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n"
     ]
    }
   ],
   "source": [
    "raw_RAD_id_list = os.listdir('/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/')\n",
    "print(len(raw_RAD_id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_RAD_id(RAD_id):\n",
    "    sum_list = []\n",
    "    for k in range(61):\n",
    "        sum_list.append(np.array(PIL.Image.open('/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png'\n",
    "                         % (RAD_id, RAD_id,\n",
    "                        k))).astype(np.int8).ravel().sum())\n",
    "    sum_list = np.array(sum_list)\n",
    "    if i % 100 == 0:\n",
    "        print(time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time)))\n",
    "        print(i, sum_list[:7])\n",
    "    if sum_list.mean() < 251001 * 0:\n",
    "        return False\n",
    "    for k in range(59):\n",
    "        if abs(sum_list[k] + sum_list[k + 2] - 2 * sum_list[k + 1]) > 251001 * 2:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAD_id_list = raw_RAD_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(time.strftime('start time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "RAD_id_list = []\n",
    "for (i, RAD_id) in enumerate(raw_RAD_id_list[:100]):\n",
    "    if check_RAD_id(RAD_id):\n",
    "        RAD_id_list.append(RAD_id)\n",
    "print(time.strftime('end time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "print(time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time)))\n",
    "print(len(RAD_id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_by_RAD_id(RAD_id, offset):\n",
    "    x_matrix = np.empty((31, 501, 501, 1))\n",
    "    for i in range(31):\n",
    "        x_matrix[i] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, i + offset))).astype(np.int8).reshape((501, 501, 1)) / SCALAR\n",
    "    y_matrix = np.empty((501, 501, 1))\n",
    "    y_matrix = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, 31 + offset))).astype(np.int8).reshape((501, 501, 1)) / SCALAR\n",
    "    return ([x_matrix], [y_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_start_time = time.time()\n",
    "print(time.strftime('start time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "# sess.close()\n",
    "# with tf.device(\"/gpu:0\"):\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# define placeholder\n",
    "x = tf.placeholder(tf.float32, shape=[None, 31, 501, 501, 1], name='x')\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 501, 501, 1], name='y_')\n",
    "\n",
    "# define the network\n",
    "# network = tl.layers.InputLayer(x, name='input')\n",
    "network = x\n",
    "network = tf.keras.layers.ConvLSTM2D(filters=32, kernel_size=(3, 3), input_shape=x.shape, padding='same', return_sequences=False, dropout=0.8, recurrent_dropout=0.9)(network)\n",
    "network = tf.keras.layers.Conv2D(filters=1, kernel_size=(1, 1), padding='same')(network)\n",
    "\n",
    "y = network\n",
    "# cost = tf.losses.mean_squared_error(y, y_, weights=y_)\n",
    "cost = tf.losses.mean_squared_error(y, y_)\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "n_epoch = 20\n",
    "print_freq = 5\n",
    "\n",
    "sum_cost = 0\n",
    "for epoch in range(n_epoch):\n",
    "    for i in range(1):\n",
    "        for j in range(0, 10, 3):\n",
    "            X_train, Y_train = read_by_RAD_id(RAD_id_list[i], j)\n",
    "            feed_dict = {x: X_train, y_: Y_train}\n",
    "        #     feed_dict.update(network.all_drop)  # enable noise layers\n",
    "            train_cost, _ = sess.run([cost, train_op], feed_dict=feed_dict)\n",
    "            sum_cost += train_cost / (Y_train[0].sum() + 251001)\n",
    "        if i % print_freq == 0:\n",
    "            print('%3d'%epoch, '%3d'%i, RAD_id_list[i], '%3d'%j, time.strftime('%H:%M:%S', time.localtime()), time.strftime('%H:%M:%S', time.gmtime(time.time() - all_start_time)), '%5.5f'%(sum_cost * 10 ** 10))\n",
    "        sum_cost = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {x: X_train}\n",
    "prediction = sess.run(y, feed_dict=feed_dict)\n",
    "prediction = prediction[0] * SCALAR\n",
    "Y_validation = Y_train[0] * SCALAR\n",
    "plt.imshow(Y_validation.reshape((501,501)), cmap=cm.gist_ncar_r)\n",
    "plt.show()\n",
    "print('↓↓↓下面的是模型的%d输出，上面的是%d真实值↑↑↑' % (1, 1))\n",
    "prediction = np.where(prediction<3, -1, prediction)\n",
    "plt.imshow(prediction.reshape((501,501)), cmap=cm.gist_ncar_r)\n",
    "plt.show()\n",
    "print('\\n------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    for j in range(0, 20, 7):\n",
    "        X_train, Y_train = read_by_RAD_id(RAD_id_list[i], j)\n",
    "        feed_dict = {x: X_train}\n",
    "        prediction = sess.run(y, feed_dict=feed_dict)\n",
    "        prediction = prediction[0] * SCALAR\n",
    "        Y_validation = Y_train[0] * SCALAR\n",
    "        plt.imshow(Y_validation.reshape((501,501)), cmap=cm.gist_ncar_r)\n",
    "        plt.show()\n",
    "        print('↓↓↓第%d序列，第%d次，下面的是模型的输出，上面的是真实值↑↑↑' % (i, j))\n",
    "        prediction = np.where(prediction<3, -1, prediction)\n",
    "        plt.imshow(prediction.reshape((501,501)), cmap=cm.gist_ncar_r)\n",
    "        plt.show()\n",
    "        print('\\n------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_by_RAD_id(RAD_id, offset):\n",
    "    x_matrix = np.empty((31, 501, 501, 1))\n",
    "    for i in range(31):\n",
    "        x_matrix[i] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, i + offset))).astype(np.int8).reshape((501, 501, 1)) / SCALAR\n",
    "    y_matrix = np.empty((501, 501, 1))\n",
    "    y_matrix = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, 31 + offset))).astype(np.int8).reshape((501, 501, 1)) / SCALAR\n",
    "    return ([x_matrix], [y_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    RAD_id = RAD_id_list[i]\n",
    "    \n",
    "    for j in range(30):\n",
    "        x_matrix = np.empty((31, 501, 501, 1))\n",
    "        for k in range(31):\n",
    "            x_matrix[k] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, k + j))).astype(np.int8).reshape((501, 501, 1)) / SCALAR\n",
    "        feed_dict = {x: [x_matrix]}\n",
    "        prediction = sess.run(y, feed_dict=feed_dict)\n",
    "        prediction = prediction[0] * SCALAR\n",
    "        y_matrix = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, 31 + j))).astype(np.int8).reshape((501, 501, 1)) / SCALAR\n",
    "        plt.imshow(y_matrix.reshape((501,501)), cmap=cm.gist_ncar_r)\n",
    "        plt.show()\n",
    "        print('↓↓↓第%d序列，第%d次，下面的是模型的输出，上面的是真实值↑↑↑' % (i, j))\n",
    "        prediction = np.where(prediction<3, -1, prediction)\n",
    "        plt.imshow(prediction.reshape((501,501)), cmap=cm.gist_ncar_r)\n",
    "        plt.show()\n",
    "        print('\\n------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_start_time = time.time()\n",
    "print(time.strftime('start time: %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "# sess.close()\n",
    "# with tf.device(\"/gpu:0\"):\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# define placeholder\n",
    "x = tf.placeholder(tf.float32, shape=[None, 31, 501, 501, 1], name='x')\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 501, 501, 1], name='y_')\n",
    "\n",
    "# define the network\n",
    "# network = tl.layers.InputLayer(x, name='input')\n",
    "network = x\n",
    "network = tf.keras.layers.ConvLSTM2D(filters=32, kernel_size=(5, 5), input_shape=x.shape, padding='same', return_sequences=False, dropout=0.8, recurrent_dropout=0.9)(network)\n",
    "network = tf.keras.layers.Conv2D(filters=1, kernel_size=(1, 1), padding='same')(network)\n",
    "\n",
    "y = network\n",
    "# cost = tf.losses.mean_squared_error(y, y_, weights=y_)\n",
    "w = tf.add(tf.constant(0.2), y_)\n",
    "w = tf.add(w, y)\n",
    "cost = tf.losses.mean_squared_error(y, y_, weights=w)\n",
    "# cost = tf.losses.mean_squared_error(y, y_)\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "# train_op = tf.train.GradientDescentOptimizer(learning_rate=0.003).minimize(cost)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "n_epoch = 20\n",
    "print_freq = 5\n",
    "\n",
    "sum_cost = 0\n",
    "for epoch in range(n_epoch):\n",
    "    for i in range(1):\n",
    "        RAD_id = RAD_id_list[i]\n",
    "        x_matrix = np.empty((31, 501, 501, 1))\n",
    "        for k in range(30):\n",
    "            x_matrix[k + 1] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, k))).astype(np.int8).reshape((501, 501, 1)) / SCALAR\n",
    "        prediction = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, 30))).astype(np.int8).reshape((501, 501, 1)) / SCALAR\n",
    "        for j in range(1):\n",
    "            for k in range(30):\n",
    "                x_matrix[k] = x_matrix[k + 1]\n",
    "            x_matrix[30] = prediction\n",
    "            \n",
    "            feed_dict = {x: [x_matrix], y_: [y_matrix]}\n",
    "            train_cost, _ = sess.run([cost, train_op], feed_dict=feed_dict)\n",
    "            sum_cost += train_cost / (y_matrix.sum() + 251001)\n",
    "            if i % print_freq == 0:\n",
    "                print('%3d'%epoch, '%3d'%i, RAD_id_list[i], '%3d'%j, time.strftime('%H:%M:%S', time.localtime()), time.strftime('%H:%M:%S', time.gmtime(time.time() - all_start_time)), '%5.5f'%(sum_cost * 10 ** 10))\n",
    "                sum_cost = 0\n",
    "                \n",
    "            feed_dict = {x: [x_matrix]}\n",
    "            prediction = sess.run(y, feed_dict=feed_dict)\n",
    "            prediction = prediction[0] * SCALAR\n",
    "            y_matrix = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, 31 + j))).astype(np.int8).reshape((501, 501, 1)) / SCALAR\n",
    "#             plt.imshow(y_matrix.reshape((501,501)), cmap=cm.gist_ncar_r)\n",
    "#             plt.show()\n",
    "#             print('↓↓↓第%d序列，第%d次，下面的是模型的输出，上面的是真实值↑↑↑' % (i, j))\n",
    "            prediction = np.where(prediction<3, -1, prediction)\n",
    "#             plt.imshow(prediction.reshape((501,501)), cmap=cm.gist_ncar_r)\n",
    "#             plt.show()\n",
    "#             print('\\n------------------------------------\\n')\n",
    "            prediction = prediction / SCALAR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    RAD_id = RAD_id_list[i]\n",
    "    x_matrix = np.empty((31, 501, 501, 1))\n",
    "    for k in range(30):\n",
    "        x_matrix[k + 1] = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, k))).astype(np.int8).reshape((501, 501, 1)) / SCALAR\n",
    "    prediction = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, 30))).astype(np.int8).reshape((501, 501, 1)) / SCALAR\n",
    "    for j in range(30):\n",
    "        for k in range(30):\n",
    "            x_matrix[k] = x_matrix[k + 1]\n",
    "        x_matrix[30] = prediction\n",
    "        feed_dict = {x: [x_matrix]}\n",
    "        prediction = sess.run(y, feed_dict=feed_dict)\n",
    "        prediction = prediction[0] * SCALAR\n",
    "        y_matrix = np.array(PIL.Image.open(\"/home/hadoop/Documents/Neutrino/SRAD2018/SRAD2018_train/%s/%s_%03d.png\" % (RAD_id, RAD_id, 31 + j))).astype(np.int8).reshape((501, 501, 1)) / SCALAR\n",
    "        plt.imshow(y_matrix.reshape((501,501)), cmap=cm.gist_ncar_r)\n",
    "        plt.show()\n",
    "        print('↓↓↓第%d序列，第%d次，下面的是模型的输出，上面的是真实值↑↑↑' % (i, j))\n",
    "        prediction = np.where(prediction<3, -1, prediction)\n",
    "        plt.imshow(prediction.reshape((501,501)), cmap=cm.gist_ncar_r)\n",
    "        plt.show()\n",
    "        print('\\n------------------------------------\\n')\n",
    "        prediction = prediction / SCALAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
