{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[tutorial_mlp_dropout1.py](https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_mlp_dropout1.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] Load or Download MNIST > data/mnist\n",
      "[TL] data/mnist/train-images-idx3-ubyte.gz\n",
      "[TL] data/mnist/t10k-images-idx3-ubyte.gz\n",
      "[TL] InputLayer  input: (?, 784)\n",
      "[TL] DropoutLayer drop1: keep: 0.800000 is_fix: False\n",
      "[TL] DenseLayer  relu1: 800 relu\n",
      "[TL] DropoutLayer drop2: keep: 0.500000 is_fix: False\n",
      "[TL] DenseLayer  relu2: 800 relu\n",
      "[TL] DropoutLayer drop3: keep: 0.500000 is_fix: False\n",
      "[TL] DenseLayer  output: 10 No Activation\n",
      "[TL]   param   0: relu1/W:0            (784, 800)         float32_ref (mean: 0.00014151542563922703, median: 0.00016233438509516418, std: 0.08791138976812363)   \n",
      "[TL]   param   1: relu1/b:0            (800,)             float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   2: relu2/W:0            (800, 800)         float32_ref (mean: 0.00013466845848597586, median: 6.119750469224527e-05, std: 0.08795106410980225)   \n",
      "[TL]   param   3: relu2/b:0            (800,)             float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   4: output/W:0           (800, 10)          float32_ref (mean: -0.0011779007036238909, median: -0.002222686540335417, std: 0.08765094727277756)   \n",
      "[TL]   param   5: output/b:0           (10,)              float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   num of params: 1276810\n",
      "[TL]   layer   0: x:0                  (?, 784)           float32\n",
      "[TL]   layer   1: drop1/mul:0          (?, 784)           float32\n",
      "[TL]   layer   2: relu1/Relu:0         (?, 800)           float32\n",
      "[TL]   layer   3: drop2/mul:0          (?, 800)           float32\n",
      "[TL]   layer   4: relu2/Relu:0         (?, 800)           float32\n",
      "[TL]   layer   5: drop3/mul:0          (?, 800)           float32\n",
      "[TL]   layer   6: output/bias_add:0    (?, 10)            float32\n",
      "Epoch 1 of 50 took 0.420143s\n",
      "   train loss: 0.630767\n",
      "   train acc: 0.792940\n",
      "   val loss: 0.574484\n",
      "   val acc: 0.815800\n",
      "Epoch 5 of 50 took 0.374062s\n",
      "   train loss: 0.315989\n",
      "   train acc: 0.905780\n",
      "   val loss: 0.286975\n",
      "   val acc: 0.916200\n",
      "Epoch 10 of 50 took 0.375595s\n",
      "   train loss: 0.241054\n",
      "   train acc: 0.930920\n",
      "   val loss: 0.223665\n",
      "   val acc: 0.937100\n",
      "Epoch 15 of 50 took 0.382453s\n",
      "   train loss: 0.199213\n",
      "   train acc: 0.943740\n",
      "   val loss: 0.187421\n",
      "   val acc: 0.949500\n",
      "Epoch 20 of 50 took 0.376041s\n",
      "   train loss: 0.169255\n",
      "   train acc: 0.952720\n",
      "   val loss: 0.163315\n",
      "   val acc: 0.955700\n",
      "Epoch 25 of 50 took 0.371489s\n",
      "   train loss: 0.147330\n",
      "   train acc: 0.958780\n",
      "   val loss: 0.146133\n",
      "   val acc: 0.960900\n",
      "Epoch 30 of 50 took 0.377723s\n",
      "   train loss: 0.129142\n",
      "   train acc: 0.964200\n",
      "   val loss: 0.131392\n",
      "   val acc: 0.963200\n",
      "Epoch 35 of 50 took 0.401277s\n",
      "   train loss: 0.116229\n",
      "   train acc: 0.968280\n",
      "   val loss: 0.121903\n",
      "   val acc: 0.966200\n",
      "Epoch 40 of 50 took 0.379699s\n",
      "   train loss: 0.103380\n",
      "   train acc: 0.972100\n",
      "   val loss: 0.112087\n",
      "   val acc: 0.969100\n",
      "Epoch 45 of 50 took 0.405135s\n",
      "   train loss: 0.092631\n",
      "   train acc: 0.975060\n",
      "   val loss: 0.104372\n",
      "   val acc: 0.971500\n",
      "Epoch 50 of 50 took 0.378135s\n",
      "   train loss: 0.084544\n",
      "   train acc: 0.977020\n",
      "   val loss: 0.098767\n",
      "   val acc: 0.972300\n",
      "Evaluation\n",
      "   test loss: 0.100581\n",
      "   test acc: 0.971600\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
    "tl.logging.set_verbosity(tl.logging.DEBUG)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# prepare data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 784))\n",
    "# define placeholder\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784], name='x')\n",
    "y_ = tf.placeholder(tf.int64, shape=[None], name='y_')\n",
    "\n",
    "# define the network\n",
    "network = tl.layers.InputLayer(x, name='input')\n",
    "network = tl.layers.DropoutLayer(network, keep=0.8, name='drop1')\n",
    "network = tl.layers.DenseLayer(network, n_units=800, act=tf.nn.relu, name='relu1')\n",
    "network = tl.layers.DropoutLayer(network, keep=0.5, name='drop2')\n",
    "network = tl.layers.DenseLayer(network, n_units=800, act=tf.nn.relu, name='relu2')\n",
    "network = tl.layers.DropoutLayer(network, keep=0.5, name='drop3')\n",
    "# the softmax is implemented internally in tl.cost.cross_entropy(y, y_) to\n",
    "# speed up computation, so we use identity here.\n",
    "# see tf.nn.sparse_softmax_cross_entropy_with_logits()\n",
    "network = tl.layers.DenseLayer(network, n_units=10, act=None, name='output')\n",
    "\n",
    "# define cost function and metric.\n",
    "y = network.outputs\n",
    "cost = tl.cost.cross_entropy(y, y_, name='xentropy')\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), y_)\n",
    "acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "y_op = tf.argmax(tf.nn.softmax(y), 1)\n",
    "\n",
    "# define the optimizer\n",
    "train_params = network.all_params\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost, var_list=train_params)\n",
    "\n",
    "# initialize all variables in the session\n",
    "tl.layers.initialize_global_variables(sess)\n",
    "\n",
    "# print network information\n",
    "network.print_params()\n",
    "network.print_layers()\n",
    "\n",
    "n_epoch = 50\n",
    "batch_size = 500\n",
    "print_freq = 5\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    start_time = time.time()\n",
    "    for X_train_a, y_train_a in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\n",
    "        feed_dict = {x: X_train_a, y_: y_train_a}\n",
    "        feed_dict.update(network.all_drop)  # enable noise layers\n",
    "        sess.run(train_op, feed_dict=feed_dict)\n",
    "\n",
    "    if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n",
    "        print(\"Epoch %d of %d took %fs\" % (epoch + 1, n_epoch, time.time() - start_time))\n",
    "        train_loss, train_acc, n_batch = 0, 0, 0\n",
    "        for X_train_a, y_train_a in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\n",
    "            dp_dict = tl.utils.dict_to_one(network.all_drop)  # disable noise layers\n",
    "            feed_dict = {x: X_train_a, y_: y_train_a}\n",
    "            feed_dict.update(dp_dict)\n",
    "            err, ac = sess.run([cost, acc], feed_dict=feed_dict)\n",
    "            train_loss += err\n",
    "            train_acc += ac\n",
    "            n_batch += 1\n",
    "        print(\"   train loss: %f\" % (train_loss / n_batch))\n",
    "        print(\"   train acc: %f\" % (train_acc / n_batch))\n",
    "        val_loss, val_acc, n_batch = 0, 0, 0\n",
    "        for X_val_a, y_val_a in tl.iterate.minibatches(X_val, y_val, batch_size, shuffle=True):\n",
    "            dp_dict = tl.utils.dict_to_one(network.all_drop)  # disable noise layers\n",
    "            feed_dict = {x: X_val_a, y_: y_val_a}\n",
    "            feed_dict.update(dp_dict)\n",
    "            err, ac = sess.run([cost, acc], feed_dict=feed_dict)\n",
    "            val_loss += err\n",
    "            val_acc += ac\n",
    "            n_batch += 1\n",
    "        print(\"   val loss: %f\" % (val_loss / n_batch))\n",
    "        print(\"   val acc: %f\" % (val_acc / n_batch))\n",
    "\n",
    "print('Evaluation')\n",
    "test_loss, test_acc, n_batch = 0, 0, 0\n",
    "for X_test_a, y_test_a in tl.iterate.minibatches(X_test, y_test, batch_size, shuffle=True):\n",
    "    dp_dict = tl.utils.dict_to_one(network.all_drop)  # disable noise layers\n",
    "    feed_dict = {x: X_test_a, y_: y_test_a}\n",
    "    feed_dict.update(dp_dict)\n",
    "    err, ac = sess.run([cost, acc], feed_dict=feed_dict)\n",
    "    test_loss += err\n",
    "    test_acc += ac\n",
    "    n_batch += 1\n",
    "print(\"   test loss: %f\" % (test_loss / n_batch))\n",
    "print(\"   test acc: %f\" % (test_acc / n_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
